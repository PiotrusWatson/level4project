{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4NbGwIC9-z",
        "colab_type": "text"
      },
      "source": [
        "##HAHA ITS TIME TO SPEND 5 HOURS DOWNLOADING THINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "5877bd4f-8671-4eee-a01d-6ba1de200229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:17:58--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  21.2MB/s    in 7.5s    \n",
            "\n",
            "2020-02-19 22:18:06 (12.1 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "ba4536af-62f9-4abf-f05f-5fd3be508bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:18:16--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-19 22:18:17--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-19 22:18:17--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.88MB/s    in 6m 30s  \n",
            "\n",
            "2020-02-19 22:24:48 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "72391a64-ed8c-447a-a772-4034391c36a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:25:16--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  2.15MB/s    in 2.2s    \n",
            "\n",
            "2020-02-19 22:25:20 (2.15 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "f1a7ed23-a2ed-4742-cb1e-2f288aa86d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "16d0cf98-2cca-466a-b63f-3a19e395b955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:25:29--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  2.44MB/s    in 2.2s    \n",
            "\n",
            "2020-02-19 22:25:32 (2.44 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "outputId": "b1bda0f5-23c9-4a1a-d556-8d823daf8006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJXKPPODFqp",
        "colab_type": "text"
      },
      "source": [
        "##LOOK AT ALL THIS CODE TO IMPORT DATA GOD THERE MUST BE SOMETHING WRONG WITH ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "outputId": "62da3783-e85d-40bf-d61e-c493a552f1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "5c7d1fc9-5eb7-4bf0-86be-784fffa1c873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "7f8d5580-0709-404c-aac5-dc3668b93ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "train_challenge, val_challenge = train_test_split(train_challenge, test_size=0.1, random_state=8)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39952</th>\n",
              "      <td>2109</td>\n",
              "      <td>A SCHOOLBOY who was almost killed when he was ...</td>\n",
              "      <td>Islamic State claims it executed American phot...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20359</th>\n",
              "      <td>1212</td>\n",
              "      <td>Tiger Woods divorced Swedish model Elin Nordeg...</td>\n",
              "      <td>Sources: Guns N' Roses Frontman Axl Rose Found...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11075</th>\n",
              "      <td>682</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>Wife and child of Islamic State leader Baghdad...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47329</th>\n",
              "      <td>2413</td>\n",
              "      <td>You may have a seen a story going around Faceb...</td>\n",
              "      <td>Armed U.S. drones spotted flying over Syria in...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15650</th>\n",
              "      <td>942</td>\n",
              "      <td>Ok, this is all still rumor, but there’s a cha...</td>\n",
              "      <td>UPDATE: BATMAN v SUPERMAN Batmobile Reportedly...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Body ID  ... Stance\n",
              "39952     2109  ...      2\n",
              "20359     1212  ...      2\n",
              "11075      682  ...      2\n",
              "47329     2413  ...      2\n",
              "15650      942  ...      1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "fake_news_challenge_mapping = {}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None, is_folding=False):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_text\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  \n",
        "  if is_folding:\n",
        "    results = []\n",
        "    folded = KFold(n_splits=10, shuffle=True)\n",
        "    splitted_object = folded.split(unique)\n",
        "    for train_result, test_result in splitted_object:\n",
        "      train_ilocs = unique.iloc[train_result][\"claim_text\"]\n",
        "      test_ilocs = unique.iloc[test_result][\"claim_text\"]\n",
        "      results.append((facts[facts[\"claim_text\"].isin(train_ilocs)], facts[facts[\"claim_text\"].isin(test_ilocs)]))\n",
        "\n",
        "    return results\n",
        "\n",
        "  train_unique, big_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "  val_unique, test_unique = train_test_split(big_unique, test_size=0.5, random_state=8)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_text\"].isin(test_unique[\"claim_text\"])]\n",
        "  val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "  train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "  return train_facts, test_facts, val_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts, val_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes, val_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "f35b916d-f66b-4145-fbde-629cc6d96e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>vice news we dont have a timeline on the decis...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>a schedule i narcotic along with heroin and ec...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>now do you think you were maybe talking just a...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>made to the new yorker that marijuana is no mo...</td>\n",
              "      <td>time.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jun_27_donald-trump_white-house-criticism...</td>\n",
              "      <td>obamacare signed law cbo estimated 23 million ...</td>\n",
              "      <td>donald trump</td>\n",
              "      <td>about the affordable health care act in its da...</td>\n",
              "      <td>eugeneweekly.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4849</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama to ban guns from 42 million social secur...</td>\n",
              "      <td>bearingarms.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama is looking to ban social security recipi...</td>\n",
              "      <td>rightwingnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4851</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>main navigation recent posts obama to ban 42 m...</td>\n",
              "      <td>downtrend.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4852</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>get news like this in your facebook news feed ...</td>\n",
              "      <td>thegatewaypundit.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>to prove everyone wrong yet again this time it...</td>\n",
              "      <td>zerohedge.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...        article_source\n",
              "187            1  ...            reason.com\n",
              "188            1  ...            reason.com\n",
              "189            1  ...               cnn.com\n",
              "190            1  ...              time.com\n",
              "526            1  ...      eugeneweekly.com\n",
              "...          ...  ...                   ...\n",
              "4849           0  ...       bearingarms.com\n",
              "4850           0  ...     rightwingnews.com\n",
              "4851           0  ...         downtrend.com\n",
              "4852           0  ...  thegatewaypundit.com\n",
              "4853           0  ...         zerohedge.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Beq65oTgcBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, val_loader, test_data, val_data, tokeniser):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.test_data = test_data\n",
        "    self.val_data = val_data\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4lYd83j-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\"claim_text\", \"article\"]\n",
        "big_labels = [\"claim_text\", \"article\", \"article_source\"]\n",
        "challenge_labels = [\"articleBody\", \"Headline\"]\n",
        "\n",
        "def get_list(panda, labels):\n",
        "  label_to_data = {}\n",
        "  for label in labels:\n",
        "    label_to_data[label] = panda[label]\n",
        "  \n",
        "  x_list = convert_to_lists(label_to_data)\n",
        "  if labels[0] == \"claim_text\":\n",
        "    y_list = panda[\"cred_label\"].tolist()\n",
        "  else:\n",
        "    y_list = panda[\"Stance\"].tolist()\n",
        "  return x_list, y_list\n",
        "\n",
        "def get_loader(x, y, vocab_size, max_length, batch_size, name, training=True, drop_last=True):\n",
        "  stuff = []\n",
        "  for key in x:\n",
        "    stuff.append(torch.from_numpy(x[key]).type(torch.LongTensor))\n",
        "  stuff.append(torch.from_numpy(y).type(torch.DoubleTensor))\n",
        "\n",
        "  tensorset = data_utils.TensorDataset(*stuff)\n",
        "  loader = data_utils.DataLoader(tensorset, batch_size=batch_size, drop_last=drop_last, shuffle=training)\n",
        "  loader.name = name\n",
        "  return loader\n",
        "\n",
        "  \n",
        "def get_dataset(train, test, val, vocab_size, max_length, batch_size, labels, name):\n",
        "  is_challenge = labels[0] == \"articleBody\"\n",
        "  train_list_x, train_list_y = get_list(train, labels)\n",
        "  if not is_challenge:\n",
        "    test_list_x, test_list_y = get_list(test, labels)\n",
        "  val_list_x, val_list_y = get_list(val, labels)\n",
        "\n",
        "\n",
        "\n",
        "  #tokenising various stuff, setting up numpy dictionaries :)\n",
        "  tokeniser = Tokeniser(train_list_x, vocab_size, max_length)\n",
        "  x_train = tokeniser.do_everything(train_list_x)\n",
        "  x_test = tokeniser.do_everything(test_list_x)\n",
        "  x_val = tokeniser.do_everything(val_list_x)\n",
        "  y_train = np.array(train_list_y, dtype=np.float32)\n",
        "  y_test = np.array(test_list_y, dtype=np.float32)\n",
        "  y_val = np.array(val_list_y, dtype=np.float32)\n",
        "  \n",
        "  #datasets/loaders\n",
        "  train_loader = get_loader(x_train, y_train, vocab_size, max_length, batch_size, name, True)\n",
        "  test_loader = get_loader(x_test, y_test, vocab_size, max_length, batch_size, name, False, drop_last=False)\n",
        "  val_loader = get_loader(x_val, y_val, vocab_size, max_length, batch_size, name, False)\n",
        "  return Dataset(train_loader, test_loader, val_loader,  y_test, y_val, tokeniser)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MMrKlu6_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "outputId": "1d6e3077-d611-4dcf-c5d4-dcea3d1c69d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "\n",
        "snopes_dataset = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "fact_dataset = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "big_snopes = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "big_fact = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39093\n",
            "33766\n",
            "44623\n",
            "37174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    \n",
        "    self.embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.normaliser = torch.nn.BatchNorm1d(self.embedding_size)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x[:, 0:self.hp.max_length])\n",
        "    #embedding = self.normaliser(embedding.transpose(1,2))\n",
        "    embedding = self.dropout(embedding)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    tanh_W_H = self.dropout(tanh_W_H)\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.premise_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    premise_factor = self.premise_dropout(premise_factor)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    hypothesis_factor = self.hypothesis_dropout(hypothesis_factor)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=50)\n",
        "    self.linear2 = torch.nn.Linear(50, 20)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "      x = self.dropout1(x)\n",
        "    else:\n",
        "      x = torch.relu(self.linear1(x.reshape(self.hp.batch_size, -1)))\n",
        "      x = self.dropout1(x)\n",
        "      x = torch.relu(self.linear2(x))\n",
        "      x = self.dropout2(x)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    print(word_embeddings.shape)\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout)]\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    self.dropouts[1](premise_embedding)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    self.dropouts[3](hypothesis_embedding)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    self.dropouts[4](factorised_mush)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention*premise_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWRHhOxCjFl",
        "colab_type": "text"
      },
      "source": [
        "##EVAL SUMMARY :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout)]*5\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    processed_premise = self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    premise_embedding = self.dropouts[1](premise_embedding)\n",
        "    \n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    processed_hypothesis = self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    hypothesis_embedding = self.dropouts[3](hypothesis_embedding)\n",
        "    combined = premise_embedding * hypothesis_embedding\n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    avg = self.dropouts[4](avg)\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db8ikkk64Kx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def load_embeddings(self, word_embeddings):\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, self.hp)\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(101, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    mean_embeddings = torch.unsqueeze(torch.sum(embeddings, 1) / self.hp.max_length, 1) #change to accurate size of lenfgth\n",
        "  \n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100\n",
        "    main_embeddings = torch.cat((mean_embeddings, added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    attention_weights = softmax(self.premise_linear(processed_premise))#TODO: turn into row vector\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis,attention_weights.transpose(1,2))\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    avg = torch.sum(combined, 1)/self.hp.max_length #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    output = torch.sigmoid(self.linear_final(smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(10000, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 8)\n",
        "    self.linear_almost_there = torch.nn.Linear(8, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.dropout = torch.nn.Dropout(p=0.2)\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    lengths = self.hp.max_length - (premise == 0).sum(dim=1)\n",
        "    lengths = lengths.repeat(50, 1).transpose(0,1)\n",
        "    summed_embeddings = torch.sum(embeddings, 1)\n",
        "    mean_embeddings = torch.unsqueeze(summed_embeddings / lengths, 1) #change to accurate size of lenfgth\n",
        "    flattened_embeddings = mean_embeddings.reshape(self.hp.batch_size, -1)\n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100]).reshape(self.hp.batch_size, -1)\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((flattened_embeddings.repeat(1, 100), added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    attention_weights = softmax(torch.tanh(self.premise_linear(main_embeddings)))#TODO: turn into row vector\n",
        "\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, attention_weights.unsqueeze(2))\n",
        "    \n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    new_lengths = lengths.repeat(1, 2)\n",
        "\n",
        "    avg = torch.sum(combined, 1)/new_lengths #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    smaller = self.dropout(smaller)\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    output = torch.sigmoid(self.linear_final(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def should_stop(losses, train_losses, limit):\n",
        "  shouldnt_stop = False \n",
        "  is_learning = False\n",
        "  print(\"losses:\",losses)\n",
        "  print(\"train_losses:\", train_losses)\n",
        "  \n",
        "  if len(losses) < limit:\n",
        "    return False\n",
        "  last = losses[-1]\n",
        "  last_train = train_losses[-1]\n",
        "\n",
        "  if limit == 2:\n",
        "    return last >= losses[-2]\n",
        "  \n",
        "  for i in range(-2,- limit-1,-1):\n",
        "    shouldnt_stop = (shouldnt_stop or last < losses[i])\n",
        "    last = losses[i]\n",
        "  return not shouldnt_stop\n",
        "\n",
        "\n",
        "def train(model=None, \n",
        "          dataset = None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "\n",
        "  is_binary = hp.num_classes == 1\n",
        "  is_validating = False\n",
        "  has_stopped = False\n",
        "  \n",
        "  if dataset.train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif dataset.train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "\n",
        "    for i in range(2):\n",
        "      \n",
        "      total_loss = 0\n",
        "      batch_count = 0\n",
        "      correct = 0\n",
        "      penal = 0\n",
        "\n",
        "      if is_validating:\n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.val_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 1\n",
        "      elif has_stopped:\n",
        "        model.train(False)\n",
        "        loader = dataset.train_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 10\n",
        "      else:\n",
        "        model.train(True)\n",
        "        loader = dataset.train_loader\n",
        "        torch.enable_grad()\n",
        "        debug_amount = 10\n",
        "      \n",
        "      for batch_index, train_data in enumerate(loader):\n",
        "      #setting everything up\n",
        "        model.hidden_state = model.reset_for_testing(train_data[0].shape[0])\n",
        "        train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "        predicted_y, attention = model(*train_data[:-1])\n",
        "        actual_y = train_data[-1]\n",
        "        squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "        if hp.C > 0:\n",
        "          attentionT = attention.transpose(1,2)\n",
        "          identity = torch.eye(attention.size(1))\n",
        "          identity = Variable(identity.unsqueeze(0).expand(loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "          penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "        if is_binary:\n",
        "          loss = loss_function(squeezed_y, actual_y.double())\n",
        "          loss += hp.C * penal/loader.batch_size\n",
        "          correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "        else:\n",
        "          loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/loader.batch_size)\n",
        "          correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if using_gradient_clipping:\n",
        "          torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      \n",
        "      #cleaning up regularisation\n",
        "        if hp.C > 0:\n",
        "          del(penal)\n",
        "          del(identity)\n",
        "          del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "        optimiser.zero_grad()\n",
        "        if not is_validating and not has_stopped:\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "        if hp.is_debug and batch_index % debug_amount == 0:\n",
        "          print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, ISvAL: {}, HasStopped: {}\".format(\n",
        "              epoch, batch_index * len(train_data[0]), len(loader.dataset),\n",
        "              100. * batch_index / len(loader), loss.item(), is_validating, has_stopped\n",
        "          ))\n",
        "\n",
        "      \n",
        "        batch_count += 1\n",
        "      \n",
        "        free_data(train_data)\n",
        "\n",
        "      print(\"Average loss is:\",total_loss/batch_count, \"while validation_status:\", is_validating, \"and stopping_status\", has_stopped)\n",
        "      correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "      accuracy = correct_but_numpy / float(batch_count * loader.batch_size)\n",
        "      print(\"Accuracy of the model\", accuracy)\n",
        "      if (not is_validating):\n",
        "        losses.append(total_loss/batch_count)\n",
        "        accuracies.append(accuracy)\n",
        "      else:\n",
        "        val_losses.append(total_loss/batch_count)\n",
        "        val_accuracies.append(accuracy)\n",
        "      if hp.use_early_stopping:\n",
        "        has_stopped = should_stop(val_losses,losses, hp.early_stopping)\n",
        "\n",
        "      is_validating = not is_validating\n",
        "  return losses, val_losses, accuracies, val_accuracies "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, val_losses, accuracies, val_accuracies, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  \n",
        "  plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Train Accuracy\")\n",
        "  plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_accuracies, scalex=True, scaley=True, label=\"Val Accuracy\")\n",
        "  plt.annotate(str(val_accuracies[-1]), xy=(epochs,val_accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Train Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_losses,scalex=True, scaley=True, label=\"Val Loss\")\n",
        "  plt.annotate(str(val_losses[-1]), xy=(epochs,val_losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYfImtudskLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp, is_plot=True):\n",
        "  runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "  bce_loss = torch.nn.BCELoss()\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(runnable_model.parameters(), lr=hp.lr, weight_decay=hp.decay)\n",
        "  losses, val_losses, accuracies, val_accuracies = train(model=runnable_model,\n",
        "                       dataset = dataset,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=hp.grad_clip)\n",
        "  if is_plot:\n",
        "    plot_stuff(hp.epochs, losses,val_losses, accuracies, val_accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  check_loader = dataset.test_loader\n",
        "  predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         check_loader,\n",
        "         hp)\n",
        "  return predicted_ys, runnable_model\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc_full}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2DLo4OoUO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc_full))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oMDNooNrMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "##RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": fact_dataset,\n",
        "    \"snopes\": snopes_dataset\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 10\n",
        "  inner_dropout = 0.5\n",
        "  outer_dropout = 0.5\n",
        "  C = 0.3\n",
        "  decay = 0\n",
        "  is_debug = True\n",
        "  lr=0.00007\n",
        "  grad_clip = True\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wRSAvtABCT",
        "colab_type": "text"
      },
      "source": [
        "##CROSS VALIDATION ZONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "7792c43d-4409-47ca-cdb2-3498070cb062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "NUM_OF_AVGS = 5\n",
        "avgs = []\n",
        "\"\"\"\n",
        "for i in range(NUM_OF_AVGS):\n",
        "  \n",
        "  \n",
        "  \n",
        "  avgs.append(results)\n",
        "\n",
        "print(process_results(list_to_dict(avgs)))\n",
        "\"\"\"\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters)\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([39093, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.648738, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 1.645830, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 1.652951, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 1.648662, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 1.642927, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 1.627337, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-cfcc529f0804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snopes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"politifact\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-183-1a3c21a92b33>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, dataset, hp, is_plot)\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        using_gradient_clipping=hp.grad_clip)\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_plot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplot_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-98fae21d5d5e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;31m#get y values - do forward pass and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpredicted_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mactual_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0msqueezed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-aeb8314fa739>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpremise_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremise_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothesis_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mhypothesis_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothesis_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-42611970826d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden_layer)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     return self.cool_lstm(embedding,\n\u001b[0;32m---> 26\u001b[0;31m                           hidden_layer)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JkXr8fcoee",
        "colab_type": "text"
      },
      "source": [
        "WOAH WERE DOIN SOME VALIDATION\n",
        "PLEASE VALIDATE ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu() ,datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zfw_4Acupe",
        "colab_type": "text"
      },
      "source": [
        "JUST TESTIN HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsngYK8wcvc8",
        "colab_type": "code",
        "outputId": "b6de5f47-bdb6-4a75-d8ae-a318e55cc82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "test_results= batch_wise_evaluate(text_model, datasets[\"politifact\"].test_loader, Hyperparameters)\n",
        "evaluation_summary(\"textual entailment test model\", test_results.cpu(), datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment test model\n",
            "Classifier 'textual entailment test model' has Acc=0.631 P=0.614 R=0.636 F1=0.606 AUC=0.614\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.417     0.648     0.507       836\n",
            "         1.0      0.811     0.624     0.705      2019\n",
            "\n",
            "    accuracy                          0.631      2855\n",
            "   macro avg      0.614     0.636     0.606      2855\n",
            "weighted avg      0.695     0.631     0.647      2855\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 542  759]\n",
            " [ 294 1260]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6137067120925691,\n",
              " 0.6361983406442623,\n",
              " 0.6311733800350263,\n",
              " 0.6062714155888395)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "##running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ngDR0NMc26u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 8\n",
        "  inner_dropout=0.5\n",
        "  outer_dropout=0.5\n",
        "  C = 0.3\n",
        "  is_debug = True\n",
        "  grad_clip = True\n",
        "  lr=0.00008\n",
        "  decay = 0\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "outputId": "b5bafa9e-3f4b-4b20-cfe7-15beab8af57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters)\n",
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters)\n"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.635040, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/23817 (4%)]\tLoss: 1.636341, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/23817 (8%)]\tLoss: 1.631181, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/23817 (13%)]\tLoss: 1.634871, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/23817 (17%)]\tLoss: 1.635460, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/23817 (21%)]\tLoss: 1.636772, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/23817 (25%)]\tLoss: 1.635798, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/23817 (29%)]\tLoss: 1.634752, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/23817 (34%)]\tLoss: 1.638183, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/23817 (38%)]\tLoss: 1.635914, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/23817 (42%)]\tLoss: 1.632843, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/23817 (46%)]\tLoss: 1.637441, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/23817 (50%)]\tLoss: 1.636854, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [13000/23817 (55%)]\tLoss: 1.633266, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [14000/23817 (59%)]\tLoss: 1.634763, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [15000/23817 (63%)]\tLoss: 1.632688, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [16000/23817 (67%)]\tLoss: 1.637544, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [17000/23817 (71%)]\tLoss: 1.635597, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [18000/23817 (76%)]\tLoss: 1.633024, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [19000/23817 (80%)]\tLoss: 1.634824, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [20000/23817 (84%)]\tLoss: 1.632168, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [21000/23817 (88%)]\tLoss: 1.636209, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [22000/23817 (92%)]\tLoss: 1.636809, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [23000/23817 (97%)]\tLoss: 1.634424, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6355, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5021008403361344\n",
            "losses: []\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.641284, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/2884 (4%)]\tLoss: 1.632435, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/2884 (7%)]\tLoss: 1.638824, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/2884 (11%)]\tLoss: 1.626627, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/2884 (14%)]\tLoss: 1.632434, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/2884 (18%)]\tLoss: 1.637758, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/2884 (21%)]\tLoss: 1.633460, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/2884 (25%)]\tLoss: 1.635622, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/2884 (29%)]\tLoss: 1.628488, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/2884 (32%)]\tLoss: 1.635166, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/2884 (36%)]\tLoss: 1.632764, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/2884 (39%)]\tLoss: 1.636215, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/2884 (43%)]\tLoss: 1.638937, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/2884 (46%)]\tLoss: 1.648429, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/2884 (50%)]\tLoss: 1.633061, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1500/2884 (54%)]\tLoss: 1.626774, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1600/2884 (57%)]\tLoss: 1.626939, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1700/2884 (61%)]\tLoss: 1.632592, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1800/2884 (64%)]\tLoss: 1.640186, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1900/2884 (68%)]\tLoss: 1.642217, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2000/2884 (71%)]\tLoss: 1.626991, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2100/2884 (75%)]\tLoss: 1.627661, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2200/2884 (79%)]\tLoss: 1.638692, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2300/2884 (82%)]\tLoss: 1.638984, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2400/2884 (86%)]\tLoss: 1.638433, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2500/2884 (89%)]\tLoss: 1.635859, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2600/2884 (93%)]\tLoss: 1.635641, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2700/2884 (96%)]\tLoss: 1.630860, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6348, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5189285714285714\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.636651, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/23817 (4%)]\tLoss: 1.633358, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/23817 (8%)]\tLoss: 1.638255, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/23817 (13%)]\tLoss: 1.634498, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/23817 (17%)]\tLoss: 1.636002, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/23817 (21%)]\tLoss: 1.635561, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/23817 (25%)]\tLoss: 1.634065, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/23817 (29%)]\tLoss: 1.629972, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/23817 (34%)]\tLoss: 1.633744, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/23817 (38%)]\tLoss: 1.633089, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/23817 (42%)]\tLoss: 1.634578, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/23817 (46%)]\tLoss: 1.634139, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/23817 (50%)]\tLoss: 1.630401, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [13000/23817 (55%)]\tLoss: 1.630582, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [14000/23817 (59%)]\tLoss: 1.629382, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [15000/23817 (63%)]\tLoss: 1.630776, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [16000/23817 (67%)]\tLoss: 1.631100, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [17000/23817 (71%)]\tLoss: 1.627160, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [18000/23817 (76%)]\tLoss: 1.632034, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [19000/23817 (80%)]\tLoss: 1.625464, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [20000/23817 (84%)]\tLoss: 1.625012, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [21000/23817 (88%)]\tLoss: 1.630499, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [22000/23817 (92%)]\tLoss: 1.625821, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [23000/23817 (97%)]\tLoss: 1.622067, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6307, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5083193277310925\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.638939, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/2884 (4%)]\tLoss: 1.621468, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/2884 (7%)]\tLoss: 1.636602, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/2884 (11%)]\tLoss: 1.606406, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/2884 (14%)]\tLoss: 1.626596, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/2884 (18%)]\tLoss: 1.632722, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/2884 (21%)]\tLoss: 1.621644, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/2884 (25%)]\tLoss: 1.622519, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/2884 (29%)]\tLoss: 1.623501, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/2884 (32%)]\tLoss: 1.627979, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/2884 (36%)]\tLoss: 1.625716, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/2884 (39%)]\tLoss: 1.633605, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/2884 (43%)]\tLoss: 1.642378, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/2884 (46%)]\tLoss: 1.648070, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/2884 (50%)]\tLoss: 1.624220, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1500/2884 (54%)]\tLoss: 1.609888, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1600/2884 (57%)]\tLoss: 1.613779, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1700/2884 (61%)]\tLoss: 1.614554, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1800/2884 (64%)]\tLoss: 1.633055, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1900/2884 (68%)]\tLoss: 1.647397, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2000/2884 (71%)]\tLoss: 1.621902, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2100/2884 (75%)]\tLoss: 1.619372, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2200/2884 (79%)]\tLoss: 1.630878, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2300/2884 (82%)]\tLoss: 1.633473, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2400/2884 (86%)]\tLoss: 1.638913, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2500/2884 (89%)]\tLoss: 1.625944, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2600/2884 (93%)]\tLoss: 1.627474, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2700/2884 (96%)]\tLoss: 1.629203, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6278, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5389285714285714\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.620982, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/23817 (4%)]\tLoss: 1.621655, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/23817 (8%)]\tLoss: 1.621985, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/23817 (13%)]\tLoss: 1.624671, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/23817 (17%)]\tLoss: 1.631699, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/23817 (21%)]\tLoss: 1.614120, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/23817 (25%)]\tLoss: 1.621079, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/23817 (29%)]\tLoss: 1.614675, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/23817 (34%)]\tLoss: 1.605479, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/23817 (38%)]\tLoss: 1.603994, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/23817 (42%)]\tLoss: 1.620335, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/23817 (46%)]\tLoss: 1.605854, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/23817 (50%)]\tLoss: 1.585920, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [13000/23817 (55%)]\tLoss: 1.600728, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [14000/23817 (59%)]\tLoss: 1.574892, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [15000/23817 (63%)]\tLoss: 1.594805, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [16000/23817 (67%)]\tLoss: 1.591573, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [17000/23817 (71%)]\tLoss: 1.603540, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [18000/23817 (76%)]\tLoss: 1.576426, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [19000/23817 (80%)]\tLoss: 1.568383, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [20000/23817 (84%)]\tLoss: 1.578692, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [21000/23817 (88%)]\tLoss: 1.572075, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [22000/23817 (92%)]\tLoss: 1.567165, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [23000/23817 (97%)]\tLoss: 1.560076, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5987, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5865126050420169\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.582807, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/2884 (4%)]\tLoss: 1.576378, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/2884 (7%)]\tLoss: 1.599268, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/2884 (11%)]\tLoss: 1.509066, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/2884 (14%)]\tLoss: 1.635257, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/2884 (18%)]\tLoss: 1.539154, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/2884 (21%)]\tLoss: 1.552410, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/2884 (25%)]\tLoss: 1.515483, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/2884 (29%)]\tLoss: 1.666204, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/2884 (32%)]\tLoss: 1.581010, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/2884 (36%)]\tLoss: 1.576847, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/2884 (39%)]\tLoss: 1.649539, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/2884 (43%)]\tLoss: 1.682218, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/2884 (46%)]\tLoss: 1.539406, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/2884 (50%)]\tLoss: 1.570944, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1500/2884 (54%)]\tLoss: 1.548830, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1600/2884 (57%)]\tLoss: 1.598105, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1700/2884 (61%)]\tLoss: 1.502231, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1800/2884 (64%)]\tLoss: 1.579944, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1900/2884 (68%)]\tLoss: 1.640492, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2000/2884 (71%)]\tLoss: 1.655697, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2100/2884 (75%)]\tLoss: 1.601437, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2200/2884 (79%)]\tLoss: 1.585794, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2300/2884 (82%)]\tLoss: 1.579125, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2400/2884 (86%)]\tLoss: 1.670060, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2500/2884 (89%)]\tLoss: 1.535186, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2600/2884 (93%)]\tLoss: 1.548404, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2700/2884 (96%)]\tLoss: 1.663979, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5888, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5828571428571429\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23817 (0%)]\tLoss: 1.587121, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/23817 (4%)]\tLoss: 1.588727, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/23817 (8%)]\tLoss: 1.567444, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/23817 (13%)]\tLoss: 1.573748, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/23817 (17%)]\tLoss: 1.568931, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/23817 (21%)]\tLoss: 1.580476, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/23817 (25%)]\tLoss: 1.578202, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/23817 (29%)]\tLoss: 1.561384, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/23817 (34%)]\tLoss: 1.546142, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/23817 (38%)]\tLoss: 1.568659, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/23817 (42%)]\tLoss: 1.566201, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/23817 (46%)]\tLoss: 1.572170, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/23817 (50%)]\tLoss: 1.549314, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [13000/23817 (55%)]\tLoss: 1.520142, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [14000/23817 (59%)]\tLoss: 1.551738, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [15000/23817 (63%)]\tLoss: 1.574543, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [16000/23817 (67%)]\tLoss: 1.569224, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [17000/23817 (71%)]\tLoss: 1.586743, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [18000/23817 (76%)]\tLoss: 1.580768, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [19000/23817 (80%)]\tLoss: 1.523413, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [20000/23817 (84%)]\tLoss: 1.542307, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [21000/23817 (88%)]\tLoss: 1.542419, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [22000/23817 (92%)]\tLoss: 1.557896, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [23000/23817 (97%)]\tLoss: 1.577634, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5542, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.620546218487395\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/2884 (0%)]\tLoss: 1.543098, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/2884 (4%)]\tLoss: 1.522942, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/2884 (7%)]\tLoss: 1.578222, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/2884 (11%)]\tLoss: 1.487544, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/2884 (14%)]\tLoss: 1.644368, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/2884 (18%)]\tLoss: 1.548471, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/2884 (21%)]\tLoss: 1.510616, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/2884 (25%)]\tLoss: 1.482618, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/2884 (29%)]\tLoss: 1.660732, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/2884 (32%)]\tLoss: 1.581355, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/2884 (36%)]\tLoss: 1.535294, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/2884 (39%)]\tLoss: 1.642098, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/2884 (43%)]\tLoss: 1.689349, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/2884 (46%)]\tLoss: 1.480885, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/2884 (50%)]\tLoss: 1.543463, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1500/2884 (54%)]\tLoss: 1.521274, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1600/2884 (57%)]\tLoss: 1.581228, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1700/2884 (61%)]\tLoss: 1.483842, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1800/2884 (64%)]\tLoss: 1.551816, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1900/2884 (68%)]\tLoss: 1.644177, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2000/2884 (71%)]\tLoss: 1.641683, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2100/2884 (75%)]\tLoss: 1.597219, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2200/2884 (79%)]\tLoss: 1.573054, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2300/2884 (82%)]\tLoss: 1.557816, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2400/2884 (86%)]\tLoss: 1.673238, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2500/2884 (89%)]\tLoss: 1.494285, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2600/2884 (93%)]\tLoss: 1.530595, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2700/2884 (96%)]\tLoss: 1.666189, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5703, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5917857142857142\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/23817 (0%)]\tLoss: 1.578452, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/23817 (4%)]\tLoss: 1.535462, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/23817 (8%)]\tLoss: 1.551169, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/23817 (13%)]\tLoss: 1.582961, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/23817 (17%)]\tLoss: 1.528820, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/23817 (21%)]\tLoss: 1.568256, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/23817 (25%)]\tLoss: 1.513899, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/23817 (29%)]\tLoss: 1.547069, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/23817 (34%)]\tLoss: 1.506572, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/23817 (38%)]\tLoss: 1.481163, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/23817 (42%)]\tLoss: 1.506012, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/23817 (46%)]\tLoss: 1.566818, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/23817 (50%)]\tLoss: 1.585438, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [13000/23817 (55%)]\tLoss: 1.520160, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [14000/23817 (59%)]\tLoss: 1.487221, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [15000/23817 (63%)]\tLoss: 1.491944, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [16000/23817 (67%)]\tLoss: 1.511607, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [17000/23817 (71%)]\tLoss: 1.480004, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [18000/23817 (76%)]\tLoss: 1.563763, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [19000/23817 (80%)]\tLoss: 1.522359, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [20000/23817 (84%)]\tLoss: 1.500537, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [21000/23817 (88%)]\tLoss: 1.547752, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [22000/23817 (92%)]\tLoss: 1.500022, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [23000/23817 (97%)]\tLoss: 1.493669, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5332, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6363865546218488\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/2884 (0%)]\tLoss: 1.516156, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/2884 (4%)]\tLoss: 1.514450, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/2884 (7%)]\tLoss: 1.553283, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/2884 (11%)]\tLoss: 1.475323, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/2884 (14%)]\tLoss: 1.650524, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/2884 (18%)]\tLoss: 1.522992, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/2884 (21%)]\tLoss: 1.466293, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/2884 (25%)]\tLoss: 1.482300, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/2884 (29%)]\tLoss: 1.648004, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/2884 (32%)]\tLoss: 1.568333, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/2884 (36%)]\tLoss: 1.509295, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/2884 (39%)]\tLoss: 1.651699, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/2884 (43%)]\tLoss: 1.716601, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/2884 (46%)]\tLoss: 1.429284, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/2884 (50%)]\tLoss: 1.500852, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1500/2884 (54%)]\tLoss: 1.519892, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1600/2884 (57%)]\tLoss: 1.604268, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1700/2884 (61%)]\tLoss: 1.466420, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1800/2884 (64%)]\tLoss: 1.552160, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1900/2884 (68%)]\tLoss: 1.640494, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2000/2884 (71%)]\tLoss: 1.637022, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2100/2884 (75%)]\tLoss: 1.630752, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2200/2884 (79%)]\tLoss: 1.566506, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2300/2884 (82%)]\tLoss: 1.566559, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2400/2884 (86%)]\tLoss: 1.703777, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2500/2884 (89%)]\tLoss: 1.468509, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2600/2884 (93%)]\tLoss: 1.528063, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2700/2884 (96%)]\tLoss: 1.671318, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5629, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6103571428571428\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/23817 (0%)]\tLoss: 1.542900, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/23817 (4%)]\tLoss: 1.563470, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/23817 (8%)]\tLoss: 1.552792, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/23817 (13%)]\tLoss: 1.548550, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/23817 (17%)]\tLoss: 1.528937, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/23817 (21%)]\tLoss: 1.544291, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/23817 (25%)]\tLoss: 1.565416, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/23817 (29%)]\tLoss: 1.591209, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/23817 (34%)]\tLoss: 1.505473, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/23817 (38%)]\tLoss: 1.509200, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/23817 (42%)]\tLoss: 1.496564, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/23817 (46%)]\tLoss: 1.503639, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/23817 (50%)]\tLoss: 1.534607, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [13000/23817 (55%)]\tLoss: 1.502362, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [14000/23817 (59%)]\tLoss: 1.528499, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [15000/23817 (63%)]\tLoss: 1.542815, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [16000/23817 (67%)]\tLoss: 1.493244, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [17000/23817 (71%)]\tLoss: 1.486562, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [18000/23817 (76%)]\tLoss: 1.507813, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [19000/23817 (80%)]\tLoss: 1.578372, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [20000/23817 (84%)]\tLoss: 1.544139, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [21000/23817 (88%)]\tLoss: 1.531944, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [22000/23817 (92%)]\tLoss: 1.462667, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [23000/23817 (97%)]\tLoss: 1.548849, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5156, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6531932773109244\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/2884 (0%)]\tLoss: 1.517541, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/2884 (4%)]\tLoss: 1.474854, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/2884 (7%)]\tLoss: 1.569889, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/2884 (11%)]\tLoss: 1.463639, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/2884 (14%)]\tLoss: 1.633835, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/2884 (18%)]\tLoss: 1.541250, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/2884 (21%)]\tLoss: 1.427755, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/2884 (25%)]\tLoss: 1.479936, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/2884 (29%)]\tLoss: 1.639947, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/2884 (32%)]\tLoss: 1.585257, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/2884 (36%)]\tLoss: 1.504177, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/2884 (39%)]\tLoss: 1.668761, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/2884 (43%)]\tLoss: 1.708445, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/2884 (46%)]\tLoss: 1.408802, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/2884 (50%)]\tLoss: 1.484500, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1500/2884 (54%)]\tLoss: 1.537730, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1600/2884 (57%)]\tLoss: 1.618701, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1700/2884 (61%)]\tLoss: 1.450139, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1800/2884 (64%)]\tLoss: 1.562175, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1900/2884 (68%)]\tLoss: 1.619664, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2000/2884 (71%)]\tLoss: 1.645276, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2100/2884 (75%)]\tLoss: 1.670304, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2200/2884 (79%)]\tLoss: 1.581960, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2300/2884 (82%)]\tLoss: 1.550662, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2400/2884 (86%)]\tLoss: 1.688025, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2500/2884 (89%)]\tLoss: 1.457093, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2600/2884 (93%)]\tLoss: 1.519289, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2700/2884 (96%)]\tLoss: 1.677897, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5603, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.615\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/23817 (0%)]\tLoss: 1.524993, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/23817 (4%)]\tLoss: 1.493712, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/23817 (8%)]\tLoss: 1.490648, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/23817 (13%)]\tLoss: 1.465686, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/23817 (17%)]\tLoss: 1.575511, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/23817 (21%)]\tLoss: 1.481722, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/23817 (25%)]\tLoss: 1.496617, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/23817 (29%)]\tLoss: 1.525703, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/23817 (34%)]\tLoss: 1.493689, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/23817 (38%)]\tLoss: 1.482394, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/23817 (42%)]\tLoss: 1.464542, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/23817 (46%)]\tLoss: 1.441012, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/23817 (50%)]\tLoss: 1.536699, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [13000/23817 (55%)]\tLoss: 1.466415, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [14000/23817 (59%)]\tLoss: 1.519974, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [15000/23817 (63%)]\tLoss: 1.468828, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [16000/23817 (67%)]\tLoss: 1.492384, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [17000/23817 (71%)]\tLoss: 1.524345, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [18000/23817 (76%)]\tLoss: 1.502909, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [19000/23817 (80%)]\tLoss: 1.473381, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [20000/23817 (84%)]\tLoss: 1.478107, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [21000/23817 (88%)]\tLoss: 1.491710, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [22000/23817 (92%)]\tLoss: 1.499304, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [23000/23817 (97%)]\tLoss: 1.515547, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.4970, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6671008403361345\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/2884 (0%)]\tLoss: 1.495033, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/2884 (4%)]\tLoss: 1.441044, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/2884 (7%)]\tLoss: 1.549486, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/2884 (11%)]\tLoss: 1.466627, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/2884 (14%)]\tLoss: 1.634473, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/2884 (18%)]\tLoss: 1.548737, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/2884 (21%)]\tLoss: 1.398990, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/2884 (25%)]\tLoss: 1.514108, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/2884 (29%)]\tLoss: 1.627218, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/2884 (32%)]\tLoss: 1.624032, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/2884 (36%)]\tLoss: 1.475735, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/2884 (39%)]\tLoss: 1.699050, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/2884 (43%)]\tLoss: 1.729660, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/2884 (46%)]\tLoss: 1.434673, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/2884 (50%)]\tLoss: 1.468608, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1500/2884 (54%)]\tLoss: 1.558070, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1600/2884 (57%)]\tLoss: 1.587002, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1700/2884 (61%)]\tLoss: 1.451245, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1800/2884 (64%)]\tLoss: 1.535527, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1900/2884 (68%)]\tLoss: 1.625401, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2000/2884 (71%)]\tLoss: 1.661654, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2100/2884 (75%)]\tLoss: 1.679966, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2200/2884 (79%)]\tLoss: 1.636259, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2300/2884 (82%)]\tLoss: 1.579775, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2400/2884 (86%)]\tLoss: 1.739586, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2500/2884 (89%)]\tLoss: 1.446624, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2600/2884 (93%)]\tLoss: 1.516229, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2700/2884 (96%)]\tLoss: 1.646398, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5633, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6135714285714285\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64), tensor(1.5633, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/23817 (0%)]\tLoss: 1.524544, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [1000/23817 (4%)]\tLoss: 1.481832, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [2000/23817 (8%)]\tLoss: 1.468990, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [3000/23817 (13%)]\tLoss: 1.512928, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [4000/23817 (17%)]\tLoss: 1.477186, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [5000/23817 (21%)]\tLoss: 1.546695, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [6000/23817 (25%)]\tLoss: 1.444508, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [7000/23817 (29%)]\tLoss: 1.427380, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [8000/23817 (34%)]\tLoss: 1.488049, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [9000/23817 (38%)]\tLoss: 1.495904, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [10000/23817 (42%)]\tLoss: 1.511340, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [11000/23817 (46%)]\tLoss: 1.522071, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [12000/23817 (50%)]\tLoss: 1.515147, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [13000/23817 (55%)]\tLoss: 1.454190, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [14000/23817 (59%)]\tLoss: 1.490942, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [15000/23817 (63%)]\tLoss: 1.539240, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [16000/23817 (67%)]\tLoss: 1.492200, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [17000/23817 (71%)]\tLoss: 1.492442, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [18000/23817 (76%)]\tLoss: 1.451821, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [19000/23817 (80%)]\tLoss: 1.391969, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [20000/23817 (84%)]\tLoss: 1.437224, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [21000/23817 (88%)]\tLoss: 1.484536, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [22000/23817 (92%)]\tLoss: 1.510754, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [23000/23817 (97%)]\tLoss: 1.478962, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.4760, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6840756302521008\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64), tensor(1.5633, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64), tensor(1.4760, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/2884 (0%)]\tLoss: 1.475102, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [100/2884 (4%)]\tLoss: 1.431083, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [200/2884 (7%)]\tLoss: 1.559200, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [300/2884 (11%)]\tLoss: 1.480770, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [400/2884 (14%)]\tLoss: 1.634205, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [500/2884 (18%)]\tLoss: 1.590686, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [600/2884 (21%)]\tLoss: 1.368303, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [700/2884 (25%)]\tLoss: 1.479216, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [800/2884 (29%)]\tLoss: 1.612908, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [900/2884 (32%)]\tLoss: 1.683690, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1000/2884 (36%)]\tLoss: 1.447002, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1100/2884 (39%)]\tLoss: 1.677390, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1200/2884 (43%)]\tLoss: 1.717632, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1300/2884 (46%)]\tLoss: 1.364109, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1400/2884 (50%)]\tLoss: 1.461842, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1500/2884 (54%)]\tLoss: 1.618379, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1600/2884 (57%)]\tLoss: 1.624082, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1700/2884 (61%)]\tLoss: 1.478874, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1800/2884 (64%)]\tLoss: 1.554178, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1900/2884 (68%)]\tLoss: 1.557442, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2000/2884 (71%)]\tLoss: 1.660386, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2100/2884 (75%)]\tLoss: 1.748695, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2200/2884 (79%)]\tLoss: 1.642878, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2300/2884 (82%)]\tLoss: 1.573568, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2400/2884 (86%)]\tLoss: 1.770727, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2500/2884 (89%)]\tLoss: 1.423732, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2600/2884 (93%)]\tLoss: 1.514365, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2700/2884 (96%)]\tLoss: 1.678647, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5653, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6089285714285714\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64), tensor(1.5633, device='cuda:0', dtype=torch.float64), tensor(1.5653, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64), tensor(1.4760, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.636064, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 1.635740, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 1.635554, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 1.635724, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 1.635692, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 1.635371, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/12119 (50%)]\tLoss: 1.635385, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/12119 (58%)]\tLoss: 1.635483, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/12119 (66%)]\tLoss: 1.636134, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/12119 (74%)]\tLoss: 1.635961, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/12119 (83%)]\tLoss: 1.636217, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/12119 (91%)]\tLoss: 1.635440, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/12119 (99%)]\tLoss: 1.636157, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6356, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.49950413223140494\n",
            "losses: []\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 1.635525, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/1507 (7%)]\tLoss: 1.636089, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/1507 (13%)]\tLoss: 1.636263, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/1507 (20%)]\tLoss: 1.635591, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/1507 (27%)]\tLoss: 1.636291, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/1507 (33%)]\tLoss: 1.635912, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/1507 (40%)]\tLoss: 1.636065, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/1507 (47%)]\tLoss: 1.635811, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/1507 (53%)]\tLoss: 1.636025, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/1507 (60%)]\tLoss: 1.635864, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/1507 (67%)]\tLoss: 1.635144, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/1507 (73%)]\tLoss: 1.635223, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/1507 (80%)]\tLoss: 1.635737, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/1507 (87%)]\tLoss: 1.636119, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/1507 (93%)]\tLoss: 1.635179, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6358, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.492\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 1.635339, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/12119 (8%)]\tLoss: 1.635906, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/12119 (17%)]\tLoss: 1.635473, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/12119 (25%)]\tLoss: 1.635295, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/12119 (33%)]\tLoss: 1.635977, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/12119 (41%)]\tLoss: 1.635797, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/12119 (50%)]\tLoss: 1.635579, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/12119 (58%)]\tLoss: 1.635222, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/12119 (66%)]\tLoss: 1.635440, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/12119 (74%)]\tLoss: 1.635627, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/12119 (83%)]\tLoss: 1.635152, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/12119 (91%)]\tLoss: 1.635089, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/12119 (99%)]\tLoss: 1.635788, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6355, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5108264462809917\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 1.636149, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/1507 (7%)]\tLoss: 1.636165, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/1507 (13%)]\tLoss: 1.635627, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/1507 (20%)]\tLoss: 1.635391, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/1507 (27%)]\tLoss: 1.636524, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/1507 (33%)]\tLoss: 1.635951, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/1507 (40%)]\tLoss: 1.635260, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/1507 (47%)]\tLoss: 1.636029, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/1507 (53%)]\tLoss: 1.636029, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/1507 (60%)]\tLoss: 1.635751, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/1507 (67%)]\tLoss: 1.634789, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/1507 (73%)]\tLoss: 1.634656, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/1507 (80%)]\tLoss: 1.635149, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/1507 (87%)]\tLoss: 1.634773, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/1507 (93%)]\tLoss: 1.634926, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6355, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.51\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 1.635578, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/12119 (8%)]\tLoss: 1.635271, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/12119 (17%)]\tLoss: 1.634756, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/12119 (25%)]\tLoss: 1.634731, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/12119 (33%)]\tLoss: 1.634836, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/12119 (41%)]\tLoss: 1.635210, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/12119 (50%)]\tLoss: 1.634758, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/12119 (58%)]\tLoss: 1.634702, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/12119 (66%)]\tLoss: 1.634403, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/12119 (74%)]\tLoss: 1.634168, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/12119 (83%)]\tLoss: 1.633776, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/12119 (91%)]\tLoss: 1.633058, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/12119 (99%)]\tLoss: 1.632936, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6347, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5132231404958678\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 1.634181, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/1507 (7%)]\tLoss: 1.634346, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/1507 (13%)]\tLoss: 1.634950, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/1507 (20%)]\tLoss: 1.634574, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/1507 (27%)]\tLoss: 1.635044, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/1507 (33%)]\tLoss: 1.633009, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/1507 (40%)]\tLoss: 1.634678, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/1507 (47%)]\tLoss: 1.634478, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/1507 (53%)]\tLoss: 1.634240, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/1507 (60%)]\tLoss: 1.634849, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/1507 (67%)]\tLoss: 1.633195, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/1507 (73%)]\tLoss: 1.634142, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/1507 (80%)]\tLoss: 1.633851, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/1507 (87%)]\tLoss: 1.634903, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/1507 (93%)]\tLoss: 1.633606, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6343, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5186666666666667\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/12119 (0%)]\tLoss: 1.633420, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/12119 (8%)]\tLoss: 1.633064, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/12119 (17%)]\tLoss: 1.632238, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/12119 (25%)]\tLoss: 1.631861, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/12119 (33%)]\tLoss: 1.631607, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/12119 (41%)]\tLoss: 1.631011, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/12119 (50%)]\tLoss: 1.630736, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/12119 (58%)]\tLoss: 1.629777, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/12119 (66%)]\tLoss: 1.628647, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/12119 (74%)]\tLoss: 1.628500, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/12119 (83%)]\tLoss: 1.628130, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/12119 (91%)]\tLoss: 1.627824, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/12119 (99%)]\tLoss: 1.625967, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6302, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5306611570247934\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/1507 (0%)]\tLoss: 1.634317, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/1507 (7%)]\tLoss: 1.634043, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/1507 (13%)]\tLoss: 1.634657, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/1507 (20%)]\tLoss: 1.633681, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/1507 (27%)]\tLoss: 1.637844, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/1507 (33%)]\tLoss: 1.631343, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/1507 (40%)]\tLoss: 1.632861, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/1507 (47%)]\tLoss: 1.634963, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/1507 (53%)]\tLoss: 1.636275, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/1507 (60%)]\tLoss: 1.632375, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/1507 (67%)]\tLoss: 1.626056, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/1507 (73%)]\tLoss: 1.627624, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/1507 (80%)]\tLoss: 1.629719, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/1507 (87%)]\tLoss: 1.627424, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/1507 (93%)]\tLoss: 1.627174, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6320, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.49733333333333335\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/12119 (0%)]\tLoss: 1.626647, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/12119 (8%)]\tLoss: 1.625217, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/12119 (17%)]\tLoss: 1.625997, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/12119 (25%)]\tLoss: 1.625190, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/12119 (33%)]\tLoss: 1.624165, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/12119 (41%)]\tLoss: 1.624708, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/12119 (50%)]\tLoss: 1.624027, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/12119 (58%)]\tLoss: 1.622714, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/12119 (66%)]\tLoss: 1.617902, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/12119 (74%)]\tLoss: 1.619067, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/12119 (83%)]\tLoss: 1.618708, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/12119 (91%)]\tLoss: 1.613890, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/12119 (99%)]\tLoss: 1.612713, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6216, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5297520661157025\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/1507 (0%)]\tLoss: 1.623064, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/1507 (7%)]\tLoss: 1.620902, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/1507 (13%)]\tLoss: 1.626905, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/1507 (20%)]\tLoss: 1.623484, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/1507 (27%)]\tLoss: 1.626971, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/1507 (33%)]\tLoss: 1.618639, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/1507 (40%)]\tLoss: 1.624861, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/1507 (47%)]\tLoss: 1.624162, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/1507 (53%)]\tLoss: 1.622435, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/1507 (60%)]\tLoss: 1.623495, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/1507 (67%)]\tLoss: 1.618512, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/1507 (73%)]\tLoss: 1.621763, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/1507 (80%)]\tLoss: 1.621145, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/1507 (87%)]\tLoss: 1.620411, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/1507 (93%)]\tLoss: 1.621257, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6225, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.56\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/12119 (0%)]\tLoss: 1.612772, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/12119 (8%)]\tLoss: 1.607184, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/12119 (17%)]\tLoss: 1.606167, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/12119 (25%)]\tLoss: 1.602604, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/12119 (33%)]\tLoss: 1.603064, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/12119 (41%)]\tLoss: 1.598748, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/12119 (50%)]\tLoss: 1.598401, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/12119 (58%)]\tLoss: 1.591024, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/12119 (66%)]\tLoss: 1.590137, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/12119 (74%)]\tLoss: 1.582976, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/12119 (83%)]\tLoss: 1.572386, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/12119 (91%)]\tLoss: 1.581548, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/12119 (99%)]\tLoss: 1.585395, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5958, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5750413223140496\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/1507 (0%)]\tLoss: 1.601754, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/1507 (7%)]\tLoss: 1.584058, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/1507 (13%)]\tLoss: 1.630426, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/1507 (20%)]\tLoss: 1.601797, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/1507 (27%)]\tLoss: 1.640542, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/1507 (33%)]\tLoss: 1.581077, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/1507 (40%)]\tLoss: 1.608676, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/1507 (47%)]\tLoss: 1.613073, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/1507 (53%)]\tLoss: 1.607928, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/1507 (60%)]\tLoss: 1.587898, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/1507 (67%)]\tLoss: 1.581525, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/1507 (73%)]\tLoss: 1.579834, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/1507 (80%)]\tLoss: 1.578470, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/1507 (87%)]\tLoss: 1.576381, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/1507 (93%)]\tLoss: 1.582323, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5971, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5966666666666667\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/12119 (0%)]\tLoss: 1.569945, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/12119 (8%)]\tLoss: 1.571486, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/12119 (17%)]\tLoss: 1.567821, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/12119 (25%)]\tLoss: 1.575180, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/12119 (33%)]\tLoss: 1.565251, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/12119 (41%)]\tLoss: 1.550450, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/12119 (50%)]\tLoss: 1.562456, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/12119 (58%)]\tLoss: 1.553054, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/12119 (66%)]\tLoss: 1.547018, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/12119 (74%)]\tLoss: 1.546705, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/12119 (83%)]\tLoss: 1.546238, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/12119 (91%)]\tLoss: 1.540497, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/12119 (99%)]\tLoss: 1.566315, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5572, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6447107438016529\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/1507 (0%)]\tLoss: 1.526728, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/1507 (7%)]\tLoss: 1.511388, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/1507 (13%)]\tLoss: 1.651672, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/1507 (20%)]\tLoss: 1.576657, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/1507 (27%)]\tLoss: 1.617264, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/1507 (33%)]\tLoss: 1.503359, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/1507 (40%)]\tLoss: 1.593126, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/1507 (47%)]\tLoss: 1.579722, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/1507 (53%)]\tLoss: 1.552129, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/1507 (60%)]\tLoss: 1.548448, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/1507 (67%)]\tLoss: 1.572192, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/1507 (73%)]\tLoss: 1.587989, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/1507 (80%)]\tLoss: 1.521265, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/1507 (87%)]\tLoss: 1.543996, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/1507 (93%)]\tLoss: 1.584008, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5647, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.632\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64), tensor(1.5647, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/12119 (0%)]\tLoss: 1.531014, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [1000/12119 (8%)]\tLoss: 1.533155, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [2000/12119 (17%)]\tLoss: 1.547877, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [3000/12119 (25%)]\tLoss: 1.512851, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [4000/12119 (33%)]\tLoss: 1.517492, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [5000/12119 (41%)]\tLoss: 1.552986, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [6000/12119 (50%)]\tLoss: 1.513241, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [7000/12119 (58%)]\tLoss: 1.529474, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [8000/12119 (66%)]\tLoss: 1.515810, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [9000/12119 (74%)]\tLoss: 1.521979, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [10000/12119 (83%)]\tLoss: 1.504715, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [11000/12119 (91%)]\tLoss: 1.485536, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [12000/12119 (99%)]\tLoss: 1.538966, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5238, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6713223140495868\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64), tensor(1.5647, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64), tensor(1.5238, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/1507 (0%)]\tLoss: 1.527882, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [100/1507 (7%)]\tLoss: 1.485338, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [200/1507 (13%)]\tLoss: 1.707263, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [300/1507 (20%)]\tLoss: 1.594489, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [400/1507 (27%)]\tLoss: 1.673835, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [500/1507 (33%)]\tLoss: 1.468600, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [600/1507 (40%)]\tLoss: 1.615026, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [700/1507 (47%)]\tLoss: 1.590585, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [800/1507 (53%)]\tLoss: 1.556100, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [900/1507 (60%)]\tLoss: 1.533276, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1000/1507 (67%)]\tLoss: 1.546457, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1100/1507 (73%)]\tLoss: 1.546198, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1200/1507 (80%)]\tLoss: 1.481157, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1300/1507 (87%)]\tLoss: 1.496178, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1400/1507 (93%)]\tLoss: 1.565735, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5592, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6246666666666667\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64), tensor(1.5647, device='cuda:0', dtype=torch.float64), tensor(1.5592, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64), tensor(1.5238, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAogAAAEbCAYAAABZU3XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIIqIiIhIEQVEERERESmigCgiIiIiRf4/WI9udoA9bxsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"sheena model\", sheena_predicted_ys.cpu(), datasets[\"politifact\"].val_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "##OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFA1PNpzA8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeclareParameters:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0\n",
        "  epochs = 30\n",
        "  C = 0\n",
        "  is_debug = True\n",
        "  lr=0.0002\n",
        "  decay = 0\n",
        "  grad_clip = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"real_declare\"], datasets[\"snopes\"], DeclareParameters)\n",
        "results = get_results(\"real_declare\", \"snopes\", predicted_ys.cpu(), datasets[\"snopes\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JetqyT7Imn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "declare_real_results, declare_predicted_ys =  run_model(models[\"real_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"real declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZugTi6AEHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_params = {\n",
        "    \"my_model\": Hyperparameters,\n",
        "    \"sheena_model\": SheenaParameters,\n",
        "    \"broke_declare\": DeclareParameters,\n",
        "    \"real_declare\": DeclareParameters\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWBCcIDk674v",
        "colab_type": "text"
      },
      "source": [
        "##VALIDATION LAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    some_results = []\n",
        "  \n",
        "    for per_avg_i in range(per_avg_amount):\n",
        "      predicted_ys, model = run_model(models[model_name], datasets[data_name], all_params[model_name])\n",
        "      full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "    processed_results.append(process_results(list_to_dict(some_results)))\n",
        "    print(processed_results)\n",
        "      #avg_results.append(get_avgs(some_results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for result in full_results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWgiTDHE2tP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}