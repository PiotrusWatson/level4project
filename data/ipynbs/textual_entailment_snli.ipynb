{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4NbGwIC9-z",
        "colab_type": "text"
      },
      "source": [
        "##HAHA ITS TIME TO SPEND 5 HOURS DOWNLOADING THINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "f006aef2-e373-4a13-c51c-6ff0f176f319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-29 00:05:24--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  67.3MB/s    in 1.3s    \n",
            "\n",
            "2020-03-29 00:05:25 (67.3 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "91d0834c-771f-462d-a1fe-0cb5af8b3acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-29 00:05:35--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-29 00:05:35--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-29 00:05:35--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.96MB/s    in 6m 27s  \n",
            "\n",
            "2020-03-29 00:12:02 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "a1e6a544-354f-4011-a1e5-886ef9872140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-29 00:12:33--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  3.95MB/s    in 1.2s    \n",
            "\n",
            "2020-03-29 00:12:35 (3.95 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "a507869b-927a-4d7c-e2a3-e5c397cbbce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "outputId": "584cd911-e6d2-46c7-c3a2-e52353ce2f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1-baseline.git\n",
        "import sys\n",
        "sys.path.insert(1, 'fnc-1-baseline/utils')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1-baseline'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "Receiving objects:   0% (1/121)   \rReceiving objects:   1% (2/121)   \rReceiving objects:   2% (3/121)   \rReceiving objects:   3% (4/121)   \rReceiving objects:   4% (5/121)   \rReceiving objects:   5% (7/121)   \rReceiving objects:   6% (8/121)   \rReceiving objects:   7% (9/121)   \rReceiving objects:   8% (10/121)   \rReceiving objects:   9% (11/121)   \rReceiving objects:  10% (13/121)   \rReceiving objects:  11% (14/121)   \rReceiving objects:  12% (15/121)   \rReceiving objects:  13% (16/121)   \rReceiving objects:  14% (17/121)   \rReceiving objects:  15% (19/121)   \rReceiving objects:  16% (20/121)   \rReceiving objects:  17% (21/121)   \rReceiving objects:  18% (22/121)   \rReceiving objects:  19% (23/121)   \rReceiving objects:  20% (25/121)   \rReceiving objects:  21% (26/121)   \rReceiving objects:  22% (27/121)   \rReceiving objects:  23% (28/121)   \rReceiving objects:  24% (30/121)   \rReceiving objects:  25% (31/121)   \rReceiving objects:  26% (32/121)   \rReceiving objects:  27% (33/121)   \rReceiving objects:  28% (34/121)   \rReceiving objects:  29% (36/121)   \rReceiving objects:  30% (37/121)   \rReceiving objects:  31% (38/121)   \rReceiving objects:  32% (39/121)   \rReceiving objects:  33% (40/121)   \rReceiving objects:  34% (42/121)   \rReceiving objects:  35% (43/121)   \rReceiving objects:  36% (44/121)   \rReceiving objects:  37% (45/121)   \rReceiving objects:  38% (46/121)   \rReceiving objects:  39% (48/121)   \rReceiving objects:  40% (49/121)   \rReceiving objects:  41% (50/121)   \rReceiving objects:  42% (51/121)   \rremote: Total 121 (delta 0), reused 0 (delta 0), pack-reused 121\u001b[K\n",
            "Receiving objects:  43% (53/121)   \rReceiving objects:  44% (54/121)   \rReceiving objects:  45% (55/121)   \rReceiving objects:  46% (56/121)   \rReceiving objects:  47% (57/121)   \rReceiving objects:  48% (59/121)   \rReceiving objects:  49% (60/121)   \rReceiving objects:  50% (61/121)   \rReceiving objects:  51% (62/121)   \rReceiving objects:  52% (63/121)   \rReceiving objects:  53% (65/121)   \rReceiving objects:  54% (66/121)   \rReceiving objects:  55% (67/121)   \rReceiving objects:  56% (68/121)   \rReceiving objects:  57% (69/121)   \rReceiving objects:  58% (71/121)   \rReceiving objects:  59% (72/121)   \rReceiving objects:  60% (73/121)   \rReceiving objects:  61% (74/121)   \rReceiving objects:  62% (76/121)   \rReceiving objects:  63% (77/121)   \rReceiving objects:  64% (78/121)   \rReceiving objects:  65% (79/121)   \rReceiving objects:  66% (80/121)   \rReceiving objects:  67% (82/121)   \rReceiving objects:  68% (83/121)   \rReceiving objects:  69% (84/121)   \rReceiving objects:  70% (85/121)   \rReceiving objects:  71% (86/121)   \rReceiving objects:  72% (88/121)   \rReceiving objects:  73% (89/121)   \rReceiving objects:  74% (90/121)   \rReceiving objects:  75% (91/121)   \rReceiving objects:  76% (92/121)   \rReceiving objects:  77% (94/121)   \rReceiving objects:  78% (95/121)   \rReceiving objects:  79% (96/121)   \rReceiving objects:  80% (97/121)   \rReceiving objects:  81% (99/121)   \rReceiving objects:  82% (100/121)   \rReceiving objects:  83% (101/121)   \rReceiving objects:  84% (102/121)   \rReceiving objects:  85% (103/121)   \rReceiving objects:  86% (105/121)   \rReceiving objects:  87% (106/121)   \rReceiving objects:  88% (107/121)   \rReceiving objects:  89% (108/121)   \rReceiving objects:  90% (109/121)   \rReceiving objects:  91% (111/121)   \rReceiving objects:  92% (112/121)   \rReceiving objects:  93% (113/121)   \rReceiving objects:  94% (114/121)   \rReceiving objects:  95% (115/121)   \rReceiving objects:  96% (117/121)   \rReceiving objects:  97% (118/121)   \rReceiving objects:  98% (119/121)   \rReceiving objects:  99% (120/121)   \rReceiving objects: 100% (121/121)   \rReceiving objects: 100% (121/121), 23.99 KiB | 446.00 KiB/s, done.\n",
            "Resolving deltas:   0% (0/63)   \rResolving deltas:   3% (2/63)   \rResolving deltas:   4% (3/63)   \rResolving deltas:   6% (4/63)   \rResolving deltas:  19% (12/63)   \rResolving deltas:  22% (14/63)   \rResolving deltas:  28% (18/63)   \rResolving deltas:  41% (26/63)   \rResolving deltas:  42% (27/63)   \rResolving deltas:  53% (34/63)   \rResolving deltas:  55% (35/63)   \rResolving deltas:  90% (57/63)   \rResolving deltas: 100% (63/63)   \rResolving deltas: 100% (63/63), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "f2913465-f828-4711-c82c-eb2218b9d023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-29 00:12:41--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  4.39MB/s    in 1.2s    \n",
            "\n",
            "2020-03-29 00:12:43 (4.39 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "outputId": "161354f3-3e45-4b7a-b090-d31d807220ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!pwd\n",
        "from score import report_score"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJXKPPODFqp",
        "colab_type": "text"
      },
      "source": [
        "##LOOK AT ALL THIS CODE TO IMPORT DATA GOD THERE MUST BE SOMETHING WRONG WITH ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "17c71df4-fcda-4c57-d0fd-d509cb4c39c2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "fe5dd70d-f9d1-4a71-f91e-8cfed1f902e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "9a2bca48-41d5-4f04-84be-bd7ebef2408e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "\n",
        "def split_test(facts):\n",
        "   unique = facts.drop_duplicates(\"claim_text\")\n",
        "   train_unique, val_unique = train_test_split(unique, test_size=0.1, random_state=8)\n",
        "   val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "   train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "   return train_facts, val_facts\n",
        "\n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/competition_test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/competition_test_stances.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "train_challenge, val_challenge = train_test_split(train_challenge, test_size=0.2, random_state=8)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "\n",
        "train_challenge = train_challenge.rename(columns={\"articleBody\": \"article\", \"Headline\": \"claim_text\", \"Stance\": \"cred_label\"})\n",
        "test_challenge = test_challenge.rename(columns={\"articleBody\": \"article\", \"Headline\": \"claim_text\", \"Stance\": \"cred_label\"})\n",
        "print(train_challenge.head())\n",
        "train_challenge, val_challenge = split_test(train_challenge)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Body ID  ... cred_label\n",
            "11142      686  ...          2\n",
            "4398       251  ...          2\n",
            "30345     1689  ...          2\n",
            "41235     2154  ...          1\n",
            "33237     1829  ...          2\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "challenge_mapping = {\"agree\": 0, \"disagree\": 1, \"discuss\": 2, \"unrelated\": 3}\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None, is_folding=False):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_text\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  \n",
        "  if is_folding:\n",
        "    results = []\n",
        "    folded = KFold(n_splits=10, shuffle=True)\n",
        "    splitted_object = folded.split(unique)\n",
        "    for train_result, test_result in splitted_object:\n",
        "      train_ilocs = unique.iloc[train_result][\"claim_text\"]\n",
        "      test_ilocs = unique.iloc[test_result][\"claim_text\"]\n",
        "      results.append((facts[facts[\"claim_text\"].isin(train_ilocs)], facts[facts[\"claim_text\"].isin(test_ilocs)]))\n",
        "\n",
        "    return results\n",
        "\n",
        "  train_unique, big_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "  val_unique, test_unique = train_test_split(big_unique, test_size=0.5, random_state=8)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_text\"].isin(test_unique[\"claim_text\"])]\n",
        "  val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "  train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "  return train_facts, test_facts, val_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts, val_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes, val_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "2f8b7ea3-3ca4-43d5-90e3-53dcdfda9a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>vice news we dont have a timeline on the decis...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>a schedule i narcotic along with heroin and ec...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>now do you think you were maybe talking just a...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>made to the new yorker that marijuana is no mo...</td>\n",
              "      <td>time.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jun_27_donald-trump_white-house-criticism...</td>\n",
              "      <td>obamacare signed law cbo estimated 23 million ...</td>\n",
              "      <td>donald trump</td>\n",
              "      <td>about the affordable health care act in its da...</td>\n",
              "      <td>eugeneweekly.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4849</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama to ban guns from 42 million social secur...</td>\n",
              "      <td>bearingarms.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama is looking to ban social security recipi...</td>\n",
              "      <td>rightwingnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4851</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>main navigation recent posts obama to ban 42 m...</td>\n",
              "      <td>downtrend.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4852</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>get news like this in your facebook news feed ...</td>\n",
              "      <td>thegatewaypundit.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>to prove everyone wrong yet again this time it...</td>\n",
              "      <td>zerohedge.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...        article_source\n",
              "187            1  ...            reason.com\n",
              "188            1  ...            reason.com\n",
              "189            1  ...               cnn.com\n",
              "190            1  ...              time.com\n",
              "526            1  ...      eugeneweekly.com\n",
              "...          ...  ...                   ...\n",
              "4849           0  ...       bearingarms.com\n",
              "4850           0  ...     rightwingnews.com\n",
              "4851           0  ...         downtrend.com\n",
              "4852           0  ...  thegatewaypundit.com\n",
              "4853           0  ...         zerohedge.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Beq65oTgcBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, val_loader, test_data, val_data, tokeniser, batch_size):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.test_data = test_data\n",
        "    self.val_data = val_data\n",
        "    self.batch_size = batch_size\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) \n",
        "    self.word_embeddings_large = load_glove_embeddings(\"glove.6B.300d.txt\", tokeniser.word_to_id, 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4lYd83j-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\"claim_text\", \"article\"]\n",
        "big_labels = [\"claim_text\", \"article\", \"article_source\"]\n",
        "\n",
        "def get_list(panda, labels):\n",
        "  label_to_data = {}\n",
        "  for label in labels:\n",
        "    label_to_data[label] = panda[label]\n",
        "\n",
        "  x_list = convert_to_lists(label_to_data)\n",
        "  if labels[0] == \"claim_text\":\n",
        "    y_list = panda[\"cred_label\"].tolist()\n",
        "  else:\n",
        "    y_list = panda[\"Stance\"].tolist()\n",
        "  return x_list, y_list\n",
        "\n",
        "def get_loader(x, y, vocab_size, max_length, batch_size, name, training=True, drop_last=True):\n",
        "  stuff = []\n",
        "  for key in x:\n",
        "    stuff.append(torch.from_numpy(x[key]).type(torch.LongTensor))\n",
        "  stuff.append(torch.from_numpy(y).type(torch.DoubleTensor))\n",
        "  tensorset = data_utils.TensorDataset(*stuff)\n",
        "  loader = data_utils.DataLoader(tensorset, batch_size=batch_size, drop_last=drop_last, shuffle=training)\n",
        "  loader.name = name\n",
        "\n",
        "  return loader\n",
        "\n",
        "  \n",
        "def get_dataset(train, test, val, vocab_size, max_length, batch_size, labels, name):\n",
        "  is_challenge = labels[0] == \"articleBody\"\n",
        "  train_list_x, train_list_y = get_list(train, labels)\n",
        "\n",
        "  test_list_x, test_list_y = get_list(test, labels)\n",
        "  val_list_x, val_list_y = get_list(val, labels)\n",
        "\n",
        "\n",
        "\n",
        "  #tokenising various stuff, setting up numpy dictionaries :)\n",
        "  tokeniser = Tokeniser(train_list_x, vocab_size, max_length)\n",
        "  x_train = tokeniser.do_everything(train_list_x)\n",
        "  x_test = tokeniser.do_everything(test_list_x)\n",
        "  x_val = tokeniser.do_everything(val_list_x)\n",
        "  y_train = np.array(train_list_y, dtype=np.float32)\n",
        "  y_test = np.array(test_list_y, dtype=np.float32)\n",
        "  y_val = np.array(val_list_y, dtype=np.float32)\n",
        "  \n",
        "  #datasets/loaders\n",
        "  train_loader = get_loader(x_train, y_train, vocab_size, max_length, batch_size, name, True)\n",
        "  test_loader = get_loader(x_test, y_test, vocab_size, max_length, batch_size, name, False, drop_last=False)\n",
        "  val_loader = get_loader(x_val, y_val, vocab_size, max_length, batch_size, name, False)\n",
        "  return Dataset(train_loader, test_loader, val_loader, y_test, y_val, tokeniser, batch_size)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MMrKlu6_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "outputId": "1020133d-3073-4007-92c6-64c1428affc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "\n",
        "snopes_dataset = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "fact_dataset = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "challenge_dataset = get_dataset(train_challenge, test_challenge, val_challenge, VOCAB_SIZE, MAX_LENGTH, 500, labels, \"challenge_data\")\n",
        "big_snopes = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "big_fact = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39093\n",
            "39093\n",
            "33766\n",
            "33766\n",
            "27708\n",
            "27708\n",
            "44623\n",
            "44623\n",
            "37174\n",
            "37174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "outputId": "175c5282-543d-4275-a6b4-8673504f78ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nx_train = x_tokeniser.do_everything(x_train_lists)\\nx_test = x_tokeniser.do_everything(x_test_lists)\\ny_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\\ny_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "outputId": "37fef5f8-aed6-407a-a7c0-285e273a3030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "\"\"\"we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "\"\"\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we_shufflin = True\\nshufflin_test = False\\n#alright lets tensordataset textual entailment stuff\\ntrain_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\\n                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\\n                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\\ntrain_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\\ntrain_loader.name = \"entailment_data\"\\n\\ntest_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\\n                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\\n                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\\ntest_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\\ntest_loader.name = \"entailment_data\"\\n\\n\\n#POLITIFACT/SNOPES W/ SOURCES\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    \n",
        "    self.embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.normaliser = torch.nn.BatchNorm1d(self.embedding_size)\n",
        "    if hp.is_lstm:\n",
        "      self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    else:\n",
        "      self.cool_lstm = torch.nn.GRU(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x[:, 0:self.hp.max_length])\n",
        "    #embedding = self.normaliser(embedding.transpose(1,2))\n",
        "    embedding = self.dropout(embedding)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    tanh_W_H = self.dropout(tanh_W_H)\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.premise_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "    self.hp = hp\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    premise_factor = self.premise_dropout(premise_factor)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    hypothesis_factor = self.hypothesis_dropout(hypothesis_factor)\n",
        "    if self.hp.use_better:\n",
        "      return better_mush(premise_factor,hypothesis_factor)\n",
        "    else:\n",
        "      return premise_factor * hypothesis_factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=hp.mlp_one)\n",
        "    self.linear2 = torch.nn.Linear(hp.mlp_one, hp.mlp_two)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(hp.mlp_two, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "      x = self.dropout1(x)\n",
        "    else:\n",
        "      x = torch.relu(self.linear1(x.reshape(self.hp.batch_size, -1)))\n",
        "      x = self.dropout1(x)\n",
        "      x = torch.relu(self.linear2(x))\n",
        "      x = self.dropout2(x)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    if self.hp.is_lstm:\n",
        "      return (hidden_state, cell_state)\n",
        "    else:\n",
        "      return hidden_state\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    print(word_embeddings.shape)\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout)]\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    self.dropouts[1](premise_embedding)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    self.dropouts[3](hypothesis_embedding)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    self.dropouts[4](factorised_mush)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention*premise_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWRHhOxCjFl",
        "colab_type": "text"
      },
      "source": [
        "##EVAL SUMMARY :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout)]*5\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    processed_premise = self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    premise_embedding = self.dropouts[1](premise_embedding)\n",
        "    \n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    processed_hypothesis = self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    hypothesis_embedding = self.dropouts[3](hypothesis_embedding)\n",
        "    if self.hp.use_better:\n",
        "      combined = better_mush(premise_embedding, hypothesis_embedding)\n",
        "    else:\n",
        "      combined = premise_embedding * hypothesis_embedding\n",
        "    \n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    avg = self.dropouts[4](avg)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      output = softmax(self.linear_final(avg))\n",
        "    else:\n",
        "      output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(10000, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 8)\n",
        "    self.linear_almost_there = torch.nn.Linear(8, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.dropout0 = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    lengths = self.hp.max_length - (premise == 0).sum(dim=1)\n",
        "    lengths = lengths.repeat(50, 1).transpose(0,1)\n",
        "    summed_embeddings = torch.sum(embeddings, 1)\n",
        "    mean_embeddings = torch.unsqueeze(summed_embeddings / lengths, 1) #change to accurate size of lenfgth\n",
        "    flattened_embeddings = mean_embeddings.reshape(self.hp.batch_size, -1)\n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100]).reshape(self.hp.batch_size, -1)\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((flattened_embeddings.repeat(1, 100), added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    attention_weights = softmax(torch.tanh(self.premise_linear(main_embeddings)))#TODO: turn into row vector\n",
        "\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, attention_weights.unsqueeze(2))\n",
        "    \n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    new_lengths = lengths.repeat(1, 2)\n",
        "\n",
        "    avg = torch.sum(combined, 1)/new_lengths #todo: fix padding\n",
        "    avg = self.dropout0(avg)\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    smaller = self.dropout1(smaller)\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    even_smaller = self.dropout2(even_smaller)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      output = softmax(self.linear_final(even_smaller))\n",
        "    else:\n",
        "      output = torch.sigmoid(self.linear_final(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "# given the losses, checks if it should stop (or not :))\n",
        "# this performs early stopping -\n",
        "def should_stop(losses, train_losses, limit, threshold=-0.01):\n",
        "  shouldnt_stop = False \n",
        "  is_learning = False\n",
        "  #print(\"losses:\",losses)\n",
        "  #print(\"train_losses:\", train_losses)\n",
        "  \n",
        "  if len(losses) < limit:\n",
        "    return False \n",
        "  \n",
        "  last = losses[-1]\n",
        "  last_train = train_losses[-1]\n",
        "\n",
        "  if limit == 2:\n",
        "    #print(\"Threshold: {}, result {}\".format(threshold, last-losses[-2]))\n",
        "    return last - losses[-2] >= threshold\n",
        "  \n",
        "  for i in range(-2,- limit-1,-1):\n",
        "    shouldnt_stop = (shouldnt_stop or last - losses[i] < threshold)\n",
        "    last = losses[i]\n",
        "  return not shouldnt_stop\n",
        "def get_l2(model, filters):\n",
        "  reg_loss = None\n",
        "  for param in model.parameters():\n",
        "    if param.shape[0] in filters:\n",
        "      if reg_loss is None:\n",
        "        reg_loss = torch.sum(param**2)\n",
        "      else:\n",
        "        reg_loss = reg_loss + param.norm(2)**2\n",
        "  return reg_loss\n",
        "\n",
        "def train(model=None, \n",
        "          dataset = None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(dataset.train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "  old_models = []\n",
        "  old_optimisers = []\n",
        "  \n",
        "  \n",
        "  is_binary = hp.num_classes == 1\n",
        "  is_validating = False\n",
        "  has_stopped = False\n",
        "  has_reset = False\n",
        "  has_decreased = False\n",
        "\n",
        "  if is_binary:\n",
        "    loss_function=torch.nn.BCELoss()\n",
        "  else:\n",
        "    loss_function=torch.nn.CrossEntropyLoss(hp.weights)\n",
        "  \n",
        "  if dataset.train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif dataset.train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  elif dataset.train_loader.name == \"challenge_data\" and hp.num_classes != 4:\n",
        "      raise ValueError(\"Four classes are needed for challenge fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    val_results = []\n",
        "    old_model = copy.deepcopy(model.state_dict())\n",
        "    old_optimiser = copy.deepcopy(optimiser.state_dict())\n",
        "    for i in range(2):\n",
        "\n",
        "      total_loss = 0\n",
        "      batch_count = 0\n",
        "      correct = 0\n",
        "      penal = 0\n",
        "      l2 = 0\n",
        "\n",
        "      if is_validating:\n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.val_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 1\n",
        "      elif has_stopped:\n",
        "        \n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.train_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 10\n",
        "      else:\n",
        "        model.train(True)\n",
        "        #print(\"im trainin friends!!\")\n",
        "        loader = dataset.train_loader\n",
        "        torch.enable_grad()\n",
        "        debug_amount = 10\n",
        "      \n",
        "      for batch_index, train_data in enumerate(loader):\n",
        "      #setting everything up\n",
        "        model.hidden_state = model.reset_for_testing(train_data[0].shape[0])\n",
        "        train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "        predicted_y, attention = model(*train_data[:-1])\n",
        "        actual_y = train_data[-1]\n",
        "        squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "        if hp.C > 0:\n",
        "          attentionT = attention.transpose(1,2)\n",
        "          identity = torch.eye(attention.size(1))\n",
        "          identity = Variable(identity.unsqueeze(0).expand(loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "          penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "        if hp.decay > 0:\n",
        "          l2 = get_l2(model, hp.filters)\n",
        "\n",
        "      #get loss, accuracy\n",
        "        if is_binary:\n",
        "          loss = loss_function(squeezed_y, actual_y.double())\n",
        "          loss += hp.C * penal/loader.batch_size + hp.decay * l2\n",
        "          correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "          if is_validating:\n",
        "            val_results.append(torch.round(squeezed_y))\n",
        "          \n",
        "        else:\n",
        "          loss = loss_function(squeezed_y,actual_y.long())\n",
        "          loss += hp.C * (penal/loader.batch_size) + hp.decay * l2\n",
        "          correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "          if is_validating:\n",
        "            val_results.append(torch.argmax(squeezed_y, 1))\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if using_gradient_clipping:\n",
        "          torch.nn.utils.clip_grad_norm(model.parameters(), hp.grad_clip_amount)\n",
        "      \n",
        "      #cleaning up regularisation\n",
        "        if hp.C > 0:\n",
        "          del(penal)\n",
        "          del(identity)\n",
        "          del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "        optimiser.zero_grad()\n",
        "      #makin sure we dont learn if validation/early stopping is on\n",
        "        if not is_validating and not has_stopped:\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "        \n",
        "      #PRINT ZONE BABY\n",
        "        if hp.is_debug and batch_index % debug_amount == 0:\n",
        "          print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, ISvAL: {}, HasStopped: {}\".format(\n",
        "              epoch, batch_index * len(train_data[0]), len(loader.dataset),\n",
        "              100. * batch_index / len(loader), loss.item(), is_validating, has_stopped\n",
        "          ))\n",
        "\n",
        "        \n",
        "        batch_count += 1\n",
        "      \n",
        "        free_data(train_data)\n",
        "      \n",
        "      correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "      accuracy = correct_but_numpy / float(batch_count * loader.batch_size)\n",
        "\n",
        "      #either recording validation loss or not depending :)\n",
        "      if (not is_validating):\n",
        "        losses.append(total_loss/batch_count)\n",
        "        accuracies.append(accuracy)\n",
        "      else:\n",
        "        val_losses.append(total_loss/batch_count)\n",
        "        val_accuracies.append(accuracy)\n",
        "      \n",
        "      #oh boy lets deal with some crap\n",
        "      if not has_decreased:\n",
        "        has_decreased = not (should_stop(val_losses, losses, 2, hp.early_threshold)) and len(val_losses) > 1\n",
        "      if hp.use_early_stopping and not has_stopped:\n",
        "        has_stopped = should_stop(val_losses,losses, hp.early_stopping, hp.early_threshold) and has_decreased\n",
        "        if has_stopped and not has_reset:\n",
        "          print(\"resetting model HERE\")\n",
        "          model.load_state_dict(old_model)\n",
        "          optimiser.load_state_dict(old_optimiser)\n",
        "          model.eval()\n",
        "          #losses[-1] = losses[-2]\n",
        "          #val_losses[-1] = val_losses[-2]\n",
        "          has_reset = True\n",
        "          \n",
        "      print(\"Average loss is:\",total_loss/batch_count, \"while validation_status:\", is_validating, \"and stopping_status\", has_stopped)\n",
        "      print(\"Accuracy of the model\", accuracy)\n",
        "     # print(\"batch count???\", batch_count)\n",
        "\n",
        "\n",
        "\n",
        "      is_validating = not is_validating\n",
        "  return losses, val_losses, accuracies, val_accuracies, torch.cat(val_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, val_losses, accuracies, val_accuracies, title=\"sup nerds\"):\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  \n",
        "  ax1.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Train Accuracy\")\n",
        "  ax1.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  ax1.plot(range(1, epochs+1), val_accuracies, scalex=True, scaley=True, label=\"Val Accuracy\")\n",
        "  ax1.annotate(str(val_accuracies[-1]), xy=(epochs,val_accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  ax2.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Train Loss\")\n",
        "  ax2.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  ax2.plot(range(1, epochs+1), val_losses,scalex=True, scaley=True, label=\"Val Loss\")\n",
        "  ax2.annotate(str(val_losses[-1]), xy=(epochs,val_losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  ax1.legend()\n",
        "  ax2.legend()\n",
        "\n",
        "  ax1.set_xlabel(\"Epochs\", fontsize=16)\n",
        "  ax1.set_ylabel(\"Accuracy\", fontsize=16)\n",
        "  ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
        "  ax2.set_ylabel(\"Loss\", fontsize=16)\n",
        "  plt.title(title)\n",
        "  fig.tight_layout()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2DLo4OoUO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels, is_binary):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "  if (is_binary):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "    challenge_score = 0\n",
        "  else:\n",
        "    auc_full = 0\n",
        "    challenge_score = report_score([LABELS[int(e)] for e in true_labels], [LABELS[int(e)] for e in predictions])\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f Chal=%0.3f\" % (description,accuracy,precision,recall,f1, auc_full, challenge_score))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp, is_plot=True):\n",
        "  if hp.big_gloves:\n",
        "    runnable_model = model(hp, dataset.word_embeddings_large).cuda()\n",
        "  else:\n",
        "    runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "  if hp.num_classes == 1:\n",
        "    optimiser = torch.optim.Adam(runnable_model.parameters(), lr=hp.lr)\n",
        "  else:\n",
        "    optimiser = torch.optim.Adagrad(runnable_model.parameters(), lr=hp.lr)\n",
        "  losses, val_losses, accuracies, val_accuracies, val_results = train(model=runnable_model,\n",
        "                       dataset = dataset,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=hp.grad_clip)\n",
        "  if is_plot:\n",
        "    plot_stuff(hp.epochs, losses,val_losses, accuracies, val_accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "  evaluation_summary(\"VALIDATION\", val_results.cpu().detach() ,dataset.val_data[:len(val_results)], hp.num_classes==1)\n",
        "  check_loader = dataset.test_loader\n",
        "\n",
        "  predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         check_loader,\n",
        "         hp)\n",
        "\n",
        "  return predicted_ys, runnable_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc_full}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oMDNooNrMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "##RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": fact_dataset,\n",
        "    \"snopes\": snopes_dataset,\n",
        "    \"challenge\": challenge_dataset\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    #\"my_model_better\": TextualEntailmentModel,\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    #\"sheena_model_better\": BaselineSentenceEntailment,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1ykUhFf7BCH",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  is_lstm = False\n",
        "  batch_size = datasets[\"snopes\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 20\n",
        "  mlp_one = 50\n",
        "  mlp_two = 25\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 8\n",
        "  inner_dropout = 0.5\n",
        "  outer_dropout = 0.5\n",
        "  C = 2\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.0001\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 1\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.00005\n",
        "  use_better=True\n",
        "  big_gloves=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3DuwQLD_iln",
        "colab_type": "code",
        "outputId": "678b3d81-5e9e-4e75-d4e2-31fb4291db42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters_snopes)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([39093, 300])\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:140: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss is: tensor(7.0191, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5008264462809917\n",
            "Average loss is: tensor(7.0243, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.442\n",
            "Running EPOCH: 2\n",
            "Average loss is: tensor(7.0186, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.500495867768595\n",
            "Average loss is: tensor(7.0217, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.442\n",
            "Running EPOCH: 3\n",
            "Average loss is: tensor(7.0180, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.5019834710743801\n",
            "Average loss is: tensor(7.0182, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.442\n",
            "Running EPOCH: 4\n",
            "Average loss is: tensor(7.0037, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.5736363636363636\n",
            "Average loss is: tensor(6.9411, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.6626666666666666\n",
            "Running EPOCH: 5\n",
            "Average loss is: tensor(6.8626, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.7557024793388429\n",
            "resetting model HERE\n",
            "Average loss is: tensor(6.9892, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6633333333333333\n",
            "Running EPOCH: 6\n",
            "Average loss is: tensor(6.9178, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.6955371900826446\n",
            "Average loss is: tensor(6.9411, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6626666666666666\n",
            "Running EPOCH: 7\n",
            "Average loss is: tensor(6.9176, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.6955371900826446\n",
            "Average loss is: tensor(6.9411, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6626666666666666\n",
            "Running EPOCH: 8\n",
            "Average loss is: tensor(6.9175, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.6958677685950413\n",
            "Average loss is: tensor(6.9411, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6626666666666666\n",
            "Evaluation for: VALIDATION\n",
            "Classifier 'VALIDATION' has Acc=0.663 P=0.644 R=0.662 F1=0.643 AUC=0.644 Chal=0.000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.802     0.664     0.726      1011\n",
            "         1.0      0.487     0.661     0.561       489\n",
            "\n",
            "    accuracy                          0.663      1500\n",
            "   macro avg      0.644     0.662     0.643      1500\n",
            "weighted avg      0.699     0.663     0.672      1500\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[671 166]\n",
            " [340 323]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Tight layout not applied. tight_layout cannot make axes width small enough to accommodate all axes decorations\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApYAAAEbCAYAAABgNMSRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfdUujkbvIT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Njt1FiPx6FfO",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_challenge:\n",
        "  lstm_hidden_size = 100\n",
        "  dense_dimension = 200\n",
        "  attention_hops = 30\n",
        "  is_lstm = False\n",
        "  batch_size = datasets[\"challenge\"].batch_size\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 200\n",
        "  mlp_one = 100\n",
        "  mlp_two = 25\n",
        "  num_classes = 4\n",
        "  avg=False\n",
        "  weights = torch.tensor([2, 1, 0.2, 1], dtype=torch.double).cuda()\n",
        "  epochs = 14\n",
        "  inner_dropout = 0.1\n",
        "  outer_dropout = 0.3\n",
        "  C = 0.3\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.01\n",
        "  grad_clip = False\n",
        "  grad_clip_amount = 1\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.0005\n",
        "  use_better=True\n",
        "  big_gloves=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYJ_FyWx36Sr",
        "colab_type": "code",
        "outputId": "257d3f32-bb43-4a74-be55-5ca9ae4a8533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"challenge\"], Hyperparameters_challenge)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([27708, 300])\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-e012c4c42f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"challenge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters_challenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-45aa3db07545>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, dataset, hp, is_plot)\u001b[0m\n\u001b[1;32m     12\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                        using_gradient_clipping=hp.grad_clip)\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_plot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplot_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-77ac2e0a5561>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0;31m#get y values - do forward pass and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mpredicted_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mactual_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0msqueezed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9f00b1662b7d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprocessed_premise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpremise_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremise_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-0661d3992b7a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden_layer)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     return self.cool_lstm(embedding,\n\u001b[0;32m---> 35\u001b[0;31m                           hidden_layer)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 716\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 15.90 GiB total capacity; 13.19 GiB already allocated; 695.62 MiB free; 14.43 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdlajIzawdWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "5424a990-c574-416c-9234-4fc13671cef3"
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu() ,datasets[\"challenge\"].test_data, False)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment model\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |    71     |    301    |    221    |    104    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |    155    |   3053    |    866    |    390    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |    672    |   5253    |   10805   |   1619    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |    191    |    760    |    427    |    525    |\n",
            "-------------------------------------------------------------\n",
            "Score: 15927.25 out of 23985.75\t(66.4029684291715%)\n",
            "Classifier 'textual entailment model' has Acc=0.569 P=0.413 R=0.367 F1=0.364 AUC=0.000 Chal=66.403\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.102     0.065     0.080      1089\n",
            "           1      0.684     0.326     0.441      9367\n",
            "           2      0.589     0.877     0.705     12319\n",
            "           3      0.276     0.199     0.231      2638\n",
            "\n",
            "    accuracy                          0.569     25413\n",
            "   macro avg      0.413     0.367     0.364     25413\n",
            "weighted avg      0.571     0.569     0.532     25413\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   71   301   221   104]\n",
            " [  155  3053   866   390]\n",
            " [  672  5253 10805  1619]\n",
            " [  191   760   427   525]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.41263038111098654,\n",
              " 0.3668109272986052,\n",
              " 0.5687640184157715,\n",
              " 0.36421230320303527)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_politi:\n",
        "  lstm_hidden_size = 30\n",
        "  dense_dimension = 10\n",
        "  attention_hops = 5\n",
        "  batch_size = datasets[\"politifact\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 10\n",
        "  mlp_one = 50\n",
        "  mlp_two = 25\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 8\n",
        "  inner_dropout = 0.2\n",
        "  outer_dropout = 0.5\n",
        "  C = 0.5\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  is_lstm=True\n",
        "  lr=0.00008\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 1\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.004\n",
        "  use_better=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXeIuPBZF8Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wRSAvtABCT",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL_ENTAILMENT W/ BETTERMUSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no2oQo2CM3XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_b_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"snopes\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 10\n",
        "  mlp_one = 50\n",
        "  mlp_two = 25\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 11\n",
        "  inner_dropout = 0.3\n",
        "  outer_dropout = 0.6\n",
        "  C = 0.6\n",
        "  is_lstm=True\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.00004\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 2\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.01\n",
        "  use_better=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq0ID4OwKBnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters_b_snopes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uR6B4eAE7Ydh",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_b_politi:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"politifact\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 8\n",
        "  mlp_one = 50  \n",
        "  mlp_two = 25\n",
        "  num_classes = 1\n",
        "  is_lstm=True\n",
        "  avg=False\n",
        "  epochs = 8\n",
        "  inner_dropout = 0.2\n",
        "  outer_dropout = 0.7\n",
        "  weights = [1, 1, 0.25, 1]\n",
        "  C = 0.6\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.00007\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 2\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.009\n",
        "  use_better=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0_fGUTfKLKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters_b_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JkXr8fcoee",
        "colab_type": "text"
      },
      "source": [
        "WOAH WERE DOIN SOME VALIDATION\n",
        "PLEASE VALIDATE ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OgdGdgfwsOxa",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu() ,datasets[\"politifact\"].test_data, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zfw_4Acupe",
        "colab_type": "text"
      },
      "source": [
        "JUST TESTIN HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "##running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ngDR0NMc26u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"snopes\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 9\n",
        "  inner_dropout=0.5\n",
        "  outer_dropout=0.5\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  is_lstm=True\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 2\n",
        "  lr=0.00007\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.009\n",
        "  use_better = False\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Sy7yz2yfOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters_snopes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqq85WyvO4vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_politi:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"politifact\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 11\n",
        "  inner_dropout=0.3\n",
        "  outer_dropout=0.6\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 1\n",
        "  lr=0.00005\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  use_better = False\n",
        "  early_threshold = -0.0005\n",
        "  is_lstm=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters_politi)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLIae_QSsycn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yHnhc8pmszYg",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_challenge:\n",
        "  lstm_hidden_size = 100\n",
        "  dense_dimension = 200\n",
        "  attention_hops = 30\n",
        "  batch_size = datasets[\"challenge\"].batch_size\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 70\n",
        "  num_classes = 4\n",
        "  avg=False\n",
        "  weights = torch.tensor([2, 1, 0.2, 1], dtype=torch.double).cuda()\n",
        "  epochs = 12\n",
        "  inner_dropout = 0\n",
        "  outer_dropout = 0\n",
        "  C = 0\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.01\n",
        "  is_lstm = True\n",
        "  grad_clip = False\n",
        "  grad_clip_amount = 1\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.0005\n",
        "  use_better=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFF5cXaOs5cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"challenge\"], SheenaParameters_challenge)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LavaeLb_wcPS",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_b_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"snopes\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  mlp_one = 25  \n",
        "  mlp_two = 10\n",
        "  avg=False\n",
        "  epochs = 14\n",
        "  inner_dropout=0.3\n",
        "  outer_dropout=0.6\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 1\n",
        "  lr=0.00005\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.00013\n",
        "  use_better = True\n",
        "  is_lstm = True\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1qVR4CrMGJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters_b_snopes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Czr397iowaXX",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_b_politi:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"politifact\"].batch_size\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 8\n",
        "  inner_dropout=0.3\n",
        "  outer_dropout=0.5\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  grad_clip = True\n",
        "  grad_clip_amount=1\n",
        "  lr=0.0001\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  use_better = True\n",
        "  early_threshold = -0.013\n",
        "  is_lstm = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--XfnpT3wZ9c",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters_b_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"sheena model\",predicted_ys.cpu(), datasets[\"politifact\"].test_data, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "##OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFA1PNpzA8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeclareParameters_snopes:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"snopes\"].batch_size\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0.5\n",
        "  epochs = 20\n",
        "  C = 0\n",
        "  is_debug = False\n",
        "  lr=0.0001\n",
        "  decay = 0.01\n",
        "  filters = [datasets[\"snopes\"].batch_size, 100, 8]\n",
        "  grad_clip = False\n",
        "  grad_clip_amount = 3\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.0004\n",
        "  is_lstm = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"real_declare\"], datasets[\"snopes\"], DeclareParameters_snopes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1c6Az3UST7bY",
        "colab": {}
      },
      "source": [
        "class DeclareParameters_politi:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"politifact\"].batch_size\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0.5\n",
        "  epochs = 20\n",
        "  C = 0\n",
        "  is_debug = False\n",
        "  lr=0.0001\n",
        "  decay = 0.01\n",
        "  filters = [datasets[\"politifact\"].batch_size, 100, 8]\n",
        "  grad_clip = False\n",
        "  grad_clip_amount = 3\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.0004\n",
        "  is_lstm = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_model(models[\"real_declare\"], datasets[\"politifact\"], DeclareParameters_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8ocKDLu25FX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeclareParameters_challenge:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = datasets[\"challenge\"].batch_size\n",
        "  max_length = 100\n",
        "  num_classes = 4\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0.5\n",
        "  epochs = 20\n",
        "  C = 0\n",
        "  is_debug = False\n",
        "  lr=0.01\n",
        "  decay = 0.01\n",
        "  weights = torch.tensor([2, 1, 0.2, 1], dtype=torch.double).cuda()\n",
        "  filters = [datasets[\"politifact\"].batch_size, 100, 8]\n",
        "  grad_clip = False\n",
        "  grad_clip_amount = 3\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.0004\n",
        "  is_lstm = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skSB-tvD3IgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, _ = run_model(models[\"real_declare\"], datasets[\"challenge\"], DeclareParameters_challenge)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"real declare model\", predicted_ys.cpu(), datasets[\"challenge\"].test_data, False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZugTi6AEHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_params = {\n",
        "    \"my_model\": {\"politifact\": Hyperparameters_politi, \"snopes\":Hyperparameters_snopes, \"challenge\":Hyperparameters_challenge},\n",
        "    #\"my_model_better\":{\"politifact\": Hyperparameters_b_politi, \"snopes\":Hyperparameters_b_snopes},\n",
        "    \"sheena_model\": {\"politifact\": SheenaParameters_politi, \"snopes\": SheenaParameters_snopes, \"challenge\": SheenaParameters_challenge},\n",
        "    #\"sheena_model_better\": {\"politifact\": SheenaParameters_b_politi, \"snopes\" : SheenaParameters_b_snopes},\n",
        "    \"real_declare\": {\"politifact\":DeclareParameters_politi, \"snopes\": DeclareParameters_snopes, \"challenge\": DeclareParameters_challenge}\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWBCcIDk674v",
        "colab_type": "text"
      },
      "source": [
        "##VALIDATION LAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    some_results = []\n",
        "  \n",
        "    for per_avg_i in range(per_avg_amount):\n",
        "      predicted_ys, model = run_model(models[model_name], datasets[data_name], all_params[model_name][data_name])\n",
        "      full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "    processed_results.append(process_results(list_to_dict(some_results)))\n",
        "    print(processed_results)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for result in full_results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImnSu2nFmgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}