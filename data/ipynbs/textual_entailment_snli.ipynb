{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "d082310d-4739-4613-98ef-a2b1576472c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-14 13:33:00--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  70.1MB/s    in 1.3s    \n",
            "\n",
            "2020-01-14 13:33:02 (70.1 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "a39b3333-5593-4937-9f93-7fd80c70c192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-14 13:33:11--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-01-14 13:33:11--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-01-14 13:33:11--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.11MB/s    in 6m 27s  \n",
            "\n",
            "2020-01-14 13:39:38 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "55b6e323-0441-46bb-9cb1-d16e00f77eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-14 13:40:09--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  3.94MB/s    in 1.2s    \n",
            "\n",
            "2020-01-14 13:40:11 (3.94 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "19702680-6c54-4fdd-bb61-a8141b834276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "8c33cd80-392a-45d9-adb0-0f887fbe3c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-14 13:40:17--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  4.32MB/s    in 1.2s    \n",
            "\n",
            "2020-01-14 13:40:19 (4.32 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "outputId": "1d42fd80-2b0e-4fb1-9216-9b34ba5a1118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "17cd0895-b639-48f7-b1c2-b629c77f5bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "bc53ed82-03ad-4d0d-ab30-8b06d9720f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ... Stance\n",
              "0        0  ...      2\n",
              "1        0  ...      2\n",
              "2        0  ...      2\n",
              "3        0  ...      2\n",
              "4        0  ...      2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "outputId": "d3fafc1f-9b6c-4434-fd3c-af0315910c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_id\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  train_unique, test_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_id\"].isin(test_unique[\"claim_id\"])]\n",
        "  train_facts = facts[facts[\"claim_id\"].isin(train_unique[\"claim_id\"])]\n",
        "  return train_facts, test_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n",
        "\n",
        "train_facts.head(50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>for firms moving overseas in order to create a...</td>\n",
              "      <td>foxnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>get a tax break specifically by outsourcing jo...</td>\n",
              "      <td>newslines.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>confusing clashes over taxes in wednesday s pr...</td>\n",
              "      <td>wsj.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>support on this bill in a time of tight budget...</td>\n",
              "      <td>senate.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>tax a lower rate for american manufacturing an...</td>\n",
              "      <td>archives.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>jobs in america and reduce the federal budget ...</td>\n",
              "      <td>senate.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>obamas debate claims on tax writeoffs for offs...</td>\n",
              "      <td>huffingtonpost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>joseph crowley the bronx one of nine chief dep...</td>\n",
              "      <td>house.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the senate budget committee bernie issued an o...</td>\n",
              "      <td>feelthebern.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>other than becoming a fulltime radical and tha...</td>\n",
              "      <td>wikipedia.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the years i ve repeatedly made clear that we w...</td>\n",
              "      <td>archives.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>bin laden declares war on musharraf in new aud...</td>\n",
              "      <td>foxnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>dear reader please upgrade to the latest versi...</td>\n",
              "      <td>dawn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>posted by joe on sep 20 2007 in at tmv 6 comme...</td>\n",
              "      <td>themoderatevoice.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>accessibility links saturday 16 december 2017 ...</td>\n",
              "      <td>telegraph.co.uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>news middle east tuesday 19 december 2017 bin ...</td>\n",
              "      <td>independent.ie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>that we would take action within pakistan if w...</td>\n",
              "      <td>go.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>pakistan at that very moment the man who had d...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>has won most popular conservatism cant survive...</td>\n",
              "      <td>theatlantic.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>after 911 why didnt the us declare war on paki...</td>\n",
              "      <td>quora.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>behind this week s attacks on the united state...</td>\n",
              "      <td>badgerherald.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the saudi dissident opposed to washington s co...</td>\n",
              "      <td>independent.co.uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>to enjoy the full mail guardian online experie...</td>\n",
              "      <td>mg.co.za</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the saudi arabian peninsula following the gulf...</td>\n",
              "      <td>pbs.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>col john discusses the impact of osama bin lad...</td>\n",
              "      <td>usip.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>osama bin laden declares war on pakistan most ...</td>\n",
              "      <td>theatlantic.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>his supporters questioned whether he had actua...</td>\n",
              "      <td>nytimes.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>years i have repeatedly made clear that we wou...</td>\n",
              "      <td>huffingtonpost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>afraid of the american threats against me he s...</td>\n",
              "      <td>theguardian.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>news middle east tuesday 19 december 2017 bin ...</td>\n",
              "      <td>independent.ie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>this copy is for your personal noncommercial u...</td>\n",
              "      <td>thestar.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>things must come to pass but the end is not ye...</td>\n",
              "      <td>rlhymersjr.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>we hope youll especially enjoy fba items quali...</td>\n",
              "      <td>amazon.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>requested increase eight senators courageously...</td>\n",
              "      <td>delawareonline.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>trending newsletter trump s plan for increased...</td>\n",
              "      <td>panampost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>state of the union address the president most ...</td>\n",
              "      <td>duke.edu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>us spends more on military than the next eight...</td>\n",
              "      <td>wordpress.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>according to energy information administration...</td>\n",
              "      <td>factcheck.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>factors that frustrate direct budget compariso...</td>\n",
              "      <td>heritage.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>rachel and thomas over at the heritage foundat...</td>\n",
              "      <td>nationalreview.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>fearless muckraking since 1993 profits for the...</td>\n",
              "      <td>counterpunch.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>over the years even as the number of soldiers ...</td>\n",
              "      <td>washingtonpost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>and the resources they need to confront the th...</td>\n",
              "      <td>cnbc.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>that this commonplace is simplistic inaccurate...</td>\n",
              "      <td>heritage.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the most powerful nation on earth period he sa...</td>\n",
              "      <td>nationalreview.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>search form why does donald trump insist on mo...</td>\n",
              "      <td>commondreams.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>why does donald trump insist on more military ...</td>\n",
              "      <td>nationofchange.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>doubt that it still matters massively who occu...</td>\n",
              "      <td>theguardian.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>2013_jan_11_ileana-ros-lehtinen_ros-lehtinen-s...</td>\n",
              "      <td>says chuck hagel opposed sanctions iran</td>\n",
              "      <td>ileana ros-lehtinen</td>\n",
              "      <td>the president s war cabinet chuck hagel s misg...</td>\n",
              "      <td>algemeiner.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1</td>\n",
              "      <td>2013_jan_11_ileana-ros-lehtinen_ros-lehtinen-s...</td>\n",
              "      <td>says chuck hagel opposed sanctions iran</td>\n",
              "      <td>ileana ros-lehtinen</td>\n",
              "      <td>of note this week because the president appear...</td>\n",
              "      <td>cfr.org</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    cred_label  ...        article_source\n",
              "0            1  ...           foxnews.com\n",
              "1            1  ...         newslines.org\n",
              "2            1  ...               wsj.com\n",
              "3            1  ...            senate.gov\n",
              "4            1  ...          archives.gov\n",
              "5            1  ...            senate.gov\n",
              "6            1  ...    huffingtonpost.com\n",
              "7            1  ...             house.gov\n",
              "8            1  ...       feelthebern.org\n",
              "9            1  ...         wikipedia.org\n",
              "10           1  ...          archives.gov\n",
              "11           1  ...           foxnews.com\n",
              "12           1  ...              dawn.com\n",
              "13           1  ...  themoderatevoice.com\n",
              "14           1  ...       telegraph.co.uk\n",
              "15           1  ...        independent.ie\n",
              "16           1  ...                go.com\n",
              "17           1  ...               cnn.com\n",
              "18           1  ...       theatlantic.com\n",
              "19           1  ...             quora.com\n",
              "20           1  ...      badgerherald.com\n",
              "21           1  ...     independent.co.uk\n",
              "22           1  ...              mg.co.za\n",
              "23           1  ...               pbs.org\n",
              "24           1  ...              usip.org\n",
              "25           1  ...       theatlantic.com\n",
              "26           1  ...           nytimes.com\n",
              "27           1  ...    huffingtonpost.com\n",
              "28           1  ...       theguardian.com\n",
              "29           1  ...        independent.ie\n",
              "30           1  ...           thestar.com\n",
              "31           1  ...        rlhymersjr.com\n",
              "32           1  ...            amazon.com\n",
              "33           1  ...    delawareonline.com\n",
              "34           1  ...         panampost.com\n",
              "35           1  ...              duke.edu\n",
              "36           1  ...         wordpress.com\n",
              "37           1  ...         factcheck.org\n",
              "38           1  ...          heritage.org\n",
              "39           1  ...    nationalreview.com\n",
              "40           1  ...      counterpunch.org\n",
              "41           1  ...    washingtonpost.com\n",
              "42           1  ...              cnbc.com\n",
              "43           1  ...          heritage.org\n",
              "44           1  ...    nationalreview.com\n",
              "45           1  ...      commondreams.org\n",
              "46           1  ...    nationofchange.org\n",
              "47           1  ...       theguardian.com\n",
              "48           1  ...        algemeiner.com\n",
              "49           1  ...               cfr.org\n",
              "\n",
              "[50 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 256\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_fact_list = convert_to_lists({\"claim_text\": train_facts[\"claim_text\"], \n",
        "                   \"claim_source\": train_facts[\"claim_source\"],\n",
        "                   \"article\": train_facts[\"article\"],\n",
        "                   \"article_source\": train_facts[\"article_source\"]})\n",
        "y_train_fact_list = train_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_fact_list = convert_to_lists({\"claim_text\": test_facts[\"claim_text\"], \n",
        "                   \"claim_source\": test_facts[\"claim_source\"],\n",
        "                   \"article\": test_facts[\"article\"],\n",
        "                   \"article_source\": test_facts[\"article_source\"]})\n",
        "y_test_fact_list = test_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "x_train_snopes_list = convert_to_lists({\"claim_text\": train_snopes[\"claim_text\"],\n",
        "                   \"article\": train_snopes[\"article\"],\n",
        "                   \"article_source\": train_snopes[\"article_source\"]})\n",
        "x_test_snopes_list = convert_to_lists({\"claim_text\": test_snopes[\"claim_text\"],\n",
        "                   \"article\": test_snopes[\"article\"],\n",
        "                   \"article_source\": test_snopes[\"article_source\"]})\n",
        "y_train_snopes_list = train_snopes[\"cred_label\"].tolist()\n",
        "y_test_snopes_list = test_snopes[\"cred_label\"].tolist()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "fact_tokeniser = Tokeniser(x_train_fact_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "snopes_tokeniser = Tokeniser(x_train_snopes_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_fact_train = fact_tokeniser.do_everything(x_train_fact_list)\n",
        "x_fact_test = fact_tokeniser.do_everything(x_test_fact_list)\n",
        "y_fact_train = np.array(y_train_fact_list, dtype=np.float32)\n",
        "y_fact_test = np.array(y_test_fact_list, dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n",
        "x_snopes_train = snopes_tokeniser.do_everything(x_train_snopes_list)\n",
        "x_snopes_test = snopes_tokeniser.do_everything(x_test_snopes_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "train_fact_source_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True)\n",
        "train_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_source_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False)\n",
        "test_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "\n",
        "train_fact_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True)\n",
        "train_fact_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False)\n",
        "test_fact_loader.name = \"fact_data\"\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    \n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=20)\n",
        "    self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding).reshape(self.hp.batch_size, -1)\n",
        "    return self.MLP(factorised_mush), better_mush(premise_attention, hypothesis_attention)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IVrtgcddpSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "  def forward(self, premise, hypothesis):\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    outputs, hidden_state = self.premise_lstm(reshaped_embeddings)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(outputs)\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    \n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-skRc_EBRhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, unnormalised_predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  auc = roc_auc_score(true_labels, unnormalised_predictions)\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          train_loader=None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  is_binary = hp.num_classes == 1\n",
        "  \n",
        "  if train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    correct = 0\n",
        "    penal = 0\n",
        "    for batch_index, train_data in enumerate(train_loader):\n",
        "      #setting everything up\n",
        "      model.hidden_state = model.init_hidden()\n",
        "      train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "      predicted_y, attention = model(*train_data[:-1])\n",
        "      actual_y = train_data[-1]\n",
        "      squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "      if hp.C > 0:\n",
        "        attentionT = attention.transpose(1,2)\n",
        "        identity = torch.eye(attention.size(1))\n",
        "        identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "        penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "      if is_binary:\n",
        "        loss = loss_function(squeezed_y, actual_y.double())\n",
        "        loss += hp.C * penal/train_loader.batch_size\n",
        "        correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "      else:\n",
        "        loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/train_loader.batch_size)\n",
        "        correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "      total_loss += loss.data\n",
        "\n",
        "      #cleaning up regularisation\n",
        "      if hp.C > 0:\n",
        "        del(penal)\n",
        "        del(identity)\n",
        "        del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "      optimiser.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      if hp.is_debug and batch_index % 10 == 0:\n",
        "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "            epoch, batch_index * len(train_data[0]), len(train_loader.dataset),\n",
        "            100. * batch_index / len(train_loader), loss.item()\n",
        "        ))\n",
        "\n",
        "      if using_gradient_clipping:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      batch_count += 1\n",
        "      optimiser.step()\n",
        "      free_data(train_data)\n",
        "\n",
        "    print(\"Average loss is:\",total_loss/batch_count)\n",
        "    correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "    accuracy = correct_but_numpy / float(batch_count * train_loader.batch_size)\n",
        "    print(\"Accuracy of the model\", accuracy)\n",
        "    losses.append(total_loss/batch_count)\n",
        "    accuracies.append(accuracy)\n",
        "  return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(real_results, 0), torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, accuracies=None, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  if accuracies:\n",
        "    plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Accuracy\")\n",
        "    plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "OK WE MADE THE HELPERS LETS RUN THIS SHIT :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "outputId": "7219450e-765d-469e-904c-80c1720f3296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "glove_embeddings = load_glove_embeddings(\"glove.6B.300d.txt\",x_tokeniser.word_to_id,300)\n",
        "print(glove_embeddings.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34370\n",
            "torch.Size([34370, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvFn7PUSd742",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(128)\n",
        "textual_entailment_model = None\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 150\n",
        "  dense_dimension = 100\n",
        "  attention_hops = 30\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 50\n",
        "  num_classes = 1\n",
        "  epochs = 10\n",
        "  C = 0.1\n",
        "  is_debug = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "412392a6-6274-4d83-c5f8-55fb5416bd08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "if(textual_entailment_model):\n",
        "  del(textual_entailment_model)\n",
        "  torch.cuda.empty_cache()\n",
        "pass\n",
        "textual_entailment_model = TextualEntailmentModel(Hyperparameters, glove_embeddings).cuda()\n",
        "#textual_entailment_model.to(device)\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(textual_entailment_model.parameters(), \n",
        "                                    lr=0.01)\n",
        "\n",
        "loss, accuracy = train(model=textual_entailment_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)\n",
        "\n",
        "print(torch.cuda.memory_allocated)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.250607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.245215\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.240040\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.241304\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.247876\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.233073\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.240776\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.243668\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.241636\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.238903\n",
            "Average loss is: tensor(1.2782, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.49793956043956045\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.239866\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.238318\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.876305\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.240554\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.073162\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.472199\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.401610\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.624851\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.494104\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.598943\n",
            "Average loss is: tensor(1.3222, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5599673763736264\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.475889\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.947141\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.097921\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.240949\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 3.438519\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.163736\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.282582\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.588299\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.058582\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.207684\n",
            "Average loss is: tensor(1.2818, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.644273695054945\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23454 (0%)]\tLoss: 1.205446\n",
            "Train Epoch: 3 [2560/23454 (11%)]\tLoss: 0.812985\n",
            "Train Epoch: 3 [5120/23454 (22%)]\tLoss: 0.742363\n",
            "Train Epoch: 3 [7680/23454 (33%)]\tLoss: 1.254724\n",
            "Train Epoch: 3 [10240/23454 (44%)]\tLoss: 1.206382\n",
            "Train Epoch: 3 [12800/23454 (55%)]\tLoss: 1.328190\n",
            "Train Epoch: 3 [15360/23454 (66%)]\tLoss: 2.651936\n",
            "Train Epoch: 3 [17920/23454 (77%)]\tLoss: 1.072532\n",
            "Train Epoch: 3 [20480/23454 (88%)]\tLoss: 0.657934\n",
            "Train Epoch: 3 [23040/23454 (99%)]\tLoss: 1.122805\n",
            "Average loss is: tensor(1.1474, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7226991758241759\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/23454 (0%)]\tLoss: 1.141037\n",
            "Train Epoch: 4 [2560/23454 (11%)]\tLoss: 0.579376\n",
            "Train Epoch: 4 [5120/23454 (22%)]\tLoss: 0.648919\n",
            "Train Epoch: 4 [7680/23454 (33%)]\tLoss: 1.412849\n",
            "Train Epoch: 4 [10240/23454 (44%)]\tLoss: 1.099572\n",
            "Train Epoch: 4 [12800/23454 (55%)]\tLoss: 1.836431\n",
            "Train Epoch: 4 [15360/23454 (66%)]\tLoss: 1.300542\n",
            "Train Epoch: 4 [17920/23454 (77%)]\tLoss: 1.691131\n",
            "Train Epoch: 4 [20480/23454 (88%)]\tLoss: 0.679089\n",
            "Train Epoch: 4 [23040/23454 (99%)]\tLoss: 1.016049\n",
            "Average loss is: tensor(1.0900, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.811770260989011\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/23454 (0%)]\tLoss: 0.874871\n",
            "Train Epoch: 5 [2560/23454 (11%)]\tLoss: 0.565250\n",
            "Train Epoch: 5 [5120/23454 (22%)]\tLoss: 2.001862\n",
            "Train Epoch: 5 [7680/23454 (33%)]\tLoss: 1.704784\n",
            "Train Epoch: 5 [10240/23454 (44%)]\tLoss: 0.987228\n",
            "Train Epoch: 5 [12800/23454 (55%)]\tLoss: 0.706544\n",
            "Train Epoch: 5 [15360/23454 (66%)]\tLoss: 1.022158\n",
            "Train Epoch: 5 [17920/23454 (77%)]\tLoss: 0.832927\n",
            "Train Epoch: 5 [20480/23454 (88%)]\tLoss: 0.710978\n",
            "Train Epoch: 5 [23040/23454 (99%)]\tLoss: 0.694567\n",
            "Average loss is: tensor(0.9244, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8638392857142857\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/23454 (0%)]\tLoss: 0.656110\n",
            "Train Epoch: 6 [2560/23454 (11%)]\tLoss: 0.556819\n",
            "Train Epoch: 6 [5120/23454 (22%)]\tLoss: 1.047645\n",
            "Train Epoch: 6 [7680/23454 (33%)]\tLoss: 4.004717\n",
            "Train Epoch: 6 [10240/23454 (44%)]\tLoss: 0.655348\n",
            "Train Epoch: 6 [12800/23454 (55%)]\tLoss: 0.652975\n",
            "Train Epoch: 6 [15360/23454 (66%)]\tLoss: 0.755481\n",
            "Train Epoch: 6 [17920/23454 (77%)]\tLoss: 0.908139\n",
            "Train Epoch: 6 [20480/23454 (88%)]\tLoss: 0.729557\n",
            "Train Epoch: 6 [23040/23454 (99%)]\tLoss: 0.649220\n",
            "Average loss is: tensor(0.8459, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9159083104395604\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/23454 (0%)]\tLoss: 0.600980\n",
            "Train Epoch: 7 [2560/23454 (11%)]\tLoss: 0.587556\n",
            "Train Epoch: 7 [5120/23454 (22%)]\tLoss: 0.593752\n",
            "Train Epoch: 7 [7680/23454 (33%)]\tLoss: 2.491362\n",
            "Train Epoch: 7 [10240/23454 (44%)]\tLoss: 0.697819\n",
            "Train Epoch: 7 [12800/23454 (55%)]\tLoss: 0.781417\n",
            "Train Epoch: 7 [15360/23454 (66%)]\tLoss: 0.932809\n",
            "Train Epoch: 7 [17920/23454 (77%)]\tLoss: 1.482322\n",
            "Train Epoch: 7 [20480/23454 (88%)]\tLoss: 0.613600\n",
            "Train Epoch: 7 [23040/23454 (99%)]\tLoss: 0.553796\n",
            "Average loss is: tensor(0.9141, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9037603021978022\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/23454 (0%)]\tLoss: 0.616116\n",
            "Train Epoch: 8 [2560/23454 (11%)]\tLoss: 0.600008\n",
            "Train Epoch: 8 [5120/23454 (22%)]\tLoss: 0.579022\n",
            "Train Epoch: 8 [7680/23454 (33%)]\tLoss: 3.411426\n",
            "Train Epoch: 8 [10240/23454 (44%)]\tLoss: 1.546700\n",
            "Train Epoch: 8 [12800/23454 (55%)]\tLoss: 0.665034\n",
            "Train Epoch: 8 [15360/23454 (66%)]\tLoss: 1.604276\n",
            "Train Epoch: 8 [17920/23454 (77%)]\tLoss: 0.919734\n",
            "Train Epoch: 8 [20480/23454 (88%)]\tLoss: 0.566615\n",
            "Train Epoch: 8 [23040/23454 (99%)]\tLoss: 0.704880\n",
            "Average loss is: tensor(0.8798, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9124313186813187\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/23454 (0%)]\tLoss: 0.608857\n",
            "Train Epoch: 9 [2560/23454 (11%)]\tLoss: 0.563763\n",
            "Train Epoch: 9 [5120/23454 (22%)]\tLoss: 0.575108\n",
            "Train Epoch: 9 [7680/23454 (33%)]\tLoss: 2.939168\n",
            "Train Epoch: 9 [10240/23454 (44%)]\tLoss: 4.893164\n",
            "Train Epoch: 9 [12800/23454 (55%)]\tLoss: 0.578190\n",
            "Train Epoch: 9 [15360/23454 (66%)]\tLoss: 0.881231\n",
            "Train Epoch: 9 [17920/23454 (77%)]\tLoss: 0.628091\n",
            "Train Epoch: 9 [20480/23454 (88%)]\tLoss: 0.548772\n",
            "Train Epoch: 9 [23040/23454 (99%)]\tLoss: 0.577380\n",
            "Average loss is: tensor(0.9364, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9066792582417582\n",
            "<function memory_allocated at 0x7fd2fa97abf8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "outputId": "e2263cf0-05d3-40f9-f30a-e875f8fc4e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, loss, accuracy)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "outputId": "87eecec0-e08f-4d33-c0f2-cbe8f0675686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "real_results, predicted_ys = batch_wise_evaluate(textual_entailment_model, \n",
        "         test_fact_loader,\n",
        "         Hyperparameters)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "outputId": "4de79442-3fed-423b-8f5b-f3e79b185c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu(), real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment model\n",
            "Classifier 'textual entailment model' has Acc=0.596 P=0.590 R=0.600 F1=0.583 AUC=0.629\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.434     0.610     0.507      2079\n",
            "         1.0      0.745     0.589     0.658      4023\n",
            "\n",
            "    accuracy                          0.596      6102\n",
            "   macro avg      0.590     0.600     0.583      6102\n",
            "weighted avg      0.639     0.596     0.607      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1268 1653]\n",
            " [ 811 2370]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5895733192447755,\n",
              " 0.5995106062220157,\n",
              " 0.5961979678793838,\n",
              " 0.5825838978345363)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9tLOcRYOc9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}