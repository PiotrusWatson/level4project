{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4NbGwIC9-z",
        "colab_type": "text"
      },
      "source": [
        "##HAHA ITS TIME TO SPEND 5 HOURS DOWNLOADING THINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "7045bfcc-049c-4d67-df02-60a0f54fca8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 21:16:28--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  8.43MB/s    in 11s     \n",
            "\n",
            "2020-02-12 21:16:39 (8.58 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "8846be02-1144-4706-bc97-2ceaa45d4345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 21:16:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-12 21:16:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-12 21:16:50--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.05MB/s    in 6m 29s  \n",
            "\n",
            "2020-02-12 21:23:20 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "799b2e66-a14d-48f1-a096-e0f440c44810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 21:23:46--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  2.16MB/s    in 2.2s    \n",
            "\n",
            "2020-02-12 21:23:49 (2.16 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "e7e1ff92-eacd-4861-e7c2-37440bbb570a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "9929cb07-cb26-4022-e4f9-48fc84e31f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 21:23:59--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  2.43MB/s    in 2.2s    \n",
            "\n",
            "2020-02-12 21:24:02 (2.43 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "outputId": "db3d4e4a-e08a-4910-f71c-38be531a6b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJXKPPODFqp",
        "colab_type": "text"
      },
      "source": [
        "##LOOK AT ALL THIS CODE TO IMPORT DATA GOD THERE MUST BE SOMETHING WRONG WITH ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "30886642-eda5-40ce-9fc1-3fed36f42e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 366
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "d97690a6-4ef2-48c3-a127-041627a4cd92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()"
      ],
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ... Stance\n",
              "0        0  ...      2\n",
              "1        0  ...      2\n",
              "2        0  ...      2\n",
              "3        0  ...      2\n",
              "4        0  ...      2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 367
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None, is_folding=False):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_text\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  \n",
        "  if is_folding:\n",
        "    results = []\n",
        "    folded = KFold(n_splits=10, shuffle=True)\n",
        "    splitted_object = folded.split(unique)\n",
        "    for train_result, test_result in splitted_object:\n",
        "      train_ilocs = unique.iloc[train_result][\"claim_text\"]\n",
        "      test_ilocs = unique.iloc[test_result][\"claim_text\"]\n",
        "      results.append((facts[facts[\"claim_text\"].isin(train_ilocs)], facts[facts[\"claim_text\"].isin(test_ilocs)]))\n",
        "\n",
        "    return results\n",
        "\n",
        "  train_unique, big_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "  val_unique, test_unique = train_test_split(big_unique, test_size=0.5, random_state=8)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_text\"].isin(test_unique[\"claim_text\"])]\n",
        "  val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "  train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "  return train_facts, test_facts, val_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts, val_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes, val_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "d6e90488-24af-4257-b747-b2bd1465ff11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>vice news we dont have a timeline on the decis...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>a schedule i narcotic along with heroin and ec...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>now do you think you were maybe talking just a...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>made to the new yorker that marijuana is no mo...</td>\n",
              "      <td>time.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jun_27_donald-trump_white-house-criticism...</td>\n",
              "      <td>obamacare signed law cbo estimated 23 million ...</td>\n",
              "      <td>donald trump</td>\n",
              "      <td>about the affordable health care act in its da...</td>\n",
              "      <td>eugeneweekly.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4849</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama to ban guns from 42 million social secur...</td>\n",
              "      <td>bearingarms.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama is looking to ban social security recipi...</td>\n",
              "      <td>rightwingnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4851</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>main navigation recent posts obama to ban 42 m...</td>\n",
              "      <td>downtrend.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4852</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>get news like this in your facebook news feed ...</td>\n",
              "      <td>thegatewaypundit.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>to prove everyone wrong yet again this time it...</td>\n",
              "      <td>zerohedge.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...        article_source\n",
              "187            1  ...            reason.com\n",
              "188            1  ...            reason.com\n",
              "189            1  ...               cnn.com\n",
              "190            1  ...              time.com\n",
              "526            1  ...      eugeneweekly.com\n",
              "...          ...  ...                   ...\n",
              "4849           0  ...       bearingarms.com\n",
              "4850           0  ...     rightwingnews.com\n",
              "4851           0  ...         downtrend.com\n",
              "4852           0  ...  thegatewaypundit.com\n",
              "4853           0  ...         zerohedge.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 369
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Beq65oTgcBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, val_loader, test_data, val_data, tokeniser):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.test_data = test_data\n",
        "    self.val_data = val_data\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4lYd83j-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\"claim_text\", \"article\"]\n",
        "big_labels = [\"claim_text\", \"article\", \"article_source\"]\n",
        "\n",
        "def get_list(panda, labels):\n",
        "  label_to_data = {}\n",
        "  for label in labels:\n",
        "    label_to_data[label] = panda[label]\n",
        "  \n",
        "  x_list = convert_to_lists(label_to_data)\n",
        "  y_list = panda[\"cred_label\"].tolist()\n",
        "  return x_list, y_list\n",
        "\n",
        "def get_loader(x, y, vocab_size, max_length, batch_size, name, training=True, drop_last=True):\n",
        "  stuff = []\n",
        "  for key in x:\n",
        "    stuff.append(torch.from_numpy(x[key]).type(torch.LongTensor))\n",
        "  stuff.append(torch.from_numpy(y).type(torch.DoubleTensor))\n",
        "\n",
        "  tensorset = data_utils.TensorDataset(*stuff)\n",
        "  loader = data_utils.DataLoader(tensorset, batch_size=batch_size, drop_last=drop_last, shuffle=training)\n",
        "  loader.name = name\n",
        "  return loader\n",
        "\n",
        "  \n",
        "def get_dataset(train, test, val, vocab_size, max_length, batch_size, labels, name):\n",
        "  train_list_x, train_list_y = get_list(train, labels)\n",
        "  test_list_x, test_list_y = get_list(test, labels)\n",
        "  val_list_x, val_list_y = get_list(val, labels)\n",
        "\n",
        "\n",
        "\n",
        "  #tokenising various stuff, setting up numpy dictionaries :)\n",
        "  tokeniser = Tokeniser(train_list_x, vocab_size, max_length)\n",
        "  x_train = tokeniser.do_everything(train_list_x)\n",
        "  x_test = tokeniser.do_everything(test_list_x)\n",
        "  x_val = tokeniser.do_everything(val_list_x)\n",
        "  y_train = np.array(train_list_y, dtype=np.float32)\n",
        "  y_test = np.array(test_list_y, dtype=np.float32)\n",
        "  y_val = np.array(val_list_y, dtype=np.float32)\n",
        "  \n",
        "  #datasets/loaders\n",
        "  train_loader = get_loader(x_train, y_train, vocab_size, max_length, batch_size, name, True)\n",
        "  test_loader = get_loader(x_test, y_test, vocab_size, max_length, batch_size, name, False, drop_last=False)\n",
        "  val_loader = get_loader(x_val, y_val, vocab_size, max_length, batch_size, name, False)\n",
        "  return Dataset(train_loader, test_loader, val_loader,  y_test, y_val, tokeniser)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MMrKlu6_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "outputId": "8fa88ae1-bff4-48d8-d6fb-e9e090a5f201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "\n",
        "snopes_dataset = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "fact_dataset = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "big_snopes = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "big_fact = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39093\n",
            "33766\n",
            "44623\n",
            "37174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    \n",
        "    self.embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.normaliser = torch.nn.BatchNorm1d(self.embedding_size)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x[:, 0:self.hp.max_length])\n",
        "    #embedding = self.normaliser(embedding.transpose(1,2))\n",
        "    embedding = self.dropout(embedding)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    tanh_W_H = self.dropout(tanh_W_H)\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.premise_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    premise_factor = self.premise_dropout(premise_factor)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    hypothesis_factor = self.hypothesis_dropout(hypothesis_factor)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=50)\n",
        "    self.linear2 = torch.nn.Linear(50, 20)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "      x = self.dropout1(x)\n",
        "    else:\n",
        "      x = torch.relu(self.linear1(x.reshape(self.hp.batch_size, -1)))\n",
        "      x = self.dropout1(x)\n",
        "      x = torch.relu(self.linear2(x))\n",
        "      x = self.dropout2(x)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    print(word_embeddings.shape)\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout)]\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    self.dropouts[1](premise_embedding)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    self.dropouts[3](hypothesis_embedding)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    self.dropouts[4](factorised_mush)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention*premise_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWRHhOxCjFl",
        "colab_type": "text"
      },
      "source": [
        "##EVAL SUMMARY :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    combined = premise_embedding * hypothesis_embedding\n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db8ikkk64Kx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def load_embeddings(self, word_embeddings):\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, self.hp)\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(101, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    mean_embeddings = torch.unsqueeze(torch.sum(embeddings, 1) / self.hp.max_length, 1) #change to accurate size of lenfgth\n",
        "  \n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100\n",
        "    main_embeddings = torch.cat((mean_embeddings, added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    attention_weights = softmax(self.premise_linear(processed_premise))#TODO: turn into row vector\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis,attention_weights.transpose(1,2))\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    avg = torch.sum(combined, 1)/self.hp.max_length #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    output = torch.sigmoid(self.linear_final(smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(10000, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 8)\n",
        "    self.linear_almost_there = torch.nn.Linear(8, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.dropout = torch.nn.Dropout(p=0.2)\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    lengths = self.hp.max_length - (premise == 0).sum(dim=1)\n",
        "    lengths = lengths.repeat(50, 1).transpose(0,1)\n",
        "    summed_embeddings = torch.sum(embeddings, 1)\n",
        "    mean_embeddings = torch.unsqueeze(summed_embeddings / lengths, 1) #change to accurate size of lenfgth\n",
        "    flattened_embeddings = mean_embeddings.reshape(self.hp.batch_size, -1)\n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100]).reshape(self.hp.batch_size, -1)\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((flattened_embeddings.repeat(1, 100), added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    attention_weights = softmax(torch.tanh(self.premise_linear(main_embeddings)))#TODO: turn into row vector\n",
        "\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, attention_weights.unsqueeze(2))\n",
        "    \n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    new_lengths = lengths.repeat(1, 2)\n",
        "\n",
        "    avg = torch.sum(combined, 1)/new_lengths #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    smaller = self.dropout(smaller)\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    output = torch.sigmoid(self.linear_final(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          dataset = None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "\n",
        "  is_binary = hp.num_classes == 1\n",
        "  is_validating = False\n",
        "  \n",
        "  if dataset.train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif dataset.train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "\n",
        "    for i in range(2):\n",
        "      \n",
        "      total_loss = 0\n",
        "      batch_count = 0\n",
        "      correct = 0\n",
        "      penal = 0\n",
        "\n",
        "      if is_validating:\n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.val_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 1\n",
        "      else:\n",
        "        model.train(True)\n",
        "        loader = dataset.train_loader\n",
        "        torch.enable_grad()\n",
        "        debug_amount = 10\n",
        "      \n",
        "      for batch_index, train_data in enumerate(loader):\n",
        "      #setting everything up\n",
        "        model.hidden_state = model.reset_for_testing(train_data[0].shape[0])\n",
        "        train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "        predicted_y, attention = model(*train_data[:-1])\n",
        "        actual_y = train_data[-1]\n",
        "        squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "        if hp.C > 0:\n",
        "          attentionT = attention.transpose(1,2)\n",
        "          identity = torch.eye(attention.size(1))\n",
        "          identity = Variable(identity.unsqueeze(0).expand(loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "          penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "        if is_binary:\n",
        "          loss = loss_function(squeezed_y, actual_y.double())\n",
        "          loss += hp.C * penal/loader.batch_size\n",
        "          correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "        else:\n",
        "          loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/loader.batch_size)\n",
        "          correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if using_gradient_clipping:\n",
        "          torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      \n",
        "      #cleaning up regularisation\n",
        "        if hp.C > 0:\n",
        "          del(penal)\n",
        "          del(identity)\n",
        "          del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "        optimiser.zero_grad()\n",
        "        if not is_validating:\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "        if hp.is_debug and batch_index % debug_amount == 0:\n",
        "          print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, ISvAL: {}\".format(\n",
        "              epoch, batch_index * len(train_data[0]), len(loader.dataset),\n",
        "              100. * batch_index / len(loader), loss.item(), is_validating\n",
        "          ))\n",
        "\n",
        "      \n",
        "        batch_count += 1\n",
        "      \n",
        "        free_data(train_data)\n",
        "\n",
        "      print(\"Average loss is:\",total_loss/batch_count, \"while validation_status:\", is_validating)\n",
        "      correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "      accuracy = correct_but_numpy / float(batch_count * loader.batch_size)\n",
        "      print(\"Accuracy of the model\", accuracy)\n",
        "      if (not is_validating):\n",
        "        losses.append(total_loss/batch_count)\n",
        "        accuracies.append(accuracy)\n",
        "      else:\n",
        "        val_losses.append(total_loss/batch_count)\n",
        "        val_accuracies.append(accuracy)\n",
        "\n",
        "      is_validating = not is_validating\n",
        "  return losses, val_losses, accuracies, val_accuracies "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, val_losses, accuracies, val_accuracies, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  \n",
        "  plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Train Accuracy\")\n",
        "  plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_accuracies, scalex=True, scaley=True, label=\"Val Accuracy\")\n",
        "  plt.annotate(str(val_accuracies[-1]), xy=(epochs,val_accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Train Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_losses,scalex=True, scaley=True, label=\"Val Loss\")\n",
        "  plt.annotate(str(val_losses[-1]), xy=(epochs,val_losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp, is_plot=True):\n",
        "  runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "  bce_loss = torch.nn.BCELoss()\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(runnable_model.parameters(), lr=hp.lr, weight_decay=hp.decay)\n",
        "  losses, val_losses, accuracies, val_accuracies = train(model=runnable_model,\n",
        "                       dataset = dataset,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=hp.grad_clip)\n",
        "  if is_plot:\n",
        "    plot_stuff(hp.epochs, losses,val_losses, accuracies, val_accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  check_loader = dataset.test_loader\n",
        "  predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         check_loader,\n",
        "         hp)\n",
        "  return predicted_ys, runnable_model\n",
        "\n",
        "\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc_full}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2DLo4OoUO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc_full))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oMDNooNrMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "##RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": fact_dataset,\n",
        "    \"snopes\": snopes_dataset\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    \"broke_declare\": BaselineDeclare,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 3\n",
        "  inner_dropout = 0.7\n",
        "  outer_dropout = 0.6\n",
        "  C = 0.3\n",
        "  decay = 0\n",
        "  is_debug = True\n",
        "  lr=0.001\n",
        "  grad_clip = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wRSAvtABCT",
        "colab_type": "text"
      },
      "source": [
        "##CROSS VALIDATION ZONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "d490506c-641c-49aa-c50d-42ac0b489872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "NUM_OF_AVGS = 5\n",
        "avgs = []\n",
        "\"\"\"\n",
        "for i in range(NUM_OF_AVGS):\n",
        "  \n",
        "  \n",
        "  \n",
        "  avgs.append(results)\n",
        "\n",
        "print(process_results(list_to_dict(avgs)))\n",
        "\"\"\"\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters)\n",
        "results = get_results(\"my_model\", \"snopes\", predicted_ys.cpu(), datasets[\"snopes\"].test_data)\n",
        "print(results)\n",
        "  #textual_entailment_model.to(device)\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters)\n",
        "results = get_results(\"my_model\", \"politifact\", predicted_ys.cpu(), datasets[\"politifact\"].test_data)\n",
        "print(results)"
      ],
      "execution_count": 527,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([39093, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.643543, ISvAL: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 1.640033, ISvAL: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 1.642156, ISvAL: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 1.643182, ISvAL: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 1.642738, ISvAL: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 1.644176, ISvAL: False\n",
            "Train Epoch: 0 [6000/12119 (50%)]\tLoss: 1.640720, ISvAL: False\n",
            "Train Epoch: 0 [7000/12119 (58%)]\tLoss: 1.642329, ISvAL: False\n",
            "Train Epoch: 0 [8000/12119 (66%)]\tLoss: 1.641115, ISvAL: False\n",
            "Train Epoch: 0 [9000/12119 (74%)]\tLoss: 1.644728, ISvAL: False\n",
            "Train Epoch: 0 [10000/12119 (83%)]\tLoss: 1.642373, ISvAL: False\n",
            "Train Epoch: 0 [11000/12119 (91%)]\tLoss: 1.643153, ISvAL: False\n",
            "Train Epoch: 0 [12000/12119 (99%)]\tLoss: 1.642671, ISvAL: False\n",
            "Average loss is: tensor(1.6421, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.4994214876033058\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 1.631751, ISvAL: True\n",
            "Train Epoch: 0 [100/1507 (7%)]\tLoss: 1.633167, ISvAL: True\n",
            "Train Epoch: 0 [200/1507 (13%)]\tLoss: 1.637609, ISvAL: True\n",
            "Train Epoch: 0 [300/1507 (20%)]\tLoss: 1.638331, ISvAL: True\n",
            "Train Epoch: 0 [400/1507 (27%)]\tLoss: 1.629104, ISvAL: True\n",
            "Train Epoch: 0 [500/1507 (33%)]\tLoss: 1.637146, ISvAL: True\n",
            "Train Epoch: 0 [600/1507 (40%)]\tLoss: 1.637736, ISvAL: True\n",
            "Train Epoch: 0 [700/1507 (47%)]\tLoss: 1.634792, ISvAL: True\n",
            "Train Epoch: 0 [800/1507 (53%)]\tLoss: 1.630931, ISvAL: True\n",
            "Train Epoch: 0 [900/1507 (60%)]\tLoss: 1.640058, ISvAL: True\n",
            "Train Epoch: 0 [1000/1507 (67%)]\tLoss: 1.652739, ISvAL: True\n",
            "Train Epoch: 0 [1100/1507 (73%)]\tLoss: 1.653019, ISvAL: True\n",
            "Train Epoch: 0 [1200/1507 (80%)]\tLoss: 1.645179, ISvAL: True\n",
            "Train Epoch: 0 [1300/1507 (87%)]\tLoss: 1.646660, ISvAL: True\n",
            "Train Epoch: 0 [1400/1507 (93%)]\tLoss: 1.653338, ISvAL: True\n",
            "Average loss is: tensor(1.6401, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.558\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 1.642209, ISvAL: False\n",
            "Train Epoch: 1 [1000/12119 (8%)]\tLoss: 1.642231, ISvAL: False\n",
            "Train Epoch: 1 [2000/12119 (17%)]\tLoss: 1.639715, ISvAL: False\n",
            "Train Epoch: 1 [3000/12119 (25%)]\tLoss: 1.642571, ISvAL: False\n",
            "Train Epoch: 1 [4000/12119 (33%)]\tLoss: 1.639491, ISvAL: False\n",
            "Train Epoch: 1 [5000/12119 (41%)]\tLoss: 1.629707, ISvAL: False\n",
            "Train Epoch: 1 [6000/12119 (50%)]\tLoss: 1.630600, ISvAL: False\n",
            "Train Epoch: 1 [7000/12119 (58%)]\tLoss: 1.612815, ISvAL: False\n",
            "Train Epoch: 1 [8000/12119 (66%)]\tLoss: 1.656369, ISvAL: False\n",
            "Train Epoch: 1 [9000/12119 (74%)]\tLoss: 1.643879, ISvAL: False\n",
            "Train Epoch: 1 [10000/12119 (83%)]\tLoss: 1.604058, ISvAL: False\n",
            "Train Epoch: 1 [11000/12119 (91%)]\tLoss: 1.605246, ISvAL: False\n",
            "Train Epoch: 1 [12000/12119 (99%)]\tLoss: 1.590557, ISvAL: False\n",
            "Average loss is: tensor(1.6217, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5819008264462809\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 1.487444, ISvAL: True\n",
            "Train Epoch: 1 [100/1507 (7%)]\tLoss: 1.458796, ISvAL: True\n",
            "Train Epoch: 1 [200/1507 (13%)]\tLoss: 1.841509, ISvAL: True\n",
            "Train Epoch: 1 [300/1507 (20%)]\tLoss: 1.649283, ISvAL: True\n",
            "Train Epoch: 1 [400/1507 (27%)]\tLoss: 1.909204, ISvAL: True\n",
            "Train Epoch: 1 [500/1507 (33%)]\tLoss: 1.434777, ISvAL: True\n",
            "Train Epoch: 1 [600/1507 (40%)]\tLoss: 1.678313, ISvAL: True\n",
            "Train Epoch: 1 [700/1507 (47%)]\tLoss: 1.620337, ISvAL: True\n",
            "Train Epoch: 1 [800/1507 (53%)]\tLoss: 1.563585, ISvAL: True\n",
            "Train Epoch: 1 [900/1507 (60%)]\tLoss: 1.645341, ISvAL: True\n",
            "Train Epoch: 1 [1000/1507 (67%)]\tLoss: 1.613769, ISvAL: True\n",
            "Train Epoch: 1 [1100/1507 (73%)]\tLoss: 1.652221, ISvAL: True\n",
            "Train Epoch: 1 [1200/1507 (80%)]\tLoss: 1.516090, ISvAL: True\n",
            "Train Epoch: 1 [1300/1507 (87%)]\tLoss: 1.514296, ISvAL: True\n",
            "Train Epoch: 1 [1400/1507 (93%)]\tLoss: 1.642793, ISvAL: True\n",
            "Average loss is: tensor(1.6152, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.624\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 1.568476, ISvAL: False\n",
            "Train Epoch: 2 [1000/12119 (8%)]\tLoss: 1.598191, ISvAL: False\n",
            "Train Epoch: 2 [2000/12119 (17%)]\tLoss: 1.544384, ISvAL: False\n",
            "Train Epoch: 2 [3000/12119 (25%)]\tLoss: 1.570188, ISvAL: False\n",
            "Train Epoch: 2 [4000/12119 (33%)]\tLoss: 1.570526, ISvAL: False\n",
            "Train Epoch: 2 [5000/12119 (41%)]\tLoss: 1.554784, ISvAL: False\n",
            "Train Epoch: 2 [6000/12119 (50%)]\tLoss: 1.626332, ISvAL: False\n",
            "Train Epoch: 2 [7000/12119 (58%)]\tLoss: 1.511699, ISvAL: False\n",
            "Train Epoch: 2 [8000/12119 (66%)]\tLoss: 1.486240, ISvAL: False\n",
            "Train Epoch: 2 [9000/12119 (74%)]\tLoss: 1.535117, ISvAL: False\n",
            "Train Epoch: 2 [10000/12119 (83%)]\tLoss: 1.498087, ISvAL: False\n",
            "Train Epoch: 2 [11000/12119 (91%)]\tLoss: 1.446630, ISvAL: False\n",
            "Train Epoch: 2 [12000/12119 (99%)]\tLoss: 1.446495, ISvAL: False\n",
            "Average loss is: tensor(1.5279, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7185123966942148\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 1.592609, ISvAL: True\n",
            "Train Epoch: 2 [100/1507 (7%)]\tLoss: 1.600013, ISvAL: True\n",
            "Train Epoch: 2 [200/1507 (13%)]\tLoss: 2.014743, ISvAL: True\n",
            "Train Epoch: 2 [300/1507 (20%)]\tLoss: 1.897561, ISvAL: True\n",
            "Train Epoch: 2 [400/1507 (27%)]\tLoss: 2.229631, ISvAL: True\n",
            "Train Epoch: 2 [500/1507 (33%)]\tLoss: 1.615010, ISvAL: True\n",
            "Train Epoch: 2 [600/1507 (40%)]\tLoss: 1.933057, ISvAL: True\n",
            "Train Epoch: 2 [700/1507 (47%)]\tLoss: 1.917626, ISvAL: True\n",
            "Train Epoch: 2 [800/1507 (53%)]\tLoss: 1.776147, ISvAL: True\n",
            "Train Epoch: 2 [900/1507 (60%)]\tLoss: 1.876558, ISvAL: True\n",
            "Train Epoch: 2 [1000/1507 (67%)]\tLoss: 1.636843, ISvAL: True\n",
            "Train Epoch: 2 [1100/1507 (73%)]\tLoss: 1.659073, ISvAL: True\n",
            "Train Epoch: 2 [1200/1507 (80%)]\tLoss: 1.255371, ISvAL: True\n",
            "Train Epoch: 2 [1300/1507 (87%)]\tLoss: 1.556435, ISvAL: True\n",
            "Train Epoch: 2 [1400/1507 (93%)]\tLoss: 1.569899, ISvAL: True\n",
            "Average loss is: tensor(1.7420, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6426666666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6163820248433249, 'recall': 0.6199668050374935, 'accuracy': 0.6164383561643836, 'f1': 0.6135294117647059, 'auc': 0.6163820248433249}\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.653814, ISvAL: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/23817 (4%)]\tLoss: 1.640504, ISvAL: False\n",
            "Train Epoch: 0 [2000/23817 (8%)]\tLoss: 1.647229, ISvAL: False\n",
            "Train Epoch: 0 [3000/23817 (13%)]\tLoss: 1.633673, ISvAL: False\n",
            "Train Epoch: 0 [4000/23817 (17%)]\tLoss: 1.642918, ISvAL: False\n",
            "Train Epoch: 0 [5000/23817 (21%)]\tLoss: 1.650030, ISvAL: False\n",
            "Train Epoch: 0 [6000/23817 (25%)]\tLoss: 1.634177, ISvAL: False\n",
            "Train Epoch: 0 [7000/23817 (29%)]\tLoss: 1.634650, ISvAL: False\n",
            "Train Epoch: 0 [8000/23817 (34%)]\tLoss: 1.642245, ISvAL: False\n",
            "Train Epoch: 0 [9000/23817 (38%)]\tLoss: 1.636082, ISvAL: False\n",
            "Train Epoch: 0 [10000/23817 (42%)]\tLoss: 1.640487, ISvAL: False\n",
            "Train Epoch: 0 [11000/23817 (46%)]\tLoss: 1.638900, ISvAL: False\n",
            "Train Epoch: 0 [12000/23817 (50%)]\tLoss: 1.638207, ISvAL: False\n",
            "Train Epoch: 0 [13000/23817 (55%)]\tLoss: 1.636098, ISvAL: False\n",
            "Train Epoch: 0 [14000/23817 (59%)]\tLoss: 1.631786, ISvAL: False\n",
            "Train Epoch: 0 [15000/23817 (63%)]\tLoss: 1.635731, ISvAL: False\n",
            "Train Epoch: 0 [16000/23817 (67%)]\tLoss: 1.628924, ISvAL: False\n",
            "Train Epoch: 0 [17000/23817 (71%)]\tLoss: 1.667019, ISvAL: False\n",
            "Train Epoch: 0 [18000/23817 (76%)]\tLoss: 1.612822, ISvAL: False\n",
            "Train Epoch: 0 [19000/23817 (80%)]\tLoss: 1.602072, ISvAL: False\n",
            "Train Epoch: 0 [20000/23817 (84%)]\tLoss: 1.645244, ISvAL: False\n",
            "Train Epoch: 0 [21000/23817 (88%)]\tLoss: 1.665383, ISvAL: False\n",
            "Train Epoch: 0 [22000/23817 (92%)]\tLoss: 1.646500, ISvAL: False\n",
            "Train Epoch: 0 [23000/23817 (97%)]\tLoss: 1.584726, ISvAL: False\n",
            "Average loss is: tensor(1.6337, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5418067226890756\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.696510, ISvAL: True\n",
            "Train Epoch: 0 [100/2884 (4%)]\tLoss: 1.521467, ISvAL: True\n",
            "Train Epoch: 0 [200/2884 (7%)]\tLoss: 1.721694, ISvAL: True\n",
            "Train Epoch: 0 [300/2884 (11%)]\tLoss: 1.403877, ISvAL: True\n",
            "Train Epoch: 0 [400/2884 (14%)]\tLoss: 1.675108, ISvAL: True\n",
            "Train Epoch: 0 [500/2884 (18%)]\tLoss: 1.657990, ISvAL: True\n",
            "Train Epoch: 0 [600/2884 (21%)]\tLoss: 1.524828, ISvAL: True\n",
            "Train Epoch: 0 [700/2884 (25%)]\tLoss: 1.537278, ISvAL: True\n",
            "Train Epoch: 0 [800/2884 (29%)]\tLoss: 1.665049, ISvAL: True\n",
            "Train Epoch: 0 [900/2884 (32%)]\tLoss: 1.708776, ISvAL: True\n",
            "Train Epoch: 0 [1000/2884 (36%)]\tLoss: 1.578217, ISvAL: True\n",
            "Train Epoch: 0 [1100/2884 (39%)]\tLoss: 1.731654, ISvAL: True\n",
            "Train Epoch: 0 [1200/2884 (43%)]\tLoss: 1.823878, ISvAL: True\n",
            "Train Epoch: 0 [1300/2884 (46%)]\tLoss: 1.710821, ISvAL: True\n",
            "Train Epoch: 0 [1400/2884 (50%)]\tLoss: 1.572691, ISvAL: True\n",
            "Train Epoch: 0 [1500/2884 (54%)]\tLoss: 1.460470, ISvAL: True\n",
            "Train Epoch: 0 [1600/2884 (57%)]\tLoss: 1.510362, ISvAL: True\n",
            "Train Epoch: 0 [1700/2884 (61%)]\tLoss: 1.524289, ISvAL: True\n",
            "Train Epoch: 0 [1800/2884 (64%)]\tLoss: 1.633558, ISvAL: True\n",
            "Train Epoch: 0 [1900/2884 (68%)]\tLoss: 1.799402, ISvAL: True\n",
            "Train Epoch: 0 [2000/2884 (71%)]\tLoss: 1.560530, ISvAL: True\n",
            "Train Epoch: 0 [2100/2884 (75%)]\tLoss: 1.609008, ISvAL: True\n",
            "Train Epoch: 0 [2200/2884 (79%)]\tLoss: 1.719714, ISvAL: True\n",
            "Train Epoch: 0 [2300/2884 (82%)]\tLoss: 1.689020, ISvAL: True\n",
            "Train Epoch: 0 [2400/2884 (86%)]\tLoss: 1.881820, ISvAL: True\n",
            "Train Epoch: 0 [2500/2884 (89%)]\tLoss: 1.623018, ISvAL: True\n",
            "Train Epoch: 0 [2600/2884 (93%)]\tLoss: 1.653410, ISvAL: True\n",
            "Train Epoch: 0 [2700/2884 (96%)]\tLoss: 1.662845, ISvAL: True\n",
            "Average loss is: tensor(1.6378, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.575\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.633072, ISvAL: False\n",
            "Train Epoch: 1 [1000/23817 (4%)]\tLoss: 1.586062, ISvAL: False\n",
            "Train Epoch: 1 [2000/23817 (8%)]\tLoss: 1.619890, ISvAL: False\n",
            "Train Epoch: 1 [3000/23817 (13%)]\tLoss: 1.617974, ISvAL: False\n",
            "Train Epoch: 1 [4000/23817 (17%)]\tLoss: 1.583559, ISvAL: False\n",
            "Train Epoch: 1 [5000/23817 (21%)]\tLoss: 1.578405, ISvAL: False\n",
            "Train Epoch: 1 [6000/23817 (25%)]\tLoss: 1.629291, ISvAL: False\n",
            "Train Epoch: 1 [7000/23817 (29%)]\tLoss: 1.591192, ISvAL: False\n",
            "Train Epoch: 1 [8000/23817 (34%)]\tLoss: 1.602585, ISvAL: False\n",
            "Train Epoch: 1 [9000/23817 (38%)]\tLoss: 1.593775, ISvAL: False\n",
            "Train Epoch: 1 [10000/23817 (42%)]\tLoss: 1.594402, ISvAL: False\n",
            "Train Epoch: 1 [11000/23817 (46%)]\tLoss: 1.588416, ISvAL: False\n",
            "Train Epoch: 1 [12000/23817 (50%)]\tLoss: 1.572516, ISvAL: False\n",
            "Train Epoch: 1 [13000/23817 (55%)]\tLoss: 1.578110, ISvAL: False\n",
            "Train Epoch: 1 [14000/23817 (59%)]\tLoss: 1.588315, ISvAL: False\n",
            "Train Epoch: 1 [15000/23817 (63%)]\tLoss: 1.581949, ISvAL: False\n",
            "Train Epoch: 1 [16000/23817 (67%)]\tLoss: 1.556208, ISvAL: False\n",
            "Train Epoch: 1 [17000/23817 (71%)]\tLoss: 1.614059, ISvAL: False\n",
            "Train Epoch: 1 [18000/23817 (76%)]\tLoss: 1.563965, ISvAL: False\n",
            "Train Epoch: 1 [19000/23817 (80%)]\tLoss: 1.539372, ISvAL: False\n",
            "Train Epoch: 1 [20000/23817 (84%)]\tLoss: 1.661601, ISvAL: False\n",
            "Train Epoch: 1 [21000/23817 (88%)]\tLoss: 1.518498, ISvAL: False\n",
            "Train Epoch: 1 [22000/23817 (92%)]\tLoss: 1.511589, ISvAL: False\n",
            "Train Epoch: 1 [23000/23817 (97%)]\tLoss: 1.548882, ISvAL: False\n",
            "Average loss is: tensor(1.5819, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6483613445378151\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.844260, ISvAL: True\n",
            "Train Epoch: 1 [100/2884 (4%)]\tLoss: 1.348644, ISvAL: True\n",
            "Train Epoch: 1 [200/2884 (7%)]\tLoss: 1.787880, ISvAL: True\n",
            "Train Epoch: 1 [300/2884 (11%)]\tLoss: 1.328747, ISvAL: True\n",
            "Train Epoch: 1 [400/2884 (14%)]\tLoss: 1.938300, ISvAL: True\n",
            "Train Epoch: 1 [500/2884 (18%)]\tLoss: 2.048789, ISvAL: True\n",
            "Train Epoch: 1 [600/2884 (21%)]\tLoss: 1.411815, ISvAL: True\n",
            "Train Epoch: 1 [700/2884 (25%)]\tLoss: 1.490520, ISvAL: True\n",
            "Train Epoch: 1 [800/2884 (29%)]\tLoss: 1.991259, ISvAL: True\n",
            "Train Epoch: 1 [900/2884 (32%)]\tLoss: 1.891691, ISvAL: True\n",
            "Train Epoch: 1 [1000/2884 (36%)]\tLoss: 1.477841, ISvAL: True\n",
            "Train Epoch: 1 [1100/2884 (39%)]\tLoss: 2.033641, ISvAL: True\n",
            "Train Epoch: 1 [1200/2884 (43%)]\tLoss: 2.112256, ISvAL: True\n",
            "Train Epoch: 1 [1300/2884 (46%)]\tLoss: 1.780401, ISvAL: True\n",
            "Train Epoch: 1 [1400/2884 (50%)]\tLoss: 1.590233, ISvAL: True\n",
            "Train Epoch: 1 [1500/2884 (54%)]\tLoss: 1.573243, ISvAL: True\n",
            "Train Epoch: 1 [1600/2884 (57%)]\tLoss: 1.472057, ISvAL: True\n",
            "Train Epoch: 1 [1700/2884 (61%)]\tLoss: 1.639609, ISvAL: True\n",
            "Train Epoch: 1 [1800/2884 (64%)]\tLoss: 1.537909, ISvAL: True\n",
            "Train Epoch: 1 [1900/2884 (68%)]\tLoss: 1.771551, ISvAL: True\n",
            "Train Epoch: 1 [2000/2884 (71%)]\tLoss: 1.524877, ISvAL: True\n",
            "Train Epoch: 1 [2100/2884 (75%)]\tLoss: 1.846692, ISvAL: True\n",
            "Train Epoch: 1 [2200/2884 (79%)]\tLoss: 2.262781, ISvAL: True\n",
            "Train Epoch: 1 [2300/2884 (82%)]\tLoss: 1.756656, ISvAL: True\n",
            "Train Epoch: 1 [2400/2884 (86%)]\tLoss: 2.440025, ISvAL: True\n",
            "Train Epoch: 1 [2500/2884 (89%)]\tLoss: 1.700544, ISvAL: True\n",
            "Train Epoch: 1 [2600/2884 (93%)]\tLoss: 1.923188, ISvAL: True\n",
            "Train Epoch: 1 [2700/2884 (96%)]\tLoss: 1.732187, ISvAL: True\n",
            "Average loss is: tensor(1.7592, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6185714285714285\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.554074, ISvAL: False\n",
            "Train Epoch: 2 [1000/23817 (4%)]\tLoss: 1.608388, ISvAL: False\n",
            "Train Epoch: 2 [2000/23817 (8%)]\tLoss: 1.532578, ISvAL: False\n",
            "Train Epoch: 2 [3000/23817 (13%)]\tLoss: 1.544778, ISvAL: False\n",
            "Train Epoch: 2 [4000/23817 (17%)]\tLoss: 1.532248, ISvAL: False\n",
            "Train Epoch: 2 [5000/23817 (21%)]\tLoss: 1.511514, ISvAL: False\n",
            "Train Epoch: 2 [6000/23817 (25%)]\tLoss: 1.473610, ISvAL: False\n",
            "Train Epoch: 2 [7000/23817 (29%)]\tLoss: 1.614166, ISvAL: False\n",
            "Train Epoch: 2 [8000/23817 (34%)]\tLoss: 1.535739, ISvAL: False\n",
            "Train Epoch: 2 [9000/23817 (38%)]\tLoss: 1.502491, ISvAL: False\n",
            "Train Epoch: 2 [10000/23817 (42%)]\tLoss: 1.453192, ISvAL: False\n",
            "Train Epoch: 2 [11000/23817 (46%)]\tLoss: 1.532727, ISvAL: False\n",
            "Train Epoch: 2 [12000/23817 (50%)]\tLoss: 1.394366, ISvAL: False\n",
            "Train Epoch: 2 [13000/23817 (55%)]\tLoss: 1.524696, ISvAL: False\n",
            "Train Epoch: 2 [14000/23817 (59%)]\tLoss: 1.519929, ISvAL: False\n",
            "Train Epoch: 2 [15000/23817 (63%)]\tLoss: 1.519700, ISvAL: False\n",
            "Train Epoch: 2 [16000/23817 (67%)]\tLoss: 1.501515, ISvAL: False\n",
            "Train Epoch: 2 [17000/23817 (71%)]\tLoss: 1.499232, ISvAL: False\n",
            "Train Epoch: 2 [18000/23817 (76%)]\tLoss: 1.453934, ISvAL: False\n",
            "Train Epoch: 2 [19000/23817 (80%)]\tLoss: 1.428862, ISvAL: False\n",
            "Train Epoch: 2 [20000/23817 (84%)]\tLoss: 1.518659, ISvAL: False\n",
            "Train Epoch: 2 [21000/23817 (88%)]\tLoss: 1.495176, ISvAL: False\n",
            "Train Epoch: 2 [22000/23817 (92%)]\tLoss: 1.474114, ISvAL: False\n",
            "Train Epoch: 2 [23000/23817 (97%)]\tLoss: 1.497095, ISvAL: False\n",
            "Average loss is: tensor(1.5080, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7220168067226891\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 2.119248, ISvAL: True\n",
            "Train Epoch: 2 [100/2884 (4%)]\tLoss: 1.403932, ISvAL: True\n",
            "Train Epoch: 2 [200/2884 (7%)]\tLoss: 1.736395, ISvAL: True\n",
            "Train Epoch: 2 [300/2884 (11%)]\tLoss: 1.429814, ISvAL: True\n",
            "Train Epoch: 2 [400/2884 (14%)]\tLoss: 1.836865, ISvAL: True\n",
            "Train Epoch: 2 [500/2884 (18%)]\tLoss: 2.139299, ISvAL: True\n",
            "Train Epoch: 2 [600/2884 (21%)]\tLoss: 1.514585, ISvAL: True\n",
            "Train Epoch: 2 [700/2884 (25%)]\tLoss: 1.556590, ISvAL: True\n",
            "Train Epoch: 2 [800/2884 (29%)]\tLoss: 1.912487, ISvAL: True\n",
            "Train Epoch: 2 [900/2884 (32%)]\tLoss: 1.997401, ISvAL: True\n",
            "Train Epoch: 2 [1000/2884 (36%)]\tLoss: 1.537464, ISvAL: True\n",
            "Train Epoch: 2 [1100/2884 (39%)]\tLoss: 2.378352, ISvAL: True\n",
            "Train Epoch: 2 [1200/2884 (43%)]\tLoss: 2.013377, ISvAL: True\n",
            "Train Epoch: 2 [1300/2884 (46%)]\tLoss: 1.902552, ISvAL: True\n",
            "Train Epoch: 2 [1400/2884 (50%)]\tLoss: 1.704642, ISvAL: True\n",
            "Train Epoch: 2 [1500/2884 (54%)]\tLoss: 1.580643, ISvAL: True\n",
            "Train Epoch: 2 [1600/2884 (57%)]\tLoss: 1.511740, ISvAL: True\n",
            "Train Epoch: 2 [1700/2884 (61%)]\tLoss: 1.715291, ISvAL: True\n",
            "Train Epoch: 2 [1800/2884 (64%)]\tLoss: 1.550359, ISvAL: True\n",
            "Train Epoch: 2 [1900/2884 (68%)]\tLoss: 1.686033, ISvAL: True\n",
            "Train Epoch: 2 [2000/2884 (71%)]\tLoss: 1.420905, ISvAL: True\n",
            "Train Epoch: 2 [2100/2884 (75%)]\tLoss: 1.988711, ISvAL: True\n",
            "Train Epoch: 2 [2200/2884 (79%)]\tLoss: 2.722011, ISvAL: True\n",
            "Train Epoch: 2 [2300/2884 (82%)]\tLoss: 1.780751, ISvAL: True\n",
            "Train Epoch: 2 [2400/2884 (86%)]\tLoss: 2.290788, ISvAL: True\n",
            "Train Epoch: 2 [2500/2884 (89%)]\tLoss: 1.722763, ISvAL: True\n",
            "Train Epoch: 2 [2600/2884 (93%)]\tLoss: 2.305657, ISvAL: True\n",
            "Train Epoch: 2 [2700/2884 (96%)]\tLoss: 1.613673, ISvAL: True\n",
            "Average loss is: tensor(1.8240, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6103571428571428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6175402645425705, 'recall': 0.6714927473479109, 'accuracy': 0.6416812609457093, 'f1': 0.5987363898536944, 'auc': 0.6175402645425705}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAogAAAEbCAYAAABZU3XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhk/X85KP4CkHdIqgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JkXr8fcoee",
        "colab_type": "text"
      },
      "source": [
        "WOAH WERE DOIN SOME VALIDATION\n",
        "PLEASE VALIDATE ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu() ,datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zfw_4Acupe",
        "colab_type": "text"
      },
      "source": [
        "JUST TESTIN HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsngYK8wcvc8",
        "colab_type": "code",
        "outputId": "74ea10c0-b5f9-4dfa-a8c7-0c9839eafee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "test_results= batch_wise_evaluate(text_model, datasets[\"politifact\"].test_loader, Hyperparameters)\n",
        "evaluation_summary(\"textual entailment test model\", test_results.cpu(), datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment test model\n",
            "Classifier 'textual entailment test model' has Acc=0.544 P=0.500 R=0.272 F1=0.352 AUC=0.500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.000     0.000     0.000         0\n",
            "         1.0      1.000     0.544     0.705      2855\n",
            "\n",
            "    accuracy                          0.544      2855\n",
            "   macro avg      0.500     0.272     0.352      2855\n",
            "weighted avg      1.000     0.544     0.705      2855\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   0 1301]\n",
            " [   0 1554]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5, 0.27215411558669, 0.54430823117338, 0.3524608754819687)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 362
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "##running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ngDR0NMc26u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 3\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0\n",
        "  C = 0.3\n",
        "  is_debug = True\n",
        "  lr=0.01\n",
        "  decay = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "outputId": "fc8a0977-b0b9-4d91-e522-28d973b49692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters)\n",
        "results = get_results(\"sheena_model\", \"politifact\", sheena_predicted_ys.cpu(), datasets[\"politifact\"].test_data)\n",
        "print(results)\n",
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters)\n",
        "results = get_results(\"sheena_model\", \"snopes\", sheena_predicted_ys.cpu(), datasets[\"snopes\"].test_data)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.639707, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.607014, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.565281, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.568010, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.419298, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.402506, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.357367, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.331855, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.248741, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.199029, ISvAL: False\n",
            "Average loss is: tensor(1.4330, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7292506720430108\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.593128, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.529531, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.660023, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.636965, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.753572, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.464167, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.373651, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.527486, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.758287, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.739732, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.779104, ISvAL: True\n",
            "Average loss is: tensor(1.6196, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6441761363636364\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.083913, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.084982, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.072984, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.024143, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.026011, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.052283, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 0.915682, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 0.857294, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 0.858197, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 0.767698, ISvAL: False\n",
            "Average loss is: tensor(0.9843, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.9080981182795699\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.375100, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.417621, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.734465, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.665049, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.864786, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.147310, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.366458, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.623867, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.617736, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.362221, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 2.017754, ISvAL: True\n",
            "Average loss is: tensor(1.5629, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.5951704545454546\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 0.675490, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 0.600437, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 0.588518, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 0.541119, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 0.494994, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 0.496805, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 0.518136, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 0.448048, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 0.417410, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 0.465435, ISvAL: False\n",
            "Average loss is: tensor(0.5216, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.9584173387096774\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.267141, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.387298, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.549406, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.562153, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 2.092747, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 0.940386, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.436873, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.559766, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.818287, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.247318, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 2.044966, ISvAL: True\n",
            "Average loss is: tensor(1.5369, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6253551136363636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6532656792072626, 'recall': 0.6557102110839421, 'accuracy': 0.6588441330998248, 'f1': 0.6537885956175299, 'auc': 0.6532656792072626}\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.638540, ISvAL: False\n",
            "Train Epoch: 0 [2560/12119 (21%)]\tLoss: 1.558090, ISvAL: False\n",
            "Train Epoch: 0 [5120/12119 (43%)]\tLoss: 1.508953, ISvAL: False\n",
            "Train Epoch: 0 [7680/12119 (64%)]\tLoss: 1.377036, ISvAL: False\n",
            "Train Epoch: 0 [10240/12119 (85%)]\tLoss: 1.255908, ISvAL: False\n",
            "Average loss is: tensor(1.4323, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7393617021276596\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 1.506412, ISvAL: True\n",
            "Train Epoch: 0 [256/1507 (20%)]\tLoss: 1.654882, ISvAL: True\n",
            "Train Epoch: 0 [512/1507 (40%)]\tLoss: 1.358637, ISvAL: True\n",
            "Train Epoch: 0 [768/1507 (60%)]\tLoss: 1.524361, ISvAL: True\n",
            "Train Epoch: 0 [1024/1507 (80%)]\tLoss: 1.877422, ISvAL: True\n",
            "Average loss is: tensor(1.5843, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.68515625\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 1.111713, ISvAL: False\n",
            "Train Epoch: 1 [2560/12119 (21%)]\tLoss: 1.070018, ISvAL: False\n",
            "Train Epoch: 1 [5120/12119 (43%)]\tLoss: 1.036980, ISvAL: False\n",
            "Train Epoch: 1 [7680/12119 (64%)]\tLoss: 1.024169, ISvAL: False\n",
            "Train Epoch: 1 [10240/12119 (85%)]\tLoss: 0.935741, ISvAL: False\n",
            "Average loss is: tensor(1.0183, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.953125\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 2.017303, ISvAL: True\n",
            "Train Epoch: 1 [256/1507 (20%)]\tLoss: 2.126018, ISvAL: True\n",
            "Train Epoch: 1 [512/1507 (40%)]\tLoss: 1.857252, ISvAL: True\n",
            "Train Epoch: 1 [768/1507 (60%)]\tLoss: 1.916667, ISvAL: True\n",
            "Train Epoch: 1 [1024/1507 (80%)]\tLoss: 1.636225, ISvAL: True\n",
            "Average loss is: tensor(1.9107, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6109375\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 0.876877, ISvAL: False\n",
            "Train Epoch: 2 [2560/12119 (21%)]\tLoss: 0.861047, ISvAL: False\n",
            "Train Epoch: 2 [5120/12119 (43%)]\tLoss: 0.839907, ISvAL: False\n",
            "Train Epoch: 2 [7680/12119 (64%)]\tLoss: 0.827445, ISvAL: False\n",
            "Train Epoch: 2 [10240/12119 (85%)]\tLoss: 0.808198, ISvAL: False\n",
            "Average loss is: tensor(0.8477, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.9920212765957447\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 2.254176, ISvAL: True\n",
            "Train Epoch: 2 [256/1507 (20%)]\tLoss: 2.478067, ISvAL: True\n",
            "Train Epoch: 2 [512/1507 (40%)]\tLoss: 1.941383, ISvAL: True\n",
            "Train Epoch: 2 [768/1507 (60%)]\tLoss: 1.962644, ISvAL: True\n",
            "Train Epoch: 2 [1024/1507 (80%)]\tLoss: 1.987980, ISvAL: True\n",
            "Average loss is: tensor(2.1248, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.62734375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6725280074618483, 'recall': 0.6726767243141584, 'accuracy': 0.6725375081539465, 'f1': 0.6724637804525102, 'auc': 0.6725280074618482}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+/8Nj1BzHa8gIQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"sheena model\", sheena_predicted_ys.cpu(), datasets[\"politifact\"].val_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "##OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFA1PNpzA8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeclareParameters:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0\n",
        "  epochs = 30\n",
        "  C = 0\n",
        "  is_debug = True\n",
        "  lr=0.0002\n",
        "  decay = 0\n",
        "  grad_clip = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "outputId": "d93b6ccc-cf21-44cb-9d85-c928d2fea4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"real_declare\"], datasets[\"snopes\"], DeclareParameters)\n",
        "results = get_results(\"real_declare\", \"snopes\", predicted_ys.cpu(), datasets[\"snopes\"].test_data)"
      ],
      "execution_count": 502,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 0.688383, ISvAL: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 0.707020, ISvAL: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 0.706374, ISvAL: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 0.716833, ISvAL: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 0.712793, ISvAL: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 0.728872, ISvAL: False\n",
            "Train Epoch: 0 [6000/12119 (50%)]\tLoss: 0.730494, ISvAL: False\n",
            "Train Epoch: 0 [7000/12119 (58%)]\tLoss: 0.721926, ISvAL: False\n",
            "Train Epoch: 0 [8000/12119 (66%)]\tLoss: 0.731075, ISvAL: False\n",
            "Train Epoch: 0 [9000/12119 (74%)]\tLoss: 0.721319, ISvAL: False\n",
            "Train Epoch: 0 [10000/12119 (83%)]\tLoss: 0.732362, ISvAL: False\n",
            "Train Epoch: 0 [11000/12119 (91%)]\tLoss: 0.722053, ISvAL: False\n",
            "Train Epoch: 0 [12000/12119 (99%)]\tLoss: 0.703417, ISvAL: False\n",
            "Average loss is: tensor(0.7056, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5006611570247934\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 0.766863, ISvAL: True\n",
            "Train Epoch: 0 [100/1507 (7%)]\tLoss: 0.756629, ISvAL: True\n",
            "Train Epoch: 0 [200/1507 (13%)]\tLoss: 0.728176, ISvAL: True\n",
            "Train Epoch: 0 [300/1507 (20%)]\tLoss: 0.723194, ISvAL: True\n",
            "Train Epoch: 0 [400/1507 (27%)]\tLoss: 0.787345, ISvAL: True\n",
            "Train Epoch: 0 [500/1507 (33%)]\tLoss: 0.728020, ISvAL: True\n",
            "Train Epoch: 0 [600/1507 (40%)]\tLoss: 0.726537, ISvAL: True\n",
            "Train Epoch: 0 [700/1507 (47%)]\tLoss: 0.746832, ISvAL: True\n",
            "Train Epoch: 0 [800/1507 (53%)]\tLoss: 0.773011, ISvAL: True\n",
            "Train Epoch: 0 [900/1507 (60%)]\tLoss: 0.710311, ISvAL: True\n",
            "Train Epoch: 0 [1000/1507 (67%)]\tLoss: 0.623409, ISvAL: True\n",
            "Train Epoch: 0 [1100/1507 (73%)]\tLoss: 0.621530, ISvAL: True\n",
            "Train Epoch: 0 [1200/1507 (80%)]\tLoss: 0.673523, ISvAL: True\n",
            "Train Epoch: 0 [1300/1507 (87%)]\tLoss: 0.663459, ISvAL: True\n",
            "Train Epoch: 0 [1400/1507 (93%)]\tLoss: 0.619192, ISvAL: True\n",
            "Average loss is: tensor(0.7099, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.442\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 0.706569, ISvAL: False\n",
            "Train Epoch: 1 [1000/12119 (8%)]\tLoss: 0.681491, ISvAL: False\n",
            "Train Epoch: 1 [2000/12119 (17%)]\tLoss: 0.703890, ISvAL: False\n",
            "Train Epoch: 1 [3000/12119 (25%)]\tLoss: 0.704224, ISvAL: False\n",
            "Train Epoch: 1 [4000/12119 (33%)]\tLoss: 0.707324, ISvAL: False\n",
            "Train Epoch: 1 [5000/12119 (41%)]\tLoss: 0.689788, ISvAL: False\n",
            "Train Epoch: 1 [6000/12119 (50%)]\tLoss: 0.696123, ISvAL: False\n",
            "Train Epoch: 1 [7000/12119 (58%)]\tLoss: 0.697519, ISvAL: False\n",
            "Train Epoch: 1 [8000/12119 (66%)]\tLoss: 0.685014, ISvAL: False\n",
            "Train Epoch: 1 [9000/12119 (74%)]\tLoss: 0.679773, ISvAL: False\n",
            "Train Epoch: 1 [10000/12119 (83%)]\tLoss: 0.684549, ISvAL: False\n",
            "Train Epoch: 1 [11000/12119 (91%)]\tLoss: 0.664589, ISvAL: False\n",
            "Train Epoch: 1 [12000/12119 (99%)]\tLoss: 0.674716, ISvAL: False\n",
            "Average loss is: tensor(0.6893, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5234710743801653\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 0.705646, ISvAL: True\n",
            "Train Epoch: 1 [100/1507 (7%)]\tLoss: 0.694411, ISvAL: True\n",
            "Train Epoch: 1 [200/1507 (13%)]\tLoss: 0.726502, ISvAL: True\n",
            "Train Epoch: 1 [300/1507 (20%)]\tLoss: 0.692904, ISvAL: True\n",
            "Train Epoch: 1 [400/1507 (27%)]\tLoss: 0.776486, ISvAL: True\n",
            "Train Epoch: 1 [500/1507 (33%)]\tLoss: 0.660357, ISvAL: True\n",
            "Train Epoch: 1 [600/1507 (40%)]\tLoss: 0.711013, ISvAL: True\n",
            "Train Epoch: 1 [700/1507 (47%)]\tLoss: 0.717587, ISvAL: True\n",
            "Train Epoch: 1 [800/1507 (53%)]\tLoss: 0.722699, ISvAL: True\n",
            "Train Epoch: 1 [900/1507 (60%)]\tLoss: 0.678893, ISvAL: True\n",
            "Train Epoch: 1 [1000/1507 (67%)]\tLoss: 0.618973, ISvAL: True\n",
            "Train Epoch: 1 [1100/1507 (73%)]\tLoss: 0.632850, ISvAL: True\n",
            "Train Epoch: 1 [1200/1507 (80%)]\tLoss: 0.638015, ISvAL: True\n",
            "Train Epoch: 1 [1300/1507 (87%)]\tLoss: 0.634048, ISvAL: True\n",
            "Train Epoch: 1 [1400/1507 (93%)]\tLoss: 0.615878, ISvAL: True\n",
            "Average loss is: tensor(0.6818, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.5893333333333334\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 0.663405, ISvAL: False\n",
            "Train Epoch: 2 [1000/12119 (8%)]\tLoss: 0.651342, ISvAL: False\n",
            "Train Epoch: 2 [2000/12119 (17%)]\tLoss: 0.658677, ISvAL: False\n",
            "Train Epoch: 2 [3000/12119 (25%)]\tLoss: 0.654295, ISvAL: False\n",
            "Train Epoch: 2 [4000/12119 (33%)]\tLoss: 0.645192, ISvAL: False\n",
            "Train Epoch: 2 [5000/12119 (41%)]\tLoss: 0.632270, ISvAL: False\n",
            "Train Epoch: 2 [6000/12119 (50%)]\tLoss: 0.617350, ISvAL: False\n",
            "Train Epoch: 2 [7000/12119 (58%)]\tLoss: 0.633086, ISvAL: False\n",
            "Train Epoch: 2 [8000/12119 (66%)]\tLoss: 0.617498, ISvAL: False\n",
            "Train Epoch: 2 [9000/12119 (74%)]\tLoss: 0.604777, ISvAL: False\n",
            "Train Epoch: 2 [10000/12119 (83%)]\tLoss: 0.628535, ISvAL: False\n",
            "Train Epoch: 2 [11000/12119 (91%)]\tLoss: 0.629606, ISvAL: False\n",
            "Train Epoch: 2 [12000/12119 (99%)]\tLoss: 0.587363, ISvAL: False\n",
            "Average loss is: tensor(0.6358, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.698099173553719\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 0.578975, ISvAL: True\n",
            "Train Epoch: 2 [100/1507 (7%)]\tLoss: 0.599321, ISvAL: True\n",
            "Train Epoch: 2 [200/1507 (13%)]\tLoss: 0.704253, ISvAL: True\n",
            "Train Epoch: 2 [300/1507 (20%)]\tLoss: 0.643392, ISvAL: True\n",
            "Train Epoch: 2 [400/1507 (27%)]\tLoss: 0.701099, ISvAL: True\n",
            "Train Epoch: 2 [500/1507 (33%)]\tLoss: 0.570169, ISvAL: True\n",
            "Train Epoch: 2 [600/1507 (40%)]\tLoss: 0.671272, ISvAL: True\n",
            "Train Epoch: 2 [700/1507 (47%)]\tLoss: 0.655149, ISvAL: True\n",
            "Train Epoch: 2 [800/1507 (53%)]\tLoss: 0.605834, ISvAL: True\n",
            "Train Epoch: 2 [900/1507 (60%)]\tLoss: 0.638506, ISvAL: True\n",
            "Train Epoch: 2 [1000/1507 (67%)]\tLoss: 0.663792, ISvAL: True\n",
            "Train Epoch: 2 [1100/1507 (73%)]\tLoss: 0.702429, ISvAL: True\n",
            "Train Epoch: 2 [1200/1507 (80%)]\tLoss: 0.609193, ISvAL: True\n",
            "Train Epoch: 2 [1300/1507 (87%)]\tLoss: 0.649735, ISvAL: True\n",
            "Train Epoch: 2 [1400/1507 (93%)]\tLoss: 0.664046, ISvAL: True\n",
            "Average loss is: tensor(0.6438, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6286666666666667\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/12119 (0%)]\tLoss: 0.587846, ISvAL: False\n",
            "Train Epoch: 3 [1000/12119 (8%)]\tLoss: 0.585958, ISvAL: False\n",
            "Train Epoch: 3 [2000/12119 (17%)]\tLoss: 0.574916, ISvAL: False\n",
            "Train Epoch: 3 [3000/12119 (25%)]\tLoss: 0.563076, ISvAL: False\n",
            "Train Epoch: 3 [4000/12119 (33%)]\tLoss: 0.557568, ISvAL: False\n",
            "Train Epoch: 3 [5000/12119 (41%)]\tLoss: 0.588890, ISvAL: False\n",
            "Train Epoch: 3 [6000/12119 (50%)]\tLoss: 0.565047, ISvAL: False\n",
            "Train Epoch: 3 [7000/12119 (58%)]\tLoss: 0.560089, ISvAL: False\n",
            "Train Epoch: 3 [8000/12119 (66%)]\tLoss: 0.546215, ISvAL: False\n",
            "Train Epoch: 3 [9000/12119 (74%)]\tLoss: 0.595642, ISvAL: False\n",
            "Train Epoch: 3 [10000/12119 (83%)]\tLoss: 0.502806, ISvAL: False\n",
            "Train Epoch: 3 [11000/12119 (91%)]\tLoss: 0.519825, ISvAL: False\n",
            "Train Epoch: 3 [12000/12119 (99%)]\tLoss: 0.530919, ISvAL: False\n",
            "Average loss is: tensor(0.5601, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7919834710743802\n",
            "Train Epoch: 3 [0/1507 (0%)]\tLoss: 0.537986, ISvAL: True\n",
            "Train Epoch: 3 [100/1507 (7%)]\tLoss: 0.595012, ISvAL: True\n",
            "Train Epoch: 3 [200/1507 (13%)]\tLoss: 0.719676, ISvAL: True\n",
            "Train Epoch: 3 [300/1507 (20%)]\tLoss: 0.638813, ISvAL: True\n",
            "Train Epoch: 3 [400/1507 (27%)]\tLoss: 0.736206, ISvAL: True\n",
            "Train Epoch: 3 [500/1507 (33%)]\tLoss: 0.557839, ISvAL: True\n",
            "Train Epoch: 3 [600/1507 (40%)]\tLoss: 0.675123, ISvAL: True\n",
            "Train Epoch: 3 [700/1507 (47%)]\tLoss: 0.692044, ISvAL: True\n",
            "Train Epoch: 3 [800/1507 (53%)]\tLoss: 0.585660, ISvAL: True\n",
            "Train Epoch: 3 [900/1507 (60%)]\tLoss: 0.625442, ISvAL: True\n",
            "Train Epoch: 3 [1000/1507 (67%)]\tLoss: 0.589504, ISvAL: True\n",
            "Train Epoch: 3 [1100/1507 (73%)]\tLoss: 0.662678, ISvAL: True\n",
            "Train Epoch: 3 [1200/1507 (80%)]\tLoss: 0.578778, ISvAL: True\n",
            "Train Epoch: 3 [1300/1507 (87%)]\tLoss: 0.639379, ISvAL: True\n",
            "Train Epoch: 3 [1400/1507 (93%)]\tLoss: 0.620117, ISvAL: True\n",
            "Average loss is: tensor(0.6303, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6426666666666667\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/12119 (0%)]\tLoss: 0.511112, ISvAL: False\n",
            "Train Epoch: 4 [1000/12119 (8%)]\tLoss: 0.508308, ISvAL: False\n",
            "Train Epoch: 4 [2000/12119 (17%)]\tLoss: 0.480777, ISvAL: False\n",
            "Train Epoch: 4 [3000/12119 (25%)]\tLoss: 0.459633, ISvAL: False\n",
            "Train Epoch: 4 [4000/12119 (33%)]\tLoss: 0.505328, ISvAL: False\n",
            "Train Epoch: 4 [5000/12119 (41%)]\tLoss: 0.476568, ISvAL: False\n",
            "Train Epoch: 4 [6000/12119 (50%)]\tLoss: 0.435191, ISvAL: False\n",
            "Train Epoch: 4 [7000/12119 (58%)]\tLoss: 0.527748, ISvAL: False\n",
            "Train Epoch: 4 [8000/12119 (66%)]\tLoss: 0.413542, ISvAL: False\n",
            "Train Epoch: 4 [9000/12119 (74%)]\tLoss: 0.479918, ISvAL: False\n",
            "Train Epoch: 4 [10000/12119 (83%)]\tLoss: 0.484556, ISvAL: False\n",
            "Train Epoch: 4 [11000/12119 (91%)]\tLoss: 0.428569, ISvAL: False\n",
            "Train Epoch: 4 [12000/12119 (99%)]\tLoss: 0.428356, ISvAL: False\n",
            "Average loss is: tensor(0.4794, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.8466115702479339\n",
            "Train Epoch: 4 [0/1507 (0%)]\tLoss: 0.491701, ISvAL: True\n",
            "Train Epoch: 4 [100/1507 (7%)]\tLoss: 0.605114, ISvAL: True\n",
            "Train Epoch: 4 [200/1507 (13%)]\tLoss: 0.706074, ISvAL: True\n",
            "Train Epoch: 4 [300/1507 (20%)]\tLoss: 0.634714, ISvAL: True\n",
            "Train Epoch: 4 [400/1507 (27%)]\tLoss: 0.768891, ISvAL: True\n",
            "Train Epoch: 4 [500/1507 (33%)]\tLoss: 0.532228, ISvAL: True\n",
            "Train Epoch: 4 [600/1507 (40%)]\tLoss: 0.698821, ISvAL: True\n",
            "Train Epoch: 4 [700/1507 (47%)]\tLoss: 0.698434, ISvAL: True\n",
            "Train Epoch: 4 [800/1507 (53%)]\tLoss: 0.566549, ISvAL: True\n",
            "Train Epoch: 4 [900/1507 (60%)]\tLoss: 0.652356, ISvAL: True\n",
            "Train Epoch: 4 [1000/1507 (67%)]\tLoss: 0.580504, ISvAL: True\n",
            "Train Epoch: 4 [1100/1507 (73%)]\tLoss: 0.702723, ISvAL: True\n",
            "Train Epoch: 4 [1200/1507 (80%)]\tLoss: 0.568710, ISvAL: True\n",
            "Train Epoch: 4 [1300/1507 (87%)]\tLoss: 0.688425, ISvAL: True\n",
            "Train Epoch: 4 [1400/1507 (93%)]\tLoss: 0.637312, ISvAL: True\n",
            "Average loss is: tensor(0.6355, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.634\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/12119 (0%)]\tLoss: 0.421248, ISvAL: False\n",
            "Train Epoch: 5 [1000/12119 (8%)]\tLoss: 0.396417, ISvAL: False\n",
            "Train Epoch: 5 [2000/12119 (17%)]\tLoss: 0.392853, ISvAL: False\n",
            "Train Epoch: 5 [3000/12119 (25%)]\tLoss: 0.398585, ISvAL: False\n",
            "Train Epoch: 5 [4000/12119 (33%)]\tLoss: 0.387989, ISvAL: False\n",
            "Train Epoch: 5 [5000/12119 (41%)]\tLoss: 0.358477, ISvAL: False\n",
            "Train Epoch: 5 [6000/12119 (50%)]\tLoss: 0.349726, ISvAL: False\n",
            "Train Epoch: 5 [7000/12119 (58%)]\tLoss: 0.333968, ISvAL: False\n",
            "Train Epoch: 5 [8000/12119 (66%)]\tLoss: 0.408935, ISvAL: False\n",
            "Train Epoch: 5 [9000/12119 (74%)]\tLoss: 0.391092, ISvAL: False\n",
            "Train Epoch: 5 [10000/12119 (83%)]\tLoss: 0.418413, ISvAL: False\n",
            "Train Epoch: 5 [11000/12119 (91%)]\tLoss: 0.345679, ISvAL: False\n",
            "Train Epoch: 5 [12000/12119 (99%)]\tLoss: 0.396358, ISvAL: False\n",
            "Average loss is: tensor(0.3915, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.8871074380165289\n",
            "Train Epoch: 5 [0/1507 (0%)]\tLoss: 0.582614, ISvAL: True\n",
            "Train Epoch: 5 [100/1507 (7%)]\tLoss: 0.733873, ISvAL: True\n",
            "Train Epoch: 5 [200/1507 (13%)]\tLoss: 0.798705, ISvAL: True\n",
            "Train Epoch: 5 [300/1507 (20%)]\tLoss: 0.675446, ISvAL: True\n",
            "Train Epoch: 5 [400/1507 (27%)]\tLoss: 1.008563, ISvAL: True\n",
            "Train Epoch: 5 [500/1507 (33%)]\tLoss: 0.618749, ISvAL: True\n",
            "Train Epoch: 5 [600/1507 (40%)]\tLoss: 0.774959, ISvAL: True\n",
            "Train Epoch: 5 [700/1507 (47%)]\tLoss: 0.820935, ISvAL: True\n",
            "Train Epoch: 5 [800/1507 (53%)]\tLoss: 0.692546, ISvAL: True\n",
            "Train Epoch: 5 [900/1507 (60%)]\tLoss: 0.712277, ISvAL: True\n",
            "Train Epoch: 5 [1000/1507 (67%)]\tLoss: 0.456403, ISvAL: True\n",
            "Train Epoch: 5 [1100/1507 (73%)]\tLoss: 0.590974, ISvAL: True\n",
            "Train Epoch: 5 [1200/1507 (80%)]\tLoss: 0.529440, ISvAL: True\n",
            "Train Epoch: 5 [1300/1507 (87%)]\tLoss: 0.700380, ISvAL: True\n",
            "Train Epoch: 5 [1400/1507 (93%)]\tLoss: 0.519278, ISvAL: True\n",
            "Average loss is: tensor(0.6810, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6126666666666667\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/12119 (0%)]\tLoss: 0.295426, ISvAL: False\n",
            "Train Epoch: 6 [1000/12119 (8%)]\tLoss: 0.319355, ISvAL: False\n",
            "Train Epoch: 6 [2000/12119 (17%)]\tLoss: 0.334619, ISvAL: False\n",
            "Train Epoch: 6 [3000/12119 (25%)]\tLoss: 0.325041, ISvAL: False\n",
            "Train Epoch: 6 [4000/12119 (33%)]\tLoss: 0.252577, ISvAL: False\n",
            "Train Epoch: 6 [5000/12119 (41%)]\tLoss: 0.338284, ISvAL: False\n",
            "Train Epoch: 6 [6000/12119 (50%)]\tLoss: 0.346352, ISvAL: False\n",
            "Train Epoch: 6 [7000/12119 (58%)]\tLoss: 0.343073, ISvAL: False\n",
            "Train Epoch: 6 [8000/12119 (66%)]\tLoss: 0.256327, ISvAL: False\n",
            "Train Epoch: 6 [9000/12119 (74%)]\tLoss: 0.282091, ISvAL: False\n",
            "Train Epoch: 6 [10000/12119 (83%)]\tLoss: 0.226842, ISvAL: False\n",
            "Train Epoch: 6 [11000/12119 (91%)]\tLoss: 0.264403, ISvAL: False\n",
            "Train Epoch: 6 [12000/12119 (99%)]\tLoss: 0.333813, ISvAL: False\n",
            "Average loss is: tensor(0.3059, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.9163636363636364\n",
            "Train Epoch: 6 [0/1507 (0%)]\tLoss: 0.564601, ISvAL: True\n",
            "Train Epoch: 6 [100/1507 (7%)]\tLoss: 0.761072, ISvAL: True\n",
            "Train Epoch: 6 [200/1507 (13%)]\tLoss: 0.881911, ISvAL: True\n",
            "Train Epoch: 6 [300/1507 (20%)]\tLoss: 0.709839, ISvAL: True\n",
            "Train Epoch: 6 [400/1507 (27%)]\tLoss: 1.080394, ISvAL: True\n",
            "Train Epoch: 6 [500/1507 (33%)]\tLoss: 0.592833, ISvAL: True\n",
            "Train Epoch: 6 [600/1507 (40%)]\tLoss: 0.856202, ISvAL: True\n",
            "Train Epoch: 6 [700/1507 (47%)]\tLoss: 0.874400, ISvAL: True\n",
            "Train Epoch: 6 [800/1507 (53%)]\tLoss: 0.697814, ISvAL: True\n",
            "Train Epoch: 6 [900/1507 (60%)]\tLoss: 0.772452, ISvAL: True\n",
            "Train Epoch: 6 [1000/1507 (67%)]\tLoss: 0.488186, ISvAL: True\n",
            "Train Epoch: 6 [1100/1507 (73%)]\tLoss: 0.723134, ISvAL: True\n",
            "Train Epoch: 6 [1200/1507 (80%)]\tLoss: 0.557209, ISvAL: True\n",
            "Train Epoch: 6 [1300/1507 (87%)]\tLoss: 0.796372, ISvAL: True\n",
            "Train Epoch: 6 [1400/1507 (93%)]\tLoss: 0.620139, ISvAL: True\n",
            "Average loss is: tensor(0.7318, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.624\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/12119 (0%)]\tLoss: 0.286828, ISvAL: False\n",
            "Train Epoch: 7 [1000/12119 (8%)]\tLoss: 0.196624, ISvAL: False\n",
            "Train Epoch: 7 [2000/12119 (17%)]\tLoss: 0.224520, ISvAL: False\n",
            "Train Epoch: 7 [3000/12119 (25%)]\tLoss: 0.378611, ISvAL: False\n",
            "Train Epoch: 7 [4000/12119 (33%)]\tLoss: 0.272022, ISvAL: False\n",
            "Train Epoch: 7 [5000/12119 (41%)]\tLoss: 0.264327, ISvAL: False\n",
            "Train Epoch: 7 [6000/12119 (50%)]\tLoss: 0.203248, ISvAL: False\n",
            "Train Epoch: 7 [7000/12119 (58%)]\tLoss: 0.192097, ISvAL: False\n",
            "Train Epoch: 7 [8000/12119 (66%)]\tLoss: 0.205382, ISvAL: False\n",
            "Train Epoch: 7 [9000/12119 (74%)]\tLoss: 0.163125, ISvAL: False\n",
            "Train Epoch: 7 [10000/12119 (83%)]\tLoss: 0.274059, ISvAL: False\n",
            "Train Epoch: 7 [11000/12119 (91%)]\tLoss: 0.184088, ISvAL: False\n",
            "Train Epoch: 7 [12000/12119 (99%)]\tLoss: 0.231309, ISvAL: False\n",
            "Average loss is: tensor(0.2418, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.9353719008264463\n",
            "Train Epoch: 7 [0/1507 (0%)]\tLoss: 0.658460, ISvAL: True\n",
            "Train Epoch: 7 [100/1507 (7%)]\tLoss: 0.880846, ISvAL: True\n",
            "Train Epoch: 7 [200/1507 (13%)]\tLoss: 0.988616, ISvAL: True\n",
            "Train Epoch: 7 [300/1507 (20%)]\tLoss: 0.781952, ISvAL: True\n",
            "Train Epoch: 7 [400/1507 (27%)]\tLoss: 1.265556, ISvAL: True\n",
            "Train Epoch: 7 [500/1507 (33%)]\tLoss: 0.657664, ISvAL: True\n",
            "Train Epoch: 7 [600/1507 (40%)]\tLoss: 0.939951, ISvAL: True\n",
            "Train Epoch: 7 [700/1507 (47%)]\tLoss: 0.994815, ISvAL: True\n",
            "Train Epoch: 7 [800/1507 (53%)]\tLoss: 0.788306, ISvAL: True\n",
            "Train Epoch: 7 [900/1507 (60%)]\tLoss: 0.823689, ISvAL: True\n",
            "Train Epoch: 7 [1000/1507 (67%)]\tLoss: 0.476288, ISvAL: True\n",
            "Train Epoch: 7 [1100/1507 (73%)]\tLoss: 0.777444, ISvAL: True\n",
            "Train Epoch: 7 [1200/1507 (80%)]\tLoss: 0.555394, ISvAL: True\n",
            "Train Epoch: 7 [1300/1507 (87%)]\tLoss: 0.823829, ISvAL: True\n",
            "Train Epoch: 7 [1400/1507 (93%)]\tLoss: 0.654369, ISvAL: True\n",
            "Average loss is: tensor(0.8045, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.618\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/12119 (0%)]\tLoss: 0.183701, ISvAL: False\n",
            "Train Epoch: 8 [1000/12119 (8%)]\tLoss: 0.175842, ISvAL: False\n",
            "Train Epoch: 8 [2000/12119 (17%)]\tLoss: 0.188928, ISvAL: False\n",
            "Train Epoch: 8 [3000/12119 (25%)]\tLoss: 0.159899, ISvAL: False\n",
            "Train Epoch: 8 [4000/12119 (33%)]\tLoss: 0.227010, ISvAL: False\n",
            "Train Epoch: 8 [5000/12119 (41%)]\tLoss: 0.196787, ISvAL: False\n",
            "Train Epoch: 8 [6000/12119 (50%)]\tLoss: 0.184204, ISvAL: False\n",
            "Train Epoch: 8 [7000/12119 (58%)]\tLoss: 0.231690, ISvAL: False\n",
            "Train Epoch: 8 [8000/12119 (66%)]\tLoss: 0.191734, ISvAL: False\n",
            "Train Epoch: 8 [9000/12119 (74%)]\tLoss: 0.195146, ISvAL: False\n",
            "Train Epoch: 8 [10000/12119 (83%)]\tLoss: 0.100952, ISvAL: False\n",
            "Train Epoch: 8 [11000/12119 (91%)]\tLoss: 0.126637, ISvAL: False\n",
            "Train Epoch: 8 [12000/12119 (99%)]\tLoss: 0.135691, ISvAL: False\n",
            "Average loss is: tensor(0.1859, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.9559504132231404\n",
            "Train Epoch: 8 [0/1507 (0%)]\tLoss: 0.657008, ISvAL: True\n",
            "Train Epoch: 8 [100/1507 (7%)]\tLoss: 0.871855, ISvAL: True\n",
            "Train Epoch: 8 [200/1507 (13%)]\tLoss: 1.059818, ISvAL: True\n",
            "Train Epoch: 8 [300/1507 (20%)]\tLoss: 0.817508, ISvAL: True\n",
            "Train Epoch: 8 [400/1507 (27%)]\tLoss: 1.262761, ISvAL: True\n",
            "Train Epoch: 8 [500/1507 (33%)]\tLoss: 0.686299, ISvAL: True\n",
            "Train Epoch: 8 [600/1507 (40%)]\tLoss: 1.010872, ISvAL: True\n",
            "Train Epoch: 8 [700/1507 (47%)]\tLoss: 1.023406, ISvAL: True\n",
            "Train Epoch: 8 [800/1507 (53%)]\tLoss: 0.803583, ISvAL: True\n",
            "Train Epoch: 8 [900/1507 (60%)]\tLoss: 0.885527, ISvAL: True\n",
            "Train Epoch: 8 [1000/1507 (67%)]\tLoss: 0.518680, ISvAL: True\n",
            "Train Epoch: 8 [1100/1507 (73%)]\tLoss: 0.988676, ISvAL: True\n",
            "Train Epoch: 8 [1200/1507 (80%)]\tLoss: 0.625000, ISvAL: True\n",
            "Train Epoch: 8 [1300/1507 (87%)]\tLoss: 0.937620, ISvAL: True\n",
            "Train Epoch: 8 [1400/1507 (93%)]\tLoss: 0.837780, ISvAL: True\n",
            "Average loss is: tensor(0.8658, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.628\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/12119 (0%)]\tLoss: 0.089711, ISvAL: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-502-6aaf76c919ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"real_declare\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snopes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeclareParameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"real_declare\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"snopes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snopes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-488-3573c611b217>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, dataset, hp, is_plot)\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        using_gradient_clipping=hp.grad_clip)\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_plot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplot_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-337-398bcc2a46ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_validating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m           \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_debug\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdebug_amount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JetqyT7Imn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "declare_real_results, declare_predicted_ys =  run_model(models[\"real_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"real declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZugTi6AEHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_params = {\n",
        "    \"my_model\": Hyperparameters,\n",
        "    \"sheena_model\": SheenaParameters,\n",
        "    \"broke_declare\": DeclareParameters,\n",
        "    \"real_declare\": DeclareParameters\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWBCcIDk674v",
        "colab_type": "text"
      },
      "source": [
        "##VALIDATION LAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "outputId": "9492ee2f-a65c-44d3-93a0-bf008cd3d93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    some_results = []\n",
        "  \n",
        "    for per_avg_i in range(per_avg_amount):\n",
        "      predicted_ys, model = run_model(models[model_name], datasets[data_name], all_params[model_name])\n",
        "      full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "    processed_results.append(process_results(list_to_dict(some_results)))\n",
        "    print(processed_results)\n",
        "      #avg_results.append(get_avgs(some_results))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.648372, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.651799, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.642161, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.642314, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.645262, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.639827, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.642180, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.641350, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.621805, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.630735, ISvAL: False\n",
            "Average loss is: tensor(1.6381, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5245715725806451\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.606370, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.607512, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.565960, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.629026, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.756380, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.528183, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.511506, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.716199, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.597179, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.724444, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.703453, ISvAL: True\n",
            "Average loss is: tensor(1.6315, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6026278409090909\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.644144, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.602043, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.572483, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.591847, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.587368, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.561127, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.554895, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.537610, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.583943, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.544180, ISvAL: False\n",
            "Average loss is: tensor(1.5745, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6544858870967742\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.617351, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.459865, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.532918, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.621793, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.764170, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.624956, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.526279, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.563728, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.631895, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.770831, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.531930, ISvAL: True\n",
            "Average loss is: tensor(1.6042, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6178977272727273\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.540094, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.560457, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.548154, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.538447, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.488677, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.488556, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.465334, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.459405, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.480897, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.376821, ISvAL: False\n",
            "Average loss is: tensor(1.4905, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.739205309139785\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.622460, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.399491, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.618318, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.732284, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 2.048771, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.832342, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.576776, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.519355, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.887787, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 2.061483, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.660999, ISvAL: True\n",
            "Average loss is: tensor(1.7236, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6150568181818182\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.650038, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.644555, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.642021, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.642531, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.642374, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.642302, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.637761, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.601292, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.614916, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.609027, ISvAL: False\n",
            "Average loss is: tensor(1.6319, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5383904569892473\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.638006, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.558798, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.594676, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.596645, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.705466, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.595730, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.556058, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.660692, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.581930, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.701548, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.646072, ISvAL: True\n",
            "Average loss is: tensor(1.6214, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.5990767045454546\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.607610, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.596169, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.634197, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.578175, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.511470, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.548272, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.513629, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.515880, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.545888, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.537358, ISvAL: False\n",
            "Average loss is: tensor(1.5625, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6704889112903226\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.590698, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.483041, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.559226, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.613368, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.779786, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.636909, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.598149, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.595610, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.669559, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.798206, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.592048, ISvAL: True\n",
            "Average loss is: tensor(1.6288, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.625\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.537218, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.530319, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.501360, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.457619, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.499527, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.483435, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.429110, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.530710, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.443515, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.431741, ISvAL: False\n",
            "Average loss is: tensor(1.4827, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7453377016129032\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.577208, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.587913, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.547045, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.772037, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 1.886121, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.777760, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.667462, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.606828, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.922830, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 2.003349, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.641755, ISvAL: True\n",
            "Average loss is: tensor(1.7264, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6267755681818182\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.647971, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.646699, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.635632, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.643482, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.641042, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.640030, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.641243, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.619070, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.604341, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.597738, ISvAL: False\n",
            "Average loss is: tensor(1.6326, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5369203629032258\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.584315, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.604573, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.551117, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.624535, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.714972, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.533272, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.538130, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.672286, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.617979, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.678614, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.652430, ISvAL: True\n",
            "Average loss is: tensor(1.6157, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6008522727272727\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.590523, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.610793, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.583003, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.588612, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.585086, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.559750, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.588104, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.526955, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.510788, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.495609, ISvAL: False\n",
            "Average loss is: tensor(1.5669, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6642725134408602\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.587904, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.493974, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.620536, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.600294, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.824762, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.585097, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.616866, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.588951, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.704974, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.758594, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.602687, ISvAL: True\n",
            "Average loss is: tensor(1.6350, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6260653409090909\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.550406, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.565781, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.495051, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.467592, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.491235, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.522764, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.433260, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.396745, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.474643, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.406929, ISvAL: False\n",
            "Average loss is: tensor(1.4750, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7484879032258065\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.588123, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.416367, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.661035, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.685490, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 1.965156, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.631947, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.755291, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.480904, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.864198, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.892599, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.772664, ISvAL: True\n",
            "Average loss is: tensor(1.7013, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6409801136363636\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.642383, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.644700, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.642514, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.640267, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.644307, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.640858, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.638298, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.614969, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.638020, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.592378, ISvAL: False\n",
            "Average loss is: tensor(1.6341, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5364163306451613\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.630725, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.575251, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.574113, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.603725, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.743805, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.572662, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.530401, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.714603, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.594551, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.740770, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.682222, ISvAL: True\n",
            "Average loss is: tensor(1.6330, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6004971590909091\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.571805, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.611261, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.570269, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.601677, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.573082, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.589711, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.624400, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.589761, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.575213, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.579995, ISvAL: False\n",
            "Average loss is: tensor(1.5867, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6441112231182796\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.610518, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.595627, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.686437, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.604932, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.887008, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.533516, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.601159, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.690060, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.669195, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.807670, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.666445, ISvAL: True\n",
            "Average loss is: tensor(1.6684, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6154119318181818\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.547574, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.523063, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.567662, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.529477, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.553017, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.503569, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.481087, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.423942, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.466080, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.504755, ISvAL: False\n",
            "Average loss is: tensor(1.5232, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7100554435483871\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.561820, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.517170, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.613230, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.615277, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 1.883953, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.528692, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.647091, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.614308, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.729091, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.818031, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.616465, ISvAL: True\n",
            "Average loss is: tensor(1.6496, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6299715909090909\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.650730, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.647617, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.646840, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.642055, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.642773, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.638694, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.638889, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.641178, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.606133, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.605000, ISvAL: False\n",
            "Average loss is: tensor(1.6370, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5280997983870968\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.605960, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.594761, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.574600, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.580975, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.741225, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.509893, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.514919, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.706912, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.566183, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.699910, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.698101, ISvAL: True\n",
            "Average loss is: tensor(1.6176, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6161221590909091\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.607931, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.602757, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.558205, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.602277, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.557534, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.541794, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.531469, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.549504, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.508871, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.513715, ISvAL: False\n",
            "Average loss is: tensor(1.5697, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6664986559139785\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.712600, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.517632, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.668999, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.612909, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.873802, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.662031, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.560976, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.666096, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.660602, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.815433, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.621100, ISvAL: True\n",
            "Average loss is: tensor(1.6702, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6111505681818182\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.530793, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.546347, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.501005, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.446146, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.504477, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.412643, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.430238, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.482508, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.474005, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.402384, ISvAL: False\n",
            "Average loss is: tensor(1.4884, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7342909946236559\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.692346, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.605355, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.698219, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.652848, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 2.028707, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.719909, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.502732, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.677061, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.857886, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.949969, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.664010, ISvAL: True\n",
            "Average loss is: tensor(1.7317, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6232244318181818\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.647721, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.647633, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.642143, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.643803, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.644183, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.644366, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.636460, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.641956, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.626298, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.613363, ISvAL: False\n",
            "Average loss is: tensor(1.6401, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5187331989247311\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.602193, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.653982, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.579763, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.656558, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.678254, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.565864, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.608243, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.644887, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.634198, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.642387, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.672209, ISvAL: True\n",
            "Average loss is: tensor(1.6308, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.5600142045454546\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.598779, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.605129, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.631197, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.598863, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.610074, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.633586, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.557358, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.564346, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.561683, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.573265, ISvAL: False\n",
            "Average loss is: tensor(1.5900, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6330225134408602\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.476133, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.674992, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.557055, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.737114, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.779412, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.432207, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.623820, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.713711, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.709028, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.745159, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.652800, ISvAL: True\n",
            "Average loss is: tensor(1.6456, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.62109375\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.636403, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.537946, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.550493, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.572591, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.529782, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.530966, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.518164, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.553740, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.529450, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.556540, ISvAL: False\n",
            "Average loss is: tensor(1.5386, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6918262768817204\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.473819, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.648725, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.498310, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.739716, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 1.778562, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.428929, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.619429, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.638119, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.729301, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.759775, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.686423, ISvAL: True\n",
            "Average loss is: tensor(1.6365, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6285511363636364\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.643479, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.643303, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.643427, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.639984, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.643974, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.647444, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.638555, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.622157, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.621216, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.611775, ISvAL: False\n",
            "Average loss is: tensor(1.6330, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5418346774193549\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.630032, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.565544, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.570291, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.601609, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.745103, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.573207, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.523665, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.695527, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.586676, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.741574, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.664788, ISvAL: True\n",
            "Average loss is: tensor(1.6271, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6015625\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.592235, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.581063, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.598057, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.616544, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.560038, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.554884, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.518085, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.563746, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.562076, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.536366, ISvAL: False\n",
            "Average loss is: tensor(1.5692, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.663138440860215\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.600315, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.505509, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.589280, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.575590, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.850282, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.627986, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.578782, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.612850, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.672076, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.815283, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.519259, ISvAL: True\n",
            "Average loss is: tensor(1.6316, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6299715909090909\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.504467, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.503056, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.488634, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.500751, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.513390, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.483455, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.451732, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.502757, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.461937, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.457023, ISvAL: False\n",
            "Average loss is: tensor(1.4876, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7384072580645161\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.510260, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.570325, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.516695, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.652412, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 1.963230, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.668744, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.636471, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.577611, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.899201, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.928275, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.532674, ISvAL: True\n",
            "Average loss is: tensor(1.6778, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6356534090909091\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.641560, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.641383, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.641721, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.641840, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.640507, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.641989, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.638705, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.607905, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.591537, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.571564, ISvAL: False\n",
            "Average loss is: tensor(1.6311, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5388944892473119\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.629859, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.591711, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.575752, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.605269, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.765089, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.546247, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.529804, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.705533, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.599842, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.751318, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.650650, ISvAL: True\n",
            "Average loss is: tensor(1.6319, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6122159090909091\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.615631, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.607402, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.601344, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.528504, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.609033, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.557511, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.560688, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.574619, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.563963, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.554132, ISvAL: False\n",
            "Average loss is: tensor(1.5861, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6418430779569892\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.687873, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.615143, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.717958, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.628614, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.936790, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.613309, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.605259, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.727124, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.716507, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.879069, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.688793, ISvAL: True\n",
            "Average loss is: tensor(1.7106, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6136363636363636\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.556992, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.519128, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.546408, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.563600, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.518964, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.521477, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.548621, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.527725, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.532166, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.470899, ISvAL: False\n",
            "Average loss is: tensor(1.5367, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6964465725806451\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.652618, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.602401, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.786238, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.643193, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 1.968010, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.629979, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.646273, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.656684, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.867464, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.877927, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.736390, ISvAL: True\n",
            "Average loss is: tensor(1.7334, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6196732954545454\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.641606, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.643399, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.642019, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.642713, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.642766, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.641668, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.619385, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.622819, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.619840, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.618859, ISvAL: False\n",
            "Average loss is: tensor(1.6316, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5447328629032258\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.636130, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.591992, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.613230, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.607012, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.775127, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.543673, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.530764, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.715497, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.602300, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.760417, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.677174, ISvAL: True\n",
            "Average loss is: tensor(1.6412, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6097301136363636\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.601286, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.599233, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.591425, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.607578, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.576473, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.601967, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.564858, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.553875, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.607350, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.549292, ISvAL: False\n",
            "Average loss is: tensor(1.5843, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.6450772849462365\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.581983, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.629901, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.640440, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.616128, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.828550, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.531466, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.571362, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.679330, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.656995, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.781288, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.612624, ISvAL: True\n",
            "Average loss is: tensor(1.6482, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.625\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.552345, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.530832, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.494537, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.528813, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.522654, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.538132, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.518159, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.524358, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.519647, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.486072, ISvAL: False\n",
            "Average loss is: tensor(1.5286, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7024109543010753\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.614057, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.708144, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.708055, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.674107, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 2.038968, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.564098, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.627702, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.665731, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.847807, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.893539, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.679258, ISvAL: True\n",
            "Average loss is: tensor(1.7292, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6352982954545454\n",
            "torch.Size([33766, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.651860, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.644114, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.640511, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.645825, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.641751, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.642428, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.638818, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.646611, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.643601, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.628764, ISvAL: False\n",
            "Average loss is: tensor(1.6410, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.5192792338709677\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.596820, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.681894, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.563666, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.686459, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 1.684756, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.575650, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.630889, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.659342, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.649676, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.633147, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.692785, ISvAL: True\n",
            "Average loss is: tensor(1.6414, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.5394176136363636\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.601156, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.599881, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.613573, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.576843, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.604072, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.566889, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.588979, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.528883, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.570423, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.566261, ISvAL: False\n",
            "Average loss is: tensor(1.5873, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.635710685483871\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.474354, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.669413, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.504852, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.632916, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 1.674625, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.472840, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.629824, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.603593, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 1.703875, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.766777, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.701506, ISvAL: True\n",
            "Average loss is: tensor(1.6213, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6008522727272727\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.549643, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.550260, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.577724, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.492262, ISvAL: False\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.499719, ISvAL: False\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.540706, ISvAL: False\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.509040, ISvAL: False\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.560441, ISvAL: False\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.481835, ISvAL: False\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.537220, ISvAL: False\n",
            "Average loss is: tensor(1.5191, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7089213709677419\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.468734, ISvAL: True\n",
            "Train Epoch: 2 [256/2884 (9%)]\tLoss: 1.676290, ISvAL: True\n",
            "Train Epoch: 2 [512/2884 (18%)]\tLoss: 1.481989, ISvAL: True\n",
            "Train Epoch: 2 [768/2884 (27%)]\tLoss: 1.675511, ISvAL: True\n",
            "Train Epoch: 2 [1024/2884 (36%)]\tLoss: 1.799995, ISvAL: True\n",
            "Train Epoch: 2 [1280/2884 (45%)]\tLoss: 1.428779, ISvAL: True\n",
            "Train Epoch: 2 [1536/2884 (55%)]\tLoss: 1.539381, ISvAL: True\n",
            "Train Epoch: 2 [1792/2884 (64%)]\tLoss: 1.587931, ISvAL: True\n",
            "Train Epoch: 2 [2048/2884 (73%)]\tLoss: 1.823746, ISvAL: True\n",
            "Train Epoch: 2 [2304/2884 (82%)]\tLoss: 1.910807, ISvAL: True\n",
            "Train Epoch: 2 [2560/2884 (91%)]\tLoss: 1.731947, ISvAL: True\n",
            "Average loss is: tensor(1.6477, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6388494318181818\n",
            "[('my_model', 'politifact', precision    0.640554\n",
            "recall       0.669783\n",
            "accuracy     0.657198\n",
            "f1           0.633724\n",
            "auc          0.640554\n",
            "dtype: float64, precision    0.014518\n",
            "recall       0.017881\n",
            "accuracy     0.012916\n",
            "f1           0.018936\n",
            "auc          0.014518\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.639291, ISvAL: False\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.621920, ISvAL: False\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.559864, ISvAL: False\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.557339, ISvAL: False\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.509965, ISvAL: False\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.435725, ISvAL: False\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.373930, ISvAL: False\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.295551, ISvAL: False\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.286433, ISvAL: False\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.215617, ISvAL: False\n",
            "Average loss is: tensor(1.4539, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.7134996639784946\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.500160, ISvAL: True\n",
            "Train Epoch: 0 [256/2884 (9%)]\tLoss: 1.721844, ISvAL: True\n",
            "Train Epoch: 0 [512/2884 (18%)]\tLoss: 1.673943, ISvAL: True\n",
            "Train Epoch: 0 [768/2884 (27%)]\tLoss: 1.625862, ISvAL: True\n",
            "Train Epoch: 0 [1024/2884 (36%)]\tLoss: 2.014695, ISvAL: True\n",
            "Train Epoch: 0 [1280/2884 (45%)]\tLoss: 1.363391, ISvAL: True\n",
            "Train Epoch: 0 [1536/2884 (55%)]\tLoss: 1.510337, ISvAL: True\n",
            "Train Epoch: 0 [1792/2884 (64%)]\tLoss: 1.751821, ISvAL: True\n",
            "Train Epoch: 0 [2048/2884 (73%)]\tLoss: 1.794158, ISvAL: True\n",
            "Train Epoch: 0 [2304/2884 (82%)]\tLoss: 1.845695, ISvAL: True\n",
            "Train Epoch: 0 [2560/2884 (91%)]\tLoss: 1.887916, ISvAL: True\n",
            "Average loss is: tensor(1.6991, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.5862926136363636\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.096114, ISvAL: False\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.064843, ISvAL: False\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.050304, ISvAL: False\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 0.981898, ISvAL: False\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 0.935618, ISvAL: False\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 0.835904, ISvAL: False\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 0.788744, ISvAL: False\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 0.746695, ISvAL: False\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 0.682486, ISvAL: False\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 0.601706, ISvAL: False\n",
            "Average loss is: tensor(0.8883, device='cuda:0', dtype=torch.float64) while validation_status: False\n",
            "Accuracy of the model 0.9315776209677419\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.549517, ISvAL: True\n",
            "Train Epoch: 1 [256/2884 (9%)]\tLoss: 1.459446, ISvAL: True\n",
            "Train Epoch: 1 [512/2884 (18%)]\tLoss: 1.760167, ISvAL: True\n",
            "Train Epoch: 1 [768/2884 (27%)]\tLoss: 1.708919, ISvAL: True\n",
            "Train Epoch: 1 [1024/2884 (36%)]\tLoss: 2.175684, ISvAL: True\n",
            "Train Epoch: 1 [1280/2884 (45%)]\tLoss: 1.033851, ISvAL: True\n",
            "Train Epoch: 1 [1536/2884 (55%)]\tLoss: 1.445424, ISvAL: True\n",
            "Train Epoch: 1 [1792/2884 (64%)]\tLoss: 1.408200, ISvAL: True\n",
            "Train Epoch: 1 [2048/2884 (73%)]\tLoss: 2.047006, ISvAL: True\n",
            "Train Epoch: 1 [2304/2884 (82%)]\tLoss: 1.733560, ISvAL: True\n",
            "Train Epoch: 1 [2560/2884 (91%)]\tLoss: 1.706124, ISvAL: True\n",
            "Average loss is: tensor(1.6389, device='cuda:0', dtype=torch.float64) while validation_status: True\n",
            "Accuracy of the model 0.6012073863636364\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 0.543085, ISvAL: False\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 0.495345, ISvAL: False\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 0.489521, ISvAL: False\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 0.458725, ISvAL: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-195-6293a8f15daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mper_avg_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_avg_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mfull_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0msome_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-f3e2be9f6c27>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, dataset, hp)\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        using_gradient_clipping=True)\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;31m#plot_stuff(hp.epochs, losses,val_losses, accuracies, val_accuracies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-398bcc2a46ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_validating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m           \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_debug\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdebug_amount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for result in full_results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWgiTDHE2tP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}