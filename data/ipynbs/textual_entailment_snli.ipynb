{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4NbGwIC9-z",
        "colab_type": "text"
      },
      "source": [
        "##HAHA ITS TIME TO SPEND 5 HOURS DOWNLOADING THINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "fdb032d5-6075-482d-adc8-f70cc600508e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-08 14:04:23--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  33.7MB/s    in 2.7s    \n",
            "\n",
            "2020-03-08 14:04:26 (33.7 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "c8b769b4-e0a0-4fc9-9186-6bdaec83cef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-08 14:04:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-08 14:04:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-08 14:04:35--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 26s  \n",
            "\n",
            "2020-03-08 14:11:01 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "27b26b7b-8a06-48de-c97a-cd02fa6a23ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-08 14:11:33--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  5.11MB/s    in 0.9s    \n",
            "\n",
            "2020-03-08 14:11:34 (5.11 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "02a921aa-7116-41a3-ce82-b9e9e9d51c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37ac18db-541b-46aa-b6ba-6a93c60027fd"
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1-baseline.git\n",
        "import sys\n",
        "sys.path.insert(1, 'fnc-1-baseline/utils')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fnc-1-baseline' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "792a1f87-2ffd-43fb-af4a-e4cab75aff86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-08 14:11:38--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  5.13MB/s    in 1.0s    \n",
            "\n",
            "2020-03-08 14:11:40 (5.13 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "outputId": "155bc636-4080-47dd-aec4-f249f5e0fde1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd\n",
        "from score import report_score"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJXKPPODFqp",
        "colab_type": "text"
      },
      "source": [
        "##LOOK AT ALL THIS CODE TO IMPORT DATA GOD THERE MUST BE SOMETHING WRONG WITH ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "77fe0d20-7592-4aec-8088-8f325c3015b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "79c9eab2-453c-4bf3-b6dc-cb0f0fa19e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "\n",
        "def split_test(facts):\n",
        "   unique = facts.drop_duplicates(\"claim_text\")\n",
        "   train_unique, val_unique = train_test_split(unique, test_size=0.1, random_state=8)\n",
        "   val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "   train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "   return train_facts, val_facts\n",
        "\n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/competition_test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/competition_test_stances.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "train_challenge, val_challenge = train_test_split(train_challenge, test_size=0.2, random_state=8)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "\n",
        "train_challenge = train_challenge.rename(columns={\"articleBody\": \"article\", \"Headline\": \"claim_text\", \"Stance\": \"cred_label\"})\n",
        "test_challenge = test_challenge.rename(columns={\"articleBody\": \"article\", \"Headline\": \"claim_text\", \"Stance\": \"cred_label\"})\n",
        "print(train_challenge.head())\n",
        "train_challenge, val_challenge = split_test(train_challenge)\n"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Body ID  ... cred_label\n",
            "11142      686  ...          2\n",
            "4398       251  ...          2\n",
            "30345     1689  ...          2\n",
            "41235     2154  ...          1\n",
            "33237     1829  ...          2\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "challenge_mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None, is_folding=False):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_text\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  \n",
        "  if is_folding:\n",
        "    results = []\n",
        "    folded = KFold(n_splits=10, shuffle=True)\n",
        "    splitted_object = folded.split(unique)\n",
        "    for train_result, test_result in splitted_object:\n",
        "      train_ilocs = unique.iloc[train_result][\"claim_text\"]\n",
        "      test_ilocs = unique.iloc[test_result][\"claim_text\"]\n",
        "      results.append((facts[facts[\"claim_text\"].isin(train_ilocs)], facts[facts[\"claim_text\"].isin(test_ilocs)]))\n",
        "\n",
        "    return results\n",
        "\n",
        "  train_unique, big_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "  val_unique, test_unique = train_test_split(big_unique, test_size=0.5, random_state=8)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_text\"].isin(test_unique[\"claim_text\"])]\n",
        "  val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "  train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "  return train_facts, test_facts, val_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts, val_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes, val_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "4b83dcd9-6031-4068-f38e-153f28d73d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>vice news we dont have a timeline on the decis...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>a schedule i narcotic along with heroin and ec...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>now do you think you were maybe talking just a...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>made to the new yorker that marijuana is no mo...</td>\n",
              "      <td>time.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jun_27_donald-trump_white-house-criticism...</td>\n",
              "      <td>obamacare signed law cbo estimated 23 million ...</td>\n",
              "      <td>donald trump</td>\n",
              "      <td>about the affordable health care act in its da...</td>\n",
              "      <td>eugeneweekly.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4849</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama to ban guns from 42 million social secur...</td>\n",
              "      <td>bearingarms.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama is looking to ban social security recipi...</td>\n",
              "      <td>rightwingnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4851</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>main navigation recent posts obama to ban 42 m...</td>\n",
              "      <td>downtrend.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4852</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>get news like this in your facebook news feed ...</td>\n",
              "      <td>thegatewaypundit.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>to prove everyone wrong yet again this time it...</td>\n",
              "      <td>zerohedge.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...        article_source\n",
              "187            1  ...            reason.com\n",
              "188            1  ...            reason.com\n",
              "189            1  ...               cnn.com\n",
              "190            1  ...              time.com\n",
              "526            1  ...      eugeneweekly.com\n",
              "...          ...  ...                   ...\n",
              "4849           0  ...       bearingarms.com\n",
              "4850           0  ...     rightwingnews.com\n",
              "4851           0  ...         downtrend.com\n",
              "4852           0  ...  thegatewaypundit.com\n",
              "4853           0  ...         zerohedge.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Beq65oTgcBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, val_loader, test_data, val_data, weight, tokeniser):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.test_data = test_data\n",
        "    self.val_data = val_data\n",
        "    self.weight = weight\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) \n",
        "    self.word_embeddings_large = load_glove_embeddings(\"glove.6B.300d.txt\", tokeniser.word_to_id, 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4lYd83j-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\"claim_text\", \"article\"]\n",
        "big_labels = [\"claim_text\", \"article\", \"article_source\"]\n",
        "\n",
        "def get_list(panda, labels):\n",
        "  label_to_data = {}\n",
        "  for label in labels:\n",
        "    label_to_data[label] = panda[label]\n",
        "\n",
        "  x_list = convert_to_lists(label_to_data)\n",
        "  if labels[0] == \"claim_text\":\n",
        "    y_list = panda[\"cred_label\"].tolist()\n",
        "  else:\n",
        "    y_list = panda[\"Stance\"].tolist()\n",
        "  return x_list, y_list\n",
        "\n",
        "def get_loader(x, y, vocab_size, max_length, batch_size, name, training=True, drop_last=True):\n",
        "  stuff = []\n",
        "  for key in x:\n",
        "    stuff.append(torch.from_numpy(x[key]).type(torch.LongTensor))\n",
        "  stuff.append(torch.from_numpy(y).type(torch.DoubleTensor))\n",
        "  tensorset = data_utils.TensorDataset(*stuff)\n",
        "  loader = data_utils.DataLoader(tensorset, batch_size=batch_size, drop_last=drop_last, shuffle=training)\n",
        "  loader.name = name\n",
        "\n",
        "  return loader\n",
        "\n",
        "  \n",
        "def get_dataset(train, test, val, vocab_size, max_length, batch_size, labels, name, weight=None):\n",
        "  is_challenge = labels[0] == \"articleBody\"\n",
        "  train_list_x, train_list_y = get_list(train, labels)\n",
        "\n",
        "  test_list_x, test_list_y = get_list(test, labels)\n",
        "  val_list_x, val_list_y = get_list(val, labels)\n",
        "\n",
        "\n",
        "\n",
        "  #tokenising various stuff, setting up numpy dictionaries :)\n",
        "  tokeniser = Tokeniser(train_list_x, vocab_size, max_length)\n",
        "  x_train = tokeniser.do_everything(train_list_x)\n",
        "  x_test = tokeniser.do_everything(test_list_x)\n",
        "  x_val = tokeniser.do_everything(val_list_x)\n",
        "  y_train = np.array(train_list_y, dtype=np.float32)\n",
        "  y_test = np.array(test_list_y, dtype=np.float32)\n",
        "  y_val = np.array(val_list_y, dtype=np.float32)\n",
        "  \n",
        "  #datasets/loaders\n",
        "  train_loader = get_loader(x_train, y_train, vocab_size, max_length, batch_size, name, True)\n",
        "  test_loader = get_loader(x_test, y_test, vocab_size, max_length, batch_size, name, False, drop_last=False)\n",
        "  val_loader = get_loader(x_val, y_val, vocab_size, max_length, batch_size, name, False)\n",
        "  return Dataset(train_loader, test_loader, val_loader, y_test, y_val, tokeniser)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MMrKlu6_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "outputId": "15d35c6a-5003-4a9a-cd06-03e5acee0b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "\n",
        "snopes_dataset = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "fact_dataset = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "challenge_dataset = get_dataset(train_challenge, test_challenge, val_challenge, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"challenge_data\")\n",
        "big_snopes = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "big_fact = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39002\n",
            "39002\n",
            "33766\n",
            "33766\n",
            "27708\n",
            "27708\n",
            "44589\n",
            "44589\n",
            "37174\n",
            "37174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    \n",
        "    self.embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.normaliser = torch.nn.BatchNorm1d(self.embedding_size)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x[:, 0:self.hp.max_length])\n",
        "    #embedding = self.normaliser(embedding.transpose(1,2))\n",
        "    embedding = self.dropout(embedding)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    tanh_W_H = self.dropout(tanh_W_H)\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.premise_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "    self.hp = hp\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    premise_factor = self.premise_dropout(premise_factor)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    hypothesis_factor = self.hypothesis_dropout(hypothesis_factor)\n",
        "    if self.hp.use_better:\n",
        "      return better_mush(premise_factor,hypothesis_factor)\n",
        "    else:\n",
        "      return premise_factor * hypothesis_factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=hp.mlp_one)\n",
        "    self.linear2 = torch.nn.Linear(hp.mlp_one, hp.mlp_two)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(hp.mlp_two, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "      x = self.dropout1(x)\n",
        "    else:\n",
        "      x = torch.relu(self.linear1(x.reshape(self.hp.batch_size, -1)))\n",
        "      x = self.dropout1(x)\n",
        "      x = torch.relu(self.linear2(x))\n",
        "      x = self.dropout2(x)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    print(word_embeddings.shape)\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout)]\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    self.dropouts[1](premise_embedding)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    self.dropouts[3](hypothesis_embedding)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    self.dropouts[4](factorised_mush)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention*premise_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWRHhOxCjFl",
        "colab_type": "text"
      },
      "source": [
        "##EVAL SUMMARY :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout)]*5\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    processed_premise = self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    premise_embedding = self.dropouts[1](premise_embedding)\n",
        "    \n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    processed_hypothesis = self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    hypothesis_embedding = self.dropouts[3](hypothesis_embedding)\n",
        "    if self.hp.use_better:\n",
        "      combined = better_mush(premise_embedding, hypothesis_embedding)\n",
        "    else:\n",
        "      combined = premise_embedding * hypothesis_embedding\n",
        "    \n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    avg = self.dropouts[4](avg)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      output = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      output = torch.sigmoid(self.final_linear(x))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(10000, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 8)\n",
        "    self.linear_almost_there = torch.nn.Linear(8, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.dropout0 = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    lengths = self.hp.max_length - (premise == 0).sum(dim=1)\n",
        "    lengths = lengths.repeat(50, 1).transpose(0,1)\n",
        "    summed_embeddings = torch.sum(embeddings, 1)\n",
        "    mean_embeddings = torch.unsqueeze(summed_embeddings / lengths, 1) #change to accurate size of lenfgth\n",
        "    flattened_embeddings = mean_embeddings.reshape(self.hp.batch_size, -1)\n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100]).reshape(self.hp.batch_size, -1)\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((flattened_embeddings.repeat(1, 100), added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    attention_weights = softmax(torch.tanh(self.premise_linear(main_embeddings)))#TODO: turn into row vector\n",
        "\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, attention_weights.unsqueeze(2))\n",
        "    \n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    new_lengths = lengths.repeat(1, 2)\n",
        "\n",
        "    avg = torch.sum(combined, 1)/new_lengths #todo: fix padding\n",
        "    avg = self.dropout0(avg)\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    smaller = self.dropout1(smaller)\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    even_smaller = self.dropout2(even_smaller)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      output = softmax(self.final_linear(even_smaller))\n",
        "    else:\n",
        "      output = torch.sigmoid(self.final_linear(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# given the losses, checks if it should stop (or not :))\n",
        "# this performs early stopping -\n",
        "def should_stop(losses, train_losses, limit, threshold=-0.01):\n",
        "  shouldnt_stop = False \n",
        "  is_learning = False\n",
        "  #print(\"losses:\",losses)\n",
        "  #print(\"train_losses:\", train_losses)\n",
        "  \n",
        "  if len(losses) < limit:\n",
        "    return False \n",
        "  \n",
        "  last = losses[-1]\n",
        "  last_train = train_losses[-1]\n",
        "\n",
        "  if limit == 2:\n",
        "    #print(\"Threshold: {}, result {}\".format(threshold, last-losses[-2]))\n",
        "    return last - losses[-2] >= threshold\n",
        "  \n",
        "  for i in range(-2,- limit-1,-1):\n",
        "    shouldnt_stop = (shouldnt_stop or last - losses[i] < threshold)\n",
        "    last = losses[i]\n",
        "  return not shouldnt_stop\n",
        "def get_l2(model, filters):\n",
        "  reg_loss = None\n",
        "  for param in model.parameters():\n",
        "    if param.shape[0] in filters:\n",
        "      if reg_loss is None:\n",
        "        reg_loss = torch.sum(param**2)\n",
        "      else:\n",
        "        reg_loss = reg_loss + param.norm(2)**2\n",
        "  return reg_loss\n",
        "\n",
        "def train(model=None, \n",
        "          dataset = None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "  \n",
        "\n",
        "  is_binary = hp.num_classes == 1\n",
        "  is_validating = False\n",
        "  has_stopped = False\n",
        "  has_decreased = False\n",
        "\n",
        "  if is_binary:\n",
        "    loss_function=torch.nn.BCELoss()\n",
        "  else:\n",
        "    loss_function=torch.nn.CrossEntropyLoss()\n",
        "  \n",
        "  if dataset.train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif dataset.train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  elif dataset.train_loader.name == \"challenge_data\" and hp.num_classes != 4:\n",
        "      raise ValueError(\"Four classes are needed for challenge fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    val_results = []\n",
        "    for i in range(2):\n",
        "      \n",
        "      total_loss = 0\n",
        "      batch_count = 0\n",
        "      correct = 0\n",
        "      penal = 0\n",
        "      l2 = 0\n",
        "\n",
        "      if is_validating:\n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.val_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 1\n",
        "      elif has_stopped:\n",
        "        \n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.train_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 10\n",
        "      else:\n",
        "        model.train(True)\n",
        "        #print(\"im trainin friends!!\")\n",
        "        loader = dataset.train_loader\n",
        "        torch.enable_grad()\n",
        "        debug_amount = 10\n",
        "      \n",
        "      for batch_index, train_data in enumerate(loader):\n",
        "      #setting everything up\n",
        "        model.hidden_state = model.reset_for_testing(train_data[0].shape[0])\n",
        "        train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "        predicted_y, attention = model(*train_data[:-1])\n",
        "        actual_y = train_data[-1]\n",
        "        squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "        if hp.C > 0:\n",
        "          attentionT = attention.transpose(1,2)\n",
        "          identity = torch.eye(attention.size(1))\n",
        "          identity = Variable(identity.unsqueeze(0).expand(loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "          penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "        if hp.decay > 0:\n",
        "          l2 = get_l2(model, hp.filters)\n",
        "\n",
        "      #get loss, accuracy\n",
        "        if is_binary:\n",
        "          loss = loss_function(squeezed_y, actual_y.double())\n",
        "          loss += hp.C * penal/loader.batch_size + hp.decay * l2\n",
        "          correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "          if is_validating:\n",
        "            val_results.append(torch.round(squeezed_y))\n",
        "          \n",
        "        else:\n",
        "          loss = loss_function(squeezed_y,actual_y.long())\n",
        "          loss += hp.C * (penal/loader.batch_size) + hp.decay * l2\n",
        "          correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "          if is_validating:\n",
        "            val_results.append(torch.argmax(squeezed_y, 1))\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if using_gradient_clipping:\n",
        "          torch.nn.utils.clip_grad_norm(model.parameters(), hp.grad_clip_amount)\n",
        "      \n",
        "      #cleaning up regularisation\n",
        "        if hp.C > 0:\n",
        "          del(penal)\n",
        "          del(identity)\n",
        "          del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "        optimiser.zero_grad()\n",
        "        if not is_validating and not has_stopped:\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "        if hp.is_debug and batch_index % debug_amount == 0:\n",
        "          print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, ISvAL: {}, HasStopped: {}\".format(\n",
        "              epoch, batch_index * len(train_data[0]), len(loader.dataset),\n",
        "              100. * batch_index / len(loader), loss.item(), is_validating, has_stopped\n",
        "          ))\n",
        "\n",
        "      \n",
        "        batch_count += 1\n",
        "      \n",
        "        free_data(train_data)\n",
        "      correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "      accuracy = correct_but_numpy / float(batch_count * loader.batch_size)\n",
        "\n",
        "      if (not is_validating):\n",
        "        losses.append(total_loss/batch_count)\n",
        "        accuracies.append(accuracy)\n",
        "      else:\n",
        "        val_losses.append(total_loss/batch_count)\n",
        "        val_accuracies.append(accuracy)\n",
        "      \n",
        "      if not has_decreased:\n",
        "        has_decreased = not (should_stop(val_losses, losses, 2, hp.early_threshold)) and len(val_losses) > 1\n",
        "      if hp.use_early_stopping and is_validating:\n",
        "        has_stopped = should_stop(val_losses,losses, hp.early_stopping, hp.early_threshold) and has_decreased\n",
        "\n",
        "      print(\"Average loss is:\",total_loss/batch_count, \"while validation_status:\", is_validating, \"and stopping_status\", has_stopped)\n",
        "      print(\"Accuracy of the model\", accuracy)\n",
        "      print(\"batch count???\", batch_count)\n",
        "\n",
        "\n",
        "\n",
        "      is_validating = not is_validating\n",
        "  return losses, val_losses, accuracies, val_accuracies, torch.cat(val_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, val_losses, accuracies, val_accuracies, title=\"sup nerds\"):\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  \n",
        "  ax1.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Train Accuracy\")\n",
        "  ax1.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  ax1.plot(range(1, epochs+1), val_accuracies, scalex=True, scaley=True, label=\"Val Accuracy\")\n",
        "  ax1.annotate(str(val_accuracies[-1]), xy=(epochs,val_accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  ax2.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Train Loss\")\n",
        "  ax2.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  ax2.plot(range(1, epochs+1), val_losses,scalex=True, scaley=True, label=\"Val Loss\")\n",
        "  ax2.annotate(str(val_losses[-1]), xy=(epochs,val_losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  ax1.legend()\n",
        "  ax2.legend()\n",
        "\n",
        "  ax1.set_xlabel(\"Epochs\", fontsize=16)\n",
        "  ax1.set_ylabel(\"Accuracy\", fontsize=16)\n",
        "  ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
        "  ax2.set_ylabel(\"Loss\", fontsize=16)\n",
        "  plt.title(title)\n",
        "  fig.tight_layout()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2DLo4OoUO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels, is_binary):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (is_binary):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc_full))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp, is_plot=True):\n",
        "  runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "\n",
        "  optimiser = torch.optim.Adam(runnable_model.parameters(), lr=hp.lr)\n",
        "  losses, val_losses, accuracies, val_accuracies, val_results = train(model=runnable_model,\n",
        "                       dataset = dataset,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=hp.grad_clip)\n",
        "  if is_plot:\n",
        "    plot_stuff(hp.epochs, losses,val_losses, accuracies, val_accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "  evaluation_summary(\"VALIDATION\", val_results.cpu().detach() ,dataset.val_data[:len(val_results)], hp.num_classes==1)\n",
        "  check_loader = dataset.test_loader\n",
        "\n",
        "  predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         check_loader,\n",
        "         hp)\n",
        "\n",
        "  return predicted_ys, runnable_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc_full}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oMDNooNrMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "##RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": fact_dataset,\n",
        "    \"snopes\": snopes_dataset,\n",
        "    \"challenge\": challenge_dataset\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    \"my_model_better\": TextualEntailmentModel,\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    \"sheena_model_better\": BaselineSentenceEntailment,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1ykUhFf7BCH",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 10\n",
        "  mlp_one = 60\n",
        "  mlp_two = 10\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 10\n",
        "  inner_dropout = 0.2\n",
        "  outer_dropout = 0.6\n",
        "  C = 1\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.00004\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 0.5\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.006\n",
        "  use_better=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3DuwQLD_iln",
        "colab_type": "code",
        "outputId": "b6223ec5-d216-468c-fb0f-79f48628c5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters_snopes)"
      ],
      "execution_count": 551,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([39002, 50])\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss is: tensor(3.8561, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5032231404958678\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8575, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.47714285714285715\n",
            "batch count??? 14\n",
            "Running EPOCH: 2\n",
            "Average loss is: tensor(3.8559, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5036363636363637\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8573, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.47714285714285715\n",
            "batch count??? 14\n",
            "Running EPOCH: 3\n",
            "Average loss is: tensor(3.8556, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5032231404958678\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8571, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.47714285714285715\n",
            "batch count??? 14\n",
            "Running EPOCH: 4\n",
            "Average loss is: tensor(3.8555, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5032231404958678\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8566, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.47714285714285715\n",
            "batch count??? 14\n",
            "Running EPOCH: 5\n",
            "Average loss is: tensor(3.8547, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5202479338842976\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8535, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5907142857142857\n",
            "batch count??? 14\n",
            "Running EPOCH: 6\n",
            "Average loss is: tensor(3.8461, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5540495867768596\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8454, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.5628571428571428\n",
            "batch count??? 14\n",
            "Running EPOCH: 7\n",
            "Average loss is: tensor(3.8284, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.6221487603305785\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8379, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.5814285714285714\n",
            "batch count??? 14\n",
            "Running EPOCH: 8\n",
            "Average loss is: tensor(3.8199, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.6392561983471075\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8384, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6007142857142858\n",
            "batch count??? 14\n",
            "Running EPOCH: 9\n",
            "Average loss is: tensor(3.8113, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.6462809917355372\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8384, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6007142857142858\n",
            "batch count??? 14\n",
            "Running EPOCH: 10\n",
            "Average loss is: tensor(3.8110, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.6467768595041322\n",
            "batch count??? 121\n",
            "Average loss is: tensor(3.8384, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6007142857142858\n",
            "batch count??? 14\n",
            "Evaluation for: VALIDATION\n",
            "Classifier 'VALIDATION' has Acc=0.601 P=0.597 R=0.600 F1=0.596 AUC=0.597\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.518     0.593     0.553       583\n",
            "         1.0      0.676     0.606     0.639       817\n",
            "\n",
            "    accuracy                          0.601      1400\n",
            "   macro avg      0.597     0.600     0.596      1400\n",
            "weighted avg      0.610     0.601     0.603      1400\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Tight layout not applied. tight_layout cannot make axes width small enough to accommodate all axes decorations\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion matrix:\n",
            " [[346 322]\n",
            " [237 495]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApwAAAEbCAYAAAB3FlRYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nznWtH+hTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Njt1FiPx6FfO",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_challenge:\n",
        "  lstm_hidden_size = 100\n",
        "  dense_dimension = 30\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 500\n",
        "  gravity = 70\n",
        "  mlp_one = 40\n",
        "  mlp_two = 10\n",
        "  num_classes = 4\n",
        "  avg=False\n",
        "  epochs = 3\n",
        "  inner_dropout = 0\n",
        "  outer_dropout = 0\n",
        "  C = 0.5\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.01\n",
        "  grad_clip = True\n",
        "  grad_clip_amount = 0.5\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.0005\n",
        "  use_better=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYJ_FyWx36Sr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "459ee410-a2df-45c2-ccae-e86e27413d51"
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"challenge\"], Hyperparameters_challenge)"
      ],
      "execution_count": 559,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([27708, 50])\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss is: tensor(2.5995, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.72775\n",
            "batch count??? 360\n",
            "Average loss is: tensor(2.5925, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.7323076923076923\n",
            "batch count??? 39\n",
            "Running EPOCH: 2\n",
            "Average loss is: tensor(2.5950, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.7297777777777777\n",
            "batch count??? 360\n",
            "Average loss is: tensor(2.5925, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.7323076923076923\n",
            "batch count??? 39\n",
            "Running EPOCH: 3\n",
            "Average loss is: tensor(2.5951, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.72975\n",
            "batch count??? 360\n",
            "Average loss is: tensor(2.5925, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.7323076923076923\n",
            "batch count??? 39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Tight layout not applied. tight_layout cannot make axes width small enough to accommodate all axes decorations\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: VALIDATION\n",
            "Classifier 'VALIDATION' has Acc=0.732 P=0.250 R=0.183 F1=0.211 AUC=0.000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.000     0.000     0.000         0\n",
            "         1.0      0.000     0.000     0.000         0\n",
            "         2.0      1.000     0.732     0.845      3900\n",
            "         3.0      0.000     0.000     0.000         0\n",
            "\n",
            "    accuracy                          0.732      3900\n",
            "   macro avg      0.250     0.183     0.211      3900\n",
            "weighted avg      1.000     0.732     0.845      3900\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[   0    0   66    0]\n",
            " [   0    0  652    0]\n",
            " [   0    0 2856    0]\n",
            " [   0    0  326    0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp0AAAEbCAYAAACY1D9mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_politi:\n",
        "  lstm_hidden_size = 30\n",
        "  dense_dimension = 10\n",
        "  attention_hops = 5\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 10\n",
        "  mlp_one = 50\n",
        "  mlp_two = 25\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 6\n",
        "  inner_dropout = 0.2\n",
        "  outer_dropout = 0.5\n",
        "  C = 0.5\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.00008\n",
        "  grad_clip = True\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.01\n",
        "  use_better=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXeIuPBZF8Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wRSAvtABCT",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL_ENTAILMENT W/ BETTERMUSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no2oQo2CM3XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_b_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 10\n",
        "  mlp_one = 50\n",
        "  mlp_two = 25\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 11\n",
        "  inner_dropout = 0.3\n",
        "  outer_dropout = 0.6\n",
        "  C = 0.6\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.00004\n",
        "  grad_clip = True\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.01\n",
        "  use_better=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq0ID4OwKBnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters_b_snopes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uR6B4eAE7Ydh",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_b_politi:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 8\n",
        "  mlp_one = 50  \n",
        "  mlp_two = 25\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 8\n",
        "  inner_dropout = 0.2\n",
        "  outer_dropout = 0.7\n",
        "  C = 0.6\n",
        "  decay = 0\n",
        "  is_debug = False\n",
        "  lr=0.00007\n",
        "  grad_clip = True\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.009\n",
        "  use_better=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0_fGUTfKLKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters_b_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JkXr8fcoee",
        "colab_type": "text"
      },
      "source": [
        "WOAH WERE DOIN SOME VALIDATION\n",
        "PLEASE VALIDATE ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OgdGdgfwsOxa",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu() ,datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zfw_4Acupe",
        "colab_type": "text"
      },
      "source": [
        "JUST TESTIN HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "##running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ngDR0NMc26u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 9\n",
        "  inner_dropout=0.5\n",
        "  outer_dropout=0.5\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  grad_clip = True\n",
        "  lr=0.00007\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.009\n",
        "  use_better = False\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Sy7yz2yfOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters_snopes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqq85WyvO4vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_politi:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 11\n",
        "  inner_dropout=0.3\n",
        "  outer_dropout=0.6\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  grad_clip = True\n",
        "  lr=0.00005\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  use_better = False\n",
        "  early_threshold = -0.06"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters_politi)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LavaeLb_wcPS",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_b_snopes:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  mlp_one = 25  \n",
        "  mlp_two = 10\n",
        "  avg=False\n",
        "  epochs = 14\n",
        "  inner_dropout=0.3\n",
        "  outer_dropout=0.6\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  grad_clip = True\n",
        "  lr=0.00005\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.013\n",
        "  use_better = True\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1qVR4CrMGJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters_b_snopes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Czr397iowaXX",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_b_politi:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  mlp_one = 25  \n",
        "  mlp_two = 10\n",
        "  num_classes = 1\n",
        "  avg=False\n",
        "  epochs = 8\n",
        "  inner_dropout=0.3\n",
        "  outer_dropout=0.6\n",
        "  C = 0.6\n",
        "  is_debug = False\n",
        "  grad_clip = True\n",
        "  lr=0.0001\n",
        "  decay = 0\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  use_better = True\n",
        "  early_threshold = -0.013"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--XfnpT3wZ9c",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters_b_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"sheena model\",predicted_ys.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "##OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFA1PNpzA8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeclareParameters_snopes:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0.7\n",
        "  epochs = 20\n",
        "  C = 0\n",
        "  is_debug = False\n",
        "  lr=0.00008\n",
        "  decay = 0.8\n",
        "  filters = [100, 8]\n",
        "  grad_clip = True\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"real_declare\"], datasets[\"snopes\"], DeclareParameters_snopes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1c6Az3UST7bY",
        "colab": {}
      },
      "source": [
        "class DeclareParameters_politi:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0.65\n",
        "  epochs = 13\n",
        "  C = 0\n",
        "  is_debug = False\n",
        "  lr=0.00008\n",
        "  decay = 0.8\n",
        "  filters = [100, 8]\n",
        "  grad_clip = True\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True\n",
        "  early_threshold = -0.04"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_model(models[\"real_declare\"], datasets[\"politifact\"], DeclareParameters_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JetqyT7Imn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "declare_predicted_ys, _ =  run_model(models[\"real_declare\"], datasets[\"politifact\"], DeclareParameters_politi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"real declare model\", declare_predicted_ys.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZugTi6AEHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_params = {\n",
        "    \"my_model\": {\"politifact\": Hyperparameters_politi, \"snopes\":Hyperparameters_snopes},\n",
        "    #\"my_model_better\":{\"politifact\": Hyperparameters_b_politi, \"snopes\":Hyperparameters_b_snopes},\n",
        "    \"sheena_model\": {\"politifact\": SheenaParameters_politi, \"snopes\": SheenaParameters_snopes},\n",
        "    #\"sheena_model_better\": {\"politifact\": SheenaParameters_b_politi, \"snopes\" : SheenaParameters_b_snopes},\n",
        "    \"real_declare\": {\"politifact\":DeclareParameters_politi, \"snopes\": DeclareParameters_snopes}\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWBCcIDk674v",
        "colab_type": "text"
      },
      "source": [
        "##VALIDATION LAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    some_results = []\n",
        "  \n",
        "    for per_avg_i in range(per_avg_amount):\n",
        "      predicted_ys, model = run_model(models[model_name], datasets[data_name], all_params[model_name][data_name])\n",
        "      full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "    processed_results.append(process_results(list_to_dict(some_results)))\n",
        "    print(processed_results)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for result in full_results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImnSu2nFmgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}