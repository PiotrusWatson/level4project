{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4NbGwIC9-z",
        "colab_type": "text"
      },
      "source": [
        "##HAHA ITS TIME TO SPEND 5 HOURS DOWNLOADING THINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "f0c5acb1-298d-4c25-9f4a-61932f51ece9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-02 02:05:04--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  5.55MB/s    in 10s     \n",
            "\n",
            "2020-03-02 02:05:14 (8.95 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "abd62508-dce5-4648-c8ee-5689ddaaf842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-02 02:05:21--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-02 02:05:21--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-02 02:05:22--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.19MB/s    in 6m 27s  \n",
            "\n",
            "2020-03-02 02:11:49 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "31669ee2-d0ca-4498-d78a-61cf7492dc56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-02 02:12:14--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  5.45MB/s    in 0.9s    \n",
            "\n",
            "2020-03-02 02:12:15 (5.45 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "0e9ea3aa-9e1f-44ca-cd29-bc7d7a94c131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "208a97b6-1669-48fd-dbff-a7568573f098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-02 02:12:26--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  6.06MB/s    in 0.9s    \n",
            "\n",
            "2020-03-02 02:12:27 (6.06 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "outputId": "2ab75708-87c9-4f8e-9701-57cc0e6a1c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJXKPPODFqp",
        "colab_type": "text"
      },
      "source": [
        "##LOOK AT ALL THIS CODE TO IMPORT DATA GOD THERE MUST BE SOMETHING WRONG WITH ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "outputId": "c017dd5c-075a-48ac-b54b-e3bcb81ebff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "4388ad82-c592-4a4a-cfcc-74c006645e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "71905265-1c97-4a04-86de-83b1c003c011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "train_challenge, val_challenge = train_test_split(train_challenge, test_size=0.1, random_state=8)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39952</th>\n",
              "      <td>2109</td>\n",
              "      <td>A SCHOOLBOY who was almost killed when he was ...</td>\n",
              "      <td>Islamic State claims it executed American phot...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20359</th>\n",
              "      <td>1212</td>\n",
              "      <td>Tiger Woods divorced Swedish model Elin Nordeg...</td>\n",
              "      <td>Sources: Guns N' Roses Frontman Axl Rose Found...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11075</th>\n",
              "      <td>682</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>Wife and child of Islamic State leader Baghdad...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47329</th>\n",
              "      <td>2413</td>\n",
              "      <td>You may have a seen a story going around Faceb...</td>\n",
              "      <td>Armed U.S. drones spotted flying over Syria in...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15650</th>\n",
              "      <td>942</td>\n",
              "      <td>Ok, this is all still rumor, but there’s a cha...</td>\n",
              "      <td>UPDATE: BATMAN v SUPERMAN Batmobile Reportedly...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Body ID  ... Stance\n",
              "39952     2109  ...      2\n",
              "20359     1212  ...      2\n",
              "11075      682  ...      2\n",
              "47329     2413  ...      2\n",
              "15650      942  ...      1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "fake_news_challenge_mapping = {}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None, is_folding=False):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_text\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  \n",
        "  if is_folding:\n",
        "    results = []\n",
        "    folded = KFold(n_splits=10, shuffle=True)\n",
        "    splitted_object = folded.split(unique)\n",
        "    for train_result, test_result in splitted_object:\n",
        "      train_ilocs = unique.iloc[train_result][\"claim_text\"]\n",
        "      test_ilocs = unique.iloc[test_result][\"claim_text\"]\n",
        "      results.append((facts[facts[\"claim_text\"].isin(train_ilocs)], facts[facts[\"claim_text\"].isin(test_ilocs)]))\n",
        "\n",
        "    return results\n",
        "\n",
        "  train_unique, big_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "  val_unique, test_unique = train_test_split(big_unique, test_size=0.5, random_state=8)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_text\"].isin(test_unique[\"claim_text\"])]\n",
        "  val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "  train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "  return train_facts, test_facts, val_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts, val_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes, val_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "0f85e3f1-497e-4384-8c91-5121d49c163c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>vice news we dont have a timeline on the decis...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>a schedule i narcotic along with heroin and ec...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>now do you think you were maybe talking just a...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>made to the new yorker that marijuana is no mo...</td>\n",
              "      <td>time.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jun_27_donald-trump_white-house-criticism...</td>\n",
              "      <td>obamacare signed law cbo estimated 23 million ...</td>\n",
              "      <td>donald trump</td>\n",
              "      <td>about the affordable health care act in its da...</td>\n",
              "      <td>eugeneweekly.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4849</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama to ban guns from 42 million social secur...</td>\n",
              "      <td>bearingarms.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama is looking to ban social security recipi...</td>\n",
              "      <td>rightwingnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4851</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>main navigation recent posts obama to ban 42 m...</td>\n",
              "      <td>downtrend.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4852</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>get news like this in your facebook news feed ...</td>\n",
              "      <td>thegatewaypundit.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>to prove everyone wrong yet again this time it...</td>\n",
              "      <td>zerohedge.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...        article_source\n",
              "187            1  ...            reason.com\n",
              "188            1  ...            reason.com\n",
              "189            1  ...               cnn.com\n",
              "190            1  ...              time.com\n",
              "526            1  ...      eugeneweekly.com\n",
              "...          ...  ...                   ...\n",
              "4849           0  ...       bearingarms.com\n",
              "4850           0  ...     rightwingnews.com\n",
              "4851           0  ...         downtrend.com\n",
              "4852           0  ...  thegatewaypundit.com\n",
              "4853           0  ...         zerohedge.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Beq65oTgcBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, val_loader, test_data, val_data, tokeniser):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.test_data = test_data\n",
        "    self.val_data = val_data\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4lYd83j-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\"claim_text\", \"article\"]\n",
        "big_labels = [\"claim_text\", \"article\", \"article_source\"]\n",
        "challenge_labels = [\"articleBody\", \"Headline\"]\n",
        "\n",
        "def get_list(panda, labels):\n",
        "  label_to_data = {}\n",
        "  for label in labels:\n",
        "    label_to_data[label] = panda[label]\n",
        "  \n",
        "  x_list = convert_to_lists(label_to_data)\n",
        "  if labels[0] == \"claim_text\":\n",
        "    y_list = panda[\"cred_label\"].tolist()\n",
        "  else:\n",
        "    y_list = panda[\"Stance\"].tolist()\n",
        "  return x_list, y_list\n",
        "\n",
        "def get_loader(x, y, vocab_size, max_length, batch_size, name, training=True, drop_last=True):\n",
        "  stuff = []\n",
        "  for key in x:\n",
        "    stuff.append(torch.from_numpy(x[key]).type(torch.LongTensor))\n",
        "  stuff.append(torch.from_numpy(y).type(torch.DoubleTensor))\n",
        "\n",
        "  tensorset = data_utils.TensorDataset(*stuff)\n",
        "  loader = data_utils.DataLoader(tensorset, batch_size=batch_size, drop_last=drop_last, shuffle=training)\n",
        "  loader.name = name\n",
        "  return loader\n",
        "\n",
        "  \n",
        "def get_dataset(train, test, val, vocab_size, max_length, batch_size, labels, name):\n",
        "  is_challenge = labels[0] == \"articleBody\"\n",
        "  train_list_x, train_list_y = get_list(train, labels)\n",
        "  if not is_challenge:\n",
        "    test_list_x, test_list_y = get_list(test, labels)\n",
        "  val_list_x, val_list_y = get_list(val, labels)\n",
        "\n",
        "\n",
        "\n",
        "  #tokenising various stuff, setting up numpy dictionaries :)\n",
        "  tokeniser = Tokeniser(train_list_x, vocab_size, max_length)\n",
        "  x_train = tokeniser.do_everything(train_list_x)\n",
        "  x_test = tokeniser.do_everything(test_list_x)\n",
        "  x_val = tokeniser.do_everything(val_list_x)\n",
        "  y_train = np.array(train_list_y, dtype=np.float32)\n",
        "  y_test = np.array(test_list_y, dtype=np.float32)\n",
        "  y_val = np.array(val_list_y, dtype=np.float32)\n",
        "  \n",
        "  #datasets/loaders\n",
        "  train_loader = get_loader(x_train, y_train, vocab_size, max_length, batch_size, name, True)\n",
        "  test_loader = get_loader(x_test, y_test, vocab_size, max_length, batch_size, name, False, drop_last=False)\n",
        "  val_loader = get_loader(x_val, y_val, vocab_size, max_length, batch_size, name, False)\n",
        "  return Dataset(train_loader, test_loader, val_loader,  y_test, y_val, tokeniser)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MMrKlu6_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "outputId": "0c03c7f2-b450-4490-e464-3bc7fc6ef259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "\n",
        "snopes_dataset = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "fact_dataset = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "big_snopes = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "big_fact = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39093\n",
            "33766\n",
            "44623\n",
            "37174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    \n",
        "    self.embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.normaliser = torch.nn.BatchNorm1d(self.embedding_size)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x[:, 0:self.hp.max_length])\n",
        "    #embedding = self.normaliser(embedding.transpose(1,2))\n",
        "    embedding = self.dropout(embedding)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    tanh_W_H = self.dropout(tanh_W_H)\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.premise_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    premise_factor = self.premise_dropout(premise_factor)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    hypothesis_factor = self.hypothesis_dropout(hypothesis_factor)\n",
        "    if self.hp.use_better:\n",
        "      return better_mush(premise_factor,hypothesis_factor)\n",
        "    else:\n",
        "      return premise_factor * hypothesis_factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=50)\n",
        "    self.linear2 = torch.nn.Linear(50, 20)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "      x = self.dropout1(x)\n",
        "    else:\n",
        "      x = torch.relu(self.linear1(x.reshape(self.hp.batch_size, -1)))\n",
        "      x = self.dropout1(x)\n",
        "      x = torch.relu(self.linear2(x))\n",
        "      x = self.dropout2(x)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    print(word_embeddings.shape)\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout)]\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    self.dropouts[1](premise_embedding)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    self.dropouts[3](hypothesis_embedding)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    self.dropouts[4](factorised_mush)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention*premise_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWRHhOxCjFl",
        "colab_type": "text"
      },
      "source": [
        "##EVAL SUMMARY :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout)]*5\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    processed_premise = self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    premise_embedding = self.dropouts[1](premise_embedding)\n",
        "    \n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    processed_hypothesis = self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    hypothesis_embedding = self.dropouts[3](hypothesis_embedding)\n",
        "    if self.hp.use_better:\n",
        "      combined = better_mush(premise_embedding, hypothesis_embedding)\n",
        "    else:\n",
        "      combined = premise_embedding * hypothesis_embedding\n",
        "    \n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    avg = self.dropouts[4](avg)\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(10000, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 36)\n",
        "    self.linear_almost_there = torch.nn.Linear(36, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.dropout = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    lengths = self.hp.max_length - (premise == 0).sum(dim=1)\n",
        "    lengths = lengths.repeat(50, 1).transpose(0,1)\n",
        "    summed_embeddings = torch.sum(embeddings, 1)\n",
        "    mean_embeddings = torch.unsqueeze(summed_embeddings / lengths, 1) #change to accurate size of lenfgth\n",
        "    flattened_embeddings = mean_embeddings.reshape(self.hp.batch_size, -1)\n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100]).reshape(self.hp.batch_size, -1)\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((flattened_embeddings.repeat(1, 100), added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    attention_weights = softmax(torch.tanh(self.premise_linear(main_embeddings)))#TODO: turn into row vector\n",
        "\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, attention_weights.unsqueeze(2))\n",
        "    \n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    new_lengths = lengths.repeat(1, 2)\n",
        "\n",
        "    avg = torch.sum(combined, 1)/new_lengths #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    smaller = self.dropout(smaller)\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    output = torch.sigmoid(self.linear_final(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def should_stop(losses, train_losses, limit):\n",
        "  shouldnt_stop = False \n",
        "  is_learning = False\n",
        "  print(\"losses:\",losses)\n",
        "  print(\"train_losses:\", train_losses)\n",
        "  \n",
        "  if len(losses) < limit:\n",
        "    return False\n",
        "  last = losses[-1]\n",
        "  last_train = train_losses[-1]\n",
        "\n",
        "  if limit == 2:\n",
        "    return last >= losses[-2]\n",
        "  \n",
        "  for i in range(-2,- limit-1,-1):\n",
        "    shouldnt_stop = (shouldnt_stop or last < losses[i])\n",
        "    last = losses[i]\n",
        "  return not shouldnt_stop\n",
        "\n",
        "\n",
        "def train(model=None, \n",
        "          dataset = None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "\n",
        "  is_binary = hp.num_classes == 1\n",
        "  is_validating = False\n",
        "  has_stopped = False\n",
        "  \n",
        "  if dataset.train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif dataset.train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "\n",
        "    for i in range(2):\n",
        "      \n",
        "      total_loss = 0\n",
        "      batch_count = 0\n",
        "      correct = 0\n",
        "      penal = 0\n",
        "\n",
        "      if is_validating:\n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.val_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 1\n",
        "      elif has_stopped:\n",
        "        model.train(False)\n",
        "        loader = dataset.train_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 10\n",
        "      else:\n",
        "        model.train(True)\n",
        "        loader = dataset.train_loader\n",
        "        torch.enable_grad()\n",
        "        debug_amount = 10\n",
        "      \n",
        "      for batch_index, train_data in enumerate(loader):\n",
        "      #setting everything up\n",
        "        model.hidden_state = model.reset_for_testing(train_data[0].shape[0])\n",
        "        train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "        predicted_y, attention = model(*train_data[:-1])\n",
        "        actual_y = train_data[-1]\n",
        "        squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "        if hp.C > 0:\n",
        "          attentionT = attention.transpose(1,2)\n",
        "          identity = torch.eye(attention.size(1))\n",
        "          identity = Variable(identity.unsqueeze(0).expand(loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "          penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "        if is_binary:\n",
        "          loss = loss_function(squeezed_y, actual_y.double())\n",
        "          loss += hp.C * penal/loader.batch_size\n",
        "          correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "        else:\n",
        "          loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/loader.batch_size)\n",
        "          correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if using_gradient_clipping:\n",
        "          torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      \n",
        "      #cleaning up regularisation\n",
        "        if hp.C > 0:\n",
        "          del(penal)\n",
        "          del(identity)\n",
        "          del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "        optimiser.zero_grad()\n",
        "        if not is_validating and not has_stopped:\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "        if hp.is_debug and batch_index % debug_amount == 0:\n",
        "          print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, ISvAL: {}, HasStopped: {}\".format(\n",
        "              epoch, batch_index * len(train_data[0]), len(loader.dataset),\n",
        "              100. * batch_index / len(loader), loss.item(), is_validating, has_stopped\n",
        "          ))\n",
        "\n",
        "      \n",
        "        batch_count += 1\n",
        "      \n",
        "        free_data(train_data)\n",
        "\n",
        "      print(\"Average loss is:\",total_loss/batch_count, \"while validation_status:\", is_validating, \"and stopping_status\", has_stopped)\n",
        "      correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "      accuracy = correct_but_numpy / float(batch_count * loader.batch_size)\n",
        "      print(\"Accuracy of the model\", accuracy)\n",
        "      if (not is_validating):\n",
        "        losses.append(total_loss/batch_count)\n",
        "        accuracies.append(accuracy)\n",
        "      else:\n",
        "        val_losses.append(total_loss/batch_count)\n",
        "        val_accuracies.append(accuracy)\n",
        "      if hp.use_early_stopping:\n",
        "        has_stopped = should_stop(val_losses,losses, hp.early_stopping)\n",
        "\n",
        "      is_validating = not is_validating\n",
        "  return losses, val_losses, accuracies, val_accuracies "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, val_losses, accuracies, val_accuracies, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  \n",
        "  plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Train Accuracy\")\n",
        "  plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_accuracies, scalex=True, scaley=True, label=\"Val Accuracy\")\n",
        "  plt.annotate(str(val_accuracies[-1]), xy=(epochs,val_accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Train Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_losses,scalex=True, scaley=True, label=\"Val Loss\")\n",
        "  plt.annotate(str(val_losses[-1]), xy=(epochs,val_losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYfImtudskLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp, is_plot=True):\n",
        "  runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "  bce_loss = torch.nn.BCELoss()\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(runnable_model.parameters(), lr=hp.lr, weight_decay=hp.decay)\n",
        "  losses, val_losses, accuracies, val_accuracies = train(model=runnable_model,\n",
        "                       dataset = dataset,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=hp.grad_clip)\n",
        "  if is_plot:\n",
        "    plot_stuff(hp.epochs, losses,val_losses, accuracies, val_accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  check_loader = dataset.test_loader\n",
        "  predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         check_loader,\n",
        "         hp)\n",
        "  return predicted_ys, runnable_model\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc_full}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2DLo4OoUO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc_full))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oMDNooNrMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "##RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": fact_dataset,\n",
        "    \"snopes\": snopes_dataset\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    \"my_model_better\": TextualEntailmentModel\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    \"sheena_model_better\": BaselineSentenceEntailment,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 10\n",
        "  inner_dropout = 0.5\n",
        "  outer_dropout = 0.5\n",
        "  C = 0.3\n",
        "  decay = 0\n",
        "  is_debug = True\n",
        "  lr=0.00007\n",
        "  grad_clip = True\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True\n",
        "  use_better=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no2oQo2CM3XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters_b:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 10\n",
        "  inner_dropout = 0.5\n",
        "  outer_dropout = 0.5\n",
        "  C = 0.3\n",
        "  decay = 0\n",
        "  is_debug = True\n",
        "  lr=0.00007\n",
        "  grad_clip = True\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True\n",
        "  use_better=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wRSAvtABCT",
        "colab_type": "text"
      },
      "source": [
        "##CROSS VALIDATION ZONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "2d0c500b-c568-4f08-9c38-95c191fd914e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "NUM_OF_AVGS = 5\n",
        "avgs = []\n",
        "\"\"\"\n",
        "for i in range(NUM_OF_AVGS):\n",
        "  \n",
        "  \n",
        "  \n",
        "  avgs.append(results)\n",
        "\n",
        "print(process_results(list_to_dict(avgs)))\n",
        "\"\"\"\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters)\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([39093, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.642203, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 1.642191, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 1.642999, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 1.640573, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 1.642723, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-cfcc529f0804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snopes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"politifact\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-1a3c21a92b33>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, dataset, hp, is_plot)\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        using_gradient_clipping=hp.grad_clip)\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_plot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplot_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-98fae21d5d5e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;31m#get y values - do forward pass and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpredicted_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mactual_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0msqueezed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-aeb8314fa739>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprocessed_premise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpremise_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremise_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-42611970826d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden_layer)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     return self.cool_lstm(embedding,\n\u001b[0;32m---> 26\u001b[0;31m                           hidden_layer)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JkXr8fcoee",
        "colab_type": "text"
      },
      "source": [
        "WOAH WERE DOIN SOME VALIDATION\n",
        "PLEASE VALIDATE ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "341e0b01-63b3-480e-f546-4ffbe89f4870"
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu() ,datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment model\n",
            "Classifier 'textual entailment model' has Acc=0.624 P=0.599 R=0.649 F1=0.576 AUC=0.599\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.317     0.691     0.434       596\n",
            "         1.0      0.882     0.606     0.719      2259\n",
            "\n",
            "    accuracy                          0.624      2855\n",
            "   macro avg      0.599     0.649     0.576      2855\n",
            "weighted avg      0.764     0.624     0.659      2855\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 412  889]\n",
            " [ 184 1370]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5991376794605081, 0.6488691022635781, 0.624168126094571, 0.5764821703516575)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zfw_4Acupe",
        "colab_type": "text"
      },
      "source": [
        "JUST TESTIN HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsngYK8wcvc8",
        "colab_type": "code",
        "outputId": "e7836aeb-0a10-4022-a950-702db8fb9e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "test_results= batch_wise_evaluate(text_model, datasets[\"politifact\"].test_loader, Hyperparameters)\n",
        "evaluation_summary(\"textual entailment test model\", test_results.cpu(), datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment test model\n",
            "Classifier 'textual entailment test model' has Acc=0.678 P=0.671 R=0.677 F1=0.672 AUC=0.671\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.589     0.667     0.625      1149\n",
            "         1.0      0.754     0.686     0.718      1706\n",
            "\n",
            "    accuracy                          0.678      2855\n",
            "   macro avg      0.671     0.677     0.672      2855\n",
            "weighted avg      0.687     0.678     0.681      2855\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 766  535]\n",
            " [ 383 1171]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6711585583607105,\n",
              " 0.6765338022665104,\n",
              " 0.6784588441330999,\n",
              " 0.6718555152122199)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "##running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ngDR0NMc26u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 10\n",
        "  inner_dropout=0.5\n",
        "  outer_dropout=0.5\n",
        "  C = 0.3\n",
        "  is_debug = True\n",
        "  grad_clip = True\n",
        "  lr=0.00006\n",
        "  decay = 0\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True\n",
        "  use_better = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqq85WyvO4vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters_b:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 10\n",
        "  inner_dropout=0.5\n",
        "  outer_dropout=0.5\n",
        "  C = 0.3\n",
        "  is_debug = True\n",
        "  grad_clip = True\n",
        "  lr=0.00006\n",
        "  decay = 0\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True\n",
        "  use_better = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "outputId": "894bc7a8-ccab-46f0-b8ae-564ecd25f40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters)\n",
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.634702, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/23817 (4%)]\tLoss: 1.633031, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/23817 (8%)]\tLoss: 1.633667, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/23817 (13%)]\tLoss: 1.634599, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/23817 (17%)]\tLoss: 1.636575, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/23817 (21%)]\tLoss: 1.637617, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/23817 (25%)]\tLoss: 1.636949, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/23817 (29%)]\tLoss: 1.638034, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/23817 (34%)]\tLoss: 1.633954, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/23817 (38%)]\tLoss: 1.627815, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/23817 (42%)]\tLoss: 1.637841, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/23817 (46%)]\tLoss: 1.631204, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/23817 (50%)]\tLoss: 1.636618, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [13000/23817 (55%)]\tLoss: 1.634646, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [14000/23817 (59%)]\tLoss: 1.630677, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [15000/23817 (63%)]\tLoss: 1.634808, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [16000/23817 (67%)]\tLoss: 1.634557, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [17000/23817 (71%)]\tLoss: 1.633636, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [18000/23817 (76%)]\tLoss: 1.636113, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [19000/23817 (80%)]\tLoss: 1.637699, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [20000/23817 (84%)]\tLoss: 1.636455, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [21000/23817 (88%)]\tLoss: 1.637405, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [22000/23817 (92%)]\tLoss: 1.637939, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [23000/23817 (97%)]\tLoss: 1.635961, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6358, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.4976470588235294\n",
            "losses: []\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.628233, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/2884 (4%)]\tLoss: 1.639041, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/2884 (7%)]\tLoss: 1.632465, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/2884 (11%)]\tLoss: 1.645054, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/2884 (14%)]\tLoss: 1.638172, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/2884 (18%)]\tLoss: 1.632361, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/2884 (21%)]\tLoss: 1.636560, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/2884 (25%)]\tLoss: 1.633832, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/2884 (29%)]\tLoss: 1.644111, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/2884 (32%)]\tLoss: 1.635306, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/2884 (36%)]\tLoss: 1.638211, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/2884 (39%)]\tLoss: 1.635772, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/2884 (43%)]\tLoss: 1.632771, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/2884 (46%)]\tLoss: 1.621293, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/2884 (50%)]\tLoss: 1.637686, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1500/2884 (54%)]\tLoss: 1.644404, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1600/2884 (57%)]\tLoss: 1.645316, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1700/2884 (61%)]\tLoss: 1.637330, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1800/2884 (64%)]\tLoss: 1.630373, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1900/2884 (68%)]\tLoss: 1.628238, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2000/2884 (71%)]\tLoss: 1.644101, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2100/2884 (75%)]\tLoss: 1.643663, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2200/2884 (79%)]\tLoss: 1.632323, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2300/2884 (82%)]\tLoss: 1.630689, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2400/2884 (86%)]\tLoss: 1.633425, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2500/2884 (89%)]\tLoss: 1.633979, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2600/2884 (93%)]\tLoss: 1.635051, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2700/2884 (96%)]\tLoss: 1.641482, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6361, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.4810714285714286\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.634283, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/23817 (4%)]\tLoss: 1.635674, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/23817 (8%)]\tLoss: 1.631583, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/23817 (13%)]\tLoss: 1.634203, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/23817 (17%)]\tLoss: 1.635315, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/23817 (21%)]\tLoss: 1.633025, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/23817 (25%)]\tLoss: 1.633175, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/23817 (29%)]\tLoss: 1.634327, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/23817 (34%)]\tLoss: 1.636302, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/23817 (38%)]\tLoss: 1.634126, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/23817 (42%)]\tLoss: 1.635035, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/23817 (46%)]\tLoss: 1.634127, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/23817 (50%)]\tLoss: 1.634266, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [13000/23817 (55%)]\tLoss: 1.632756, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [14000/23817 (59%)]\tLoss: 1.632848, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [15000/23817 (63%)]\tLoss: 1.632755, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [16000/23817 (67%)]\tLoss: 1.632886, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [17000/23817 (71%)]\tLoss: 1.632271, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [18000/23817 (76%)]\tLoss: 1.632524, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [19000/23817 (80%)]\tLoss: 1.631120, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [20000/23817 (84%)]\tLoss: 1.631808, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [21000/23817 (88%)]\tLoss: 1.631688, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [22000/23817 (92%)]\tLoss: 1.630094, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [23000/23817 (97%)]\tLoss: 1.631002, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6334, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5162605042016807\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.634189, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/2884 (4%)]\tLoss: 1.630902, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/2884 (7%)]\tLoss: 1.635249, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/2884 (11%)]\tLoss: 1.624763, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/2884 (14%)]\tLoss: 1.629742, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/2884 (18%)]\tLoss: 1.634728, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/2884 (21%)]\tLoss: 1.630636, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/2884 (25%)]\tLoss: 1.632593, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/2884 (29%)]\tLoss: 1.629151, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/2884 (32%)]\tLoss: 1.630779, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/2884 (36%)]\tLoss: 1.630789, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/2884 (39%)]\tLoss: 1.633498, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/2884 (43%)]\tLoss: 1.637758, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/2884 (46%)]\tLoss: 1.640347, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/2884 (50%)]\tLoss: 1.629520, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1500/2884 (54%)]\tLoss: 1.626828, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1600/2884 (57%)]\tLoss: 1.624635, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1700/2884 (61%)]\tLoss: 1.629252, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1800/2884 (64%)]\tLoss: 1.635512, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1900/2884 (68%)]\tLoss: 1.637652, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2000/2884 (71%)]\tLoss: 1.628359, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2100/2884 (75%)]\tLoss: 1.626699, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2200/2884 (79%)]\tLoss: 1.633184, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2300/2884 (82%)]\tLoss: 1.634366, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2400/2884 (86%)]\tLoss: 1.636136, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2500/2884 (89%)]\tLoss: 1.631295, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2600/2884 (93%)]\tLoss: 1.633629, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2700/2884 (96%)]\tLoss: 1.630421, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6319, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.535\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.632300, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/23817 (4%)]\tLoss: 1.630247, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/23817 (8%)]\tLoss: 1.629643, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/23817 (13%)]\tLoss: 1.629039, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/23817 (17%)]\tLoss: 1.628984, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/23817 (21%)]\tLoss: 1.626949, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/23817 (25%)]\tLoss: 1.631639, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/23817 (29%)]\tLoss: 1.630779, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/23817 (34%)]\tLoss: 1.629985, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/23817 (38%)]\tLoss: 1.624113, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/23817 (42%)]\tLoss: 1.628506, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/23817 (46%)]\tLoss: 1.624930, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/23817 (50%)]\tLoss: 1.625188, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [13000/23817 (55%)]\tLoss: 1.629247, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [14000/23817 (59%)]\tLoss: 1.629134, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [15000/23817 (63%)]\tLoss: 1.620767, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [16000/23817 (67%)]\tLoss: 1.620698, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [17000/23817 (71%)]\tLoss: 1.621796, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [18000/23817 (76%)]\tLoss: 1.623674, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [19000/23817 (80%)]\tLoss: 1.621931, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [20000/23817 (84%)]\tLoss: 1.617196, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [21000/23817 (88%)]\tLoss: 1.616648, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [22000/23817 (92%)]\tLoss: 1.623416, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [23000/23817 (97%)]\tLoss: 1.618599, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6251, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5245378151260505\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.635141, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/2884 (4%)]\tLoss: 1.611886, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/2884 (7%)]\tLoss: 1.631043, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/2884 (11%)]\tLoss: 1.586965, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/2884 (14%)]\tLoss: 1.617646, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/2884 (18%)]\tLoss: 1.627511, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/2884 (21%)]\tLoss: 1.608028, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/2884 (25%)]\tLoss: 1.618416, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/2884 (29%)]\tLoss: 1.612332, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/2884 (32%)]\tLoss: 1.621233, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/2884 (36%)]\tLoss: 1.609392, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/2884 (39%)]\tLoss: 1.628483, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/2884 (43%)]\tLoss: 1.643674, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/2884 (46%)]\tLoss: 1.643943, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/2884 (50%)]\tLoss: 1.606373, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1500/2884 (54%)]\tLoss: 1.594909, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1600/2884 (57%)]\tLoss: 1.595410, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1700/2884 (61%)]\tLoss: 1.602406, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1800/2884 (64%)]\tLoss: 1.633146, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1900/2884 (68%)]\tLoss: 1.648261, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2000/2884 (71%)]\tLoss: 1.608164, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2100/2884 (75%)]\tLoss: 1.606936, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2200/2884 (79%)]\tLoss: 1.628420, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2300/2884 (82%)]\tLoss: 1.627514, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2400/2884 (86%)]\tLoss: 1.639884, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2500/2884 (89%)]\tLoss: 1.621481, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2600/2884 (93%)]\tLoss: 1.626001, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2700/2884 (96%)]\tLoss: 1.617212, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6197, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.535\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23817 (0%)]\tLoss: 1.628999, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/23817 (4%)]\tLoss: 1.612047, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/23817 (8%)]\tLoss: 1.615131, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/23817 (13%)]\tLoss: 1.610976, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/23817 (17%)]\tLoss: 1.606007, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/23817 (21%)]\tLoss: 1.606071, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/23817 (25%)]\tLoss: 1.603101, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/23817 (29%)]\tLoss: 1.592511, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/23817 (34%)]\tLoss: 1.581157, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/23817 (38%)]\tLoss: 1.582094, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/23817 (42%)]\tLoss: 1.598867, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/23817 (46%)]\tLoss: 1.595104, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/23817 (50%)]\tLoss: 1.582782, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [13000/23817 (55%)]\tLoss: 1.589239, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [14000/23817 (59%)]\tLoss: 1.588367, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [15000/23817 (63%)]\tLoss: 1.553972, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [16000/23817 (67%)]\tLoss: 1.565072, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [17000/23817 (71%)]\tLoss: 1.573609, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [18000/23817 (76%)]\tLoss: 1.562637, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [19000/23817 (80%)]\tLoss: 1.576522, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [20000/23817 (84%)]\tLoss: 1.581572, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [21000/23817 (88%)]\tLoss: 1.562115, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [22000/23817 (92%)]\tLoss: 1.557286, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [23000/23817 (97%)]\tLoss: 1.565720, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5870, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5906722689075631\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/2884 (0%)]\tLoss: 1.575817, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/2884 (4%)]\tLoss: 1.558319, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/2884 (7%)]\tLoss: 1.589683, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/2884 (11%)]\tLoss: 1.513034, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/2884 (14%)]\tLoss: 1.583946, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/2884 (18%)]\tLoss: 1.565900, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/2884 (21%)]\tLoss: 1.550303, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/2884 (25%)]\tLoss: 1.564702, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/2884 (29%)]\tLoss: 1.620396, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/2884 (32%)]\tLoss: 1.587264, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/2884 (36%)]\tLoss: 1.565343, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/2884 (39%)]\tLoss: 1.612495, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/2884 (43%)]\tLoss: 1.635968, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/2884 (46%)]\tLoss: 1.579675, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/2884 (50%)]\tLoss: 1.565038, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1500/2884 (54%)]\tLoss: 1.556015, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1600/2884 (57%)]\tLoss: 1.556587, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1700/2884 (61%)]\tLoss: 1.533415, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1800/2884 (64%)]\tLoss: 1.589146, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1900/2884 (68%)]\tLoss: 1.609054, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2000/2884 (71%)]\tLoss: 1.590386, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2100/2884 (75%)]\tLoss: 1.593695, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2200/2884 (79%)]\tLoss: 1.581912, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2300/2884 (82%)]\tLoss: 1.600166, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2400/2884 (86%)]\tLoss: 1.644431, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2500/2884 (89%)]\tLoss: 1.576954, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2600/2884 (93%)]\tLoss: 1.602814, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2700/2884 (96%)]\tLoss: 1.634009, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5834, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6007142857142858\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/23817 (0%)]\tLoss: 1.545459, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/23817 (4%)]\tLoss: 1.578174, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/23817 (8%)]\tLoss: 1.556410, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/23817 (13%)]\tLoss: 1.553147, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/23817 (17%)]\tLoss: 1.560292, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/23817 (21%)]\tLoss: 1.558571, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/23817 (25%)]\tLoss: 1.555852, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/23817 (29%)]\tLoss: 1.578644, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/23817 (34%)]\tLoss: 1.554427, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/23817 (38%)]\tLoss: 1.543282, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/23817 (42%)]\tLoss: 1.540575, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/23817 (46%)]\tLoss: 1.569786, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/23817 (50%)]\tLoss: 1.539906, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [13000/23817 (55%)]\tLoss: 1.566661, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [14000/23817 (59%)]\tLoss: 1.550800, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [15000/23817 (63%)]\tLoss: 1.519828, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [16000/23817 (67%)]\tLoss: 1.541602, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [17000/23817 (71%)]\tLoss: 1.564559, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [18000/23817 (76%)]\tLoss: 1.538318, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [19000/23817 (80%)]\tLoss: 1.533361, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [20000/23817 (84%)]\tLoss: 1.559824, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [21000/23817 (88%)]\tLoss: 1.534134, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [22000/23817 (92%)]\tLoss: 1.543074, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [23000/23817 (97%)]\tLoss: 1.557443, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5566, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6205042016806722\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/2884 (0%)]\tLoss: 1.553239, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/2884 (4%)]\tLoss: 1.540896, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/2884 (7%)]\tLoss: 1.605373, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/2884 (11%)]\tLoss: 1.476676, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/2884 (14%)]\tLoss: 1.577449, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/2884 (18%)]\tLoss: 1.548462, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/2884 (21%)]\tLoss: 1.517659, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/2884 (25%)]\tLoss: 1.524496, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/2884 (29%)]\tLoss: 1.621696, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/2884 (32%)]\tLoss: 1.575599, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/2884 (36%)]\tLoss: 1.555649, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/2884 (39%)]\tLoss: 1.611005, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/2884 (43%)]\tLoss: 1.671737, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/2884 (46%)]\tLoss: 1.557211, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/2884 (50%)]\tLoss: 1.529445, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1500/2884 (54%)]\tLoss: 1.520545, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1600/2884 (57%)]\tLoss: 1.539763, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1700/2884 (61%)]\tLoss: 1.495302, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1800/2884 (64%)]\tLoss: 1.581254, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1900/2884 (68%)]\tLoss: 1.606775, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2000/2884 (71%)]\tLoss: 1.600356, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2100/2884 (75%)]\tLoss: 1.599409, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2200/2884 (79%)]\tLoss: 1.569026, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2300/2884 (82%)]\tLoss: 1.591365, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2400/2884 (86%)]\tLoss: 1.686942, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2500/2884 (89%)]\tLoss: 1.571464, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2600/2884 (93%)]\tLoss: 1.599264, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2700/2884 (96%)]\tLoss: 1.635689, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5737, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6071428571428571\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/23817 (0%)]\tLoss: 1.523774, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/23817 (4%)]\tLoss: 1.557653, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/23817 (8%)]\tLoss: 1.561613, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/23817 (13%)]\tLoss: 1.509978, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/23817 (17%)]\tLoss: 1.553287, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/23817 (21%)]\tLoss: 1.525216, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/23817 (25%)]\tLoss: 1.564563, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/23817 (29%)]\tLoss: 1.554400, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/23817 (34%)]\tLoss: 1.556834, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/23817 (38%)]\tLoss: 1.547555, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/23817 (42%)]\tLoss: 1.521047, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/23817 (46%)]\tLoss: 1.529419, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/23817 (50%)]\tLoss: 1.561475, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [13000/23817 (55%)]\tLoss: 1.514905, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [14000/23817 (59%)]\tLoss: 1.570147, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [15000/23817 (63%)]\tLoss: 1.517870, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [16000/23817 (67%)]\tLoss: 1.526778, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [17000/23817 (71%)]\tLoss: 1.513250, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [18000/23817 (76%)]\tLoss: 1.508608, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [19000/23817 (80%)]\tLoss: 1.513661, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [20000/23817 (84%)]\tLoss: 1.529606, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [21000/23817 (88%)]\tLoss: 1.529353, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [22000/23817 (92%)]\tLoss: 1.525953, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [23000/23817 (97%)]\tLoss: 1.508472, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5402, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.635\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/2884 (0%)]\tLoss: 1.576248, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/2884 (4%)]\tLoss: 1.525888, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/2884 (7%)]\tLoss: 1.596398, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/2884 (11%)]\tLoss: 1.453508, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/2884 (14%)]\tLoss: 1.596291, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/2884 (18%)]\tLoss: 1.523723, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/2884 (21%)]\tLoss: 1.480469, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/2884 (25%)]\tLoss: 1.531618, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/2884 (29%)]\tLoss: 1.642254, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/2884 (32%)]\tLoss: 1.585716, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/2884 (36%)]\tLoss: 1.549403, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/2884 (39%)]\tLoss: 1.626077, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/2884 (43%)]\tLoss: 1.697319, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/2884 (46%)]\tLoss: 1.544931, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/2884 (50%)]\tLoss: 1.512575, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1500/2884 (54%)]\tLoss: 1.489984, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1600/2884 (57%)]\tLoss: 1.550194, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1700/2884 (61%)]\tLoss: 1.482558, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1800/2884 (64%)]\tLoss: 1.584209, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1900/2884 (68%)]\tLoss: 1.619396, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2000/2884 (71%)]\tLoss: 1.572267, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2100/2884 (75%)]\tLoss: 1.587755, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2200/2884 (79%)]\tLoss: 1.549055, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2300/2884 (82%)]\tLoss: 1.596792, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2400/2884 (86%)]\tLoss: 1.707105, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2500/2884 (89%)]\tLoss: 1.575774, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2600/2884 (93%)]\tLoss: 1.601979, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2700/2884 (96%)]\tLoss: 1.637674, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5713, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6160714285714286\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/23817 (0%)]\tLoss: 1.540138, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/23817 (4%)]\tLoss: 1.541115, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/23817 (8%)]\tLoss: 1.501402, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/23817 (13%)]\tLoss: 1.523110, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/23817 (17%)]\tLoss: 1.537717, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/23817 (21%)]\tLoss: 1.537692, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/23817 (25%)]\tLoss: 1.511198, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/23817 (29%)]\tLoss: 1.539415, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/23817 (34%)]\tLoss: 1.570647, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/23817 (38%)]\tLoss: 1.486980, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/23817 (42%)]\tLoss: 1.519039, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/23817 (46%)]\tLoss: 1.549334, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/23817 (50%)]\tLoss: 1.513492, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [13000/23817 (55%)]\tLoss: 1.522242, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [14000/23817 (59%)]\tLoss: 1.511388, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [15000/23817 (63%)]\tLoss: 1.528823, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [16000/23817 (67%)]\tLoss: 1.528719, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [17000/23817 (71%)]\tLoss: 1.518951, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [18000/23817 (76%)]\tLoss: 1.575452, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [19000/23817 (80%)]\tLoss: 1.522746, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [20000/23817 (84%)]\tLoss: 1.529326, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [21000/23817 (88%)]\tLoss: 1.453129, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [22000/23817 (92%)]\tLoss: 1.488161, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [23000/23817 (97%)]\tLoss: 1.493380, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5246, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6466806722689076\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/2884 (0%)]\tLoss: 1.528174, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/2884 (4%)]\tLoss: 1.494091, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/2884 (7%)]\tLoss: 1.588165, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/2884 (11%)]\tLoss: 1.462419, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/2884 (14%)]\tLoss: 1.605575, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/2884 (18%)]\tLoss: 1.552408, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/2884 (21%)]\tLoss: 1.462965, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/2884 (25%)]\tLoss: 1.507031, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/2884 (29%)]\tLoss: 1.669936, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/2884 (32%)]\tLoss: 1.575477, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/2884 (36%)]\tLoss: 1.511719, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/2884 (39%)]\tLoss: 1.642065, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/2884 (43%)]\tLoss: 1.715464, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/2884 (46%)]\tLoss: 1.473240, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/2884 (50%)]\tLoss: 1.499565, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1500/2884 (54%)]\tLoss: 1.499198, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1600/2884 (57%)]\tLoss: 1.550396, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1700/2884 (61%)]\tLoss: 1.477363, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1800/2884 (64%)]\tLoss: 1.563308, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1900/2884 (68%)]\tLoss: 1.606918, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2000/2884 (71%)]\tLoss: 1.602765, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2100/2884 (75%)]\tLoss: 1.628922, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2200/2884 (79%)]\tLoss: 1.540813, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2300/2884 (82%)]\tLoss: 1.585287, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2400/2884 (86%)]\tLoss: 1.734006, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2500/2884 (89%)]\tLoss: 1.534408, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2600/2884 (93%)]\tLoss: 1.601244, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2700/2884 (96%)]\tLoss: 1.637681, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5661, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6153571428571428\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5661, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/23817 (0%)]\tLoss: 1.529093, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [1000/23817 (4%)]\tLoss: 1.502680, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [2000/23817 (8%)]\tLoss: 1.487217, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [3000/23817 (13%)]\tLoss: 1.503651, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [4000/23817 (17%)]\tLoss: 1.516575, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [5000/23817 (21%)]\tLoss: 1.506008, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [6000/23817 (25%)]\tLoss: 1.542666, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [7000/23817 (29%)]\tLoss: 1.544866, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [8000/23817 (34%)]\tLoss: 1.482445, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [9000/23817 (38%)]\tLoss: 1.457506, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [10000/23817 (42%)]\tLoss: 1.519929, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [11000/23817 (46%)]\tLoss: 1.510864, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [12000/23817 (50%)]\tLoss: 1.477706, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [13000/23817 (55%)]\tLoss: 1.497313, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [14000/23817 (59%)]\tLoss: 1.456633, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [15000/23817 (63%)]\tLoss: 1.492850, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [16000/23817 (67%)]\tLoss: 1.494498, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [17000/23817 (71%)]\tLoss: 1.458314, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [18000/23817 (76%)]\tLoss: 1.474075, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [19000/23817 (80%)]\tLoss: 1.486841, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [20000/23817 (84%)]\tLoss: 1.531465, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [21000/23817 (88%)]\tLoss: 1.537249, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [22000/23817 (92%)]\tLoss: 1.491217, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [23000/23817 (97%)]\tLoss: 1.467908, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5073, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6627310924369748\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5661, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64), tensor(1.5073, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/2884 (0%)]\tLoss: 1.518434, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [100/2884 (4%)]\tLoss: 1.498255, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [200/2884 (7%)]\tLoss: 1.578242, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [300/2884 (11%)]\tLoss: 1.452879, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [400/2884 (14%)]\tLoss: 1.641015, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [500/2884 (18%)]\tLoss: 1.562107, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [600/2884 (21%)]\tLoss: 1.437253, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [700/2884 (25%)]\tLoss: 1.493015, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [800/2884 (29%)]\tLoss: 1.725415, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [900/2884 (32%)]\tLoss: 1.588185, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1000/2884 (36%)]\tLoss: 1.504674, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1100/2884 (39%)]\tLoss: 1.659909, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1200/2884 (43%)]\tLoss: 1.711718, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1300/2884 (46%)]\tLoss: 1.464356, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1400/2884 (50%)]\tLoss: 1.481962, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1500/2884 (54%)]\tLoss: 1.509233, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1600/2884 (57%)]\tLoss: 1.554850, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1700/2884 (61%)]\tLoss: 1.462868, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1800/2884 (64%)]\tLoss: 1.590659, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1900/2884 (68%)]\tLoss: 1.581700, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2000/2884 (71%)]\tLoss: 1.613239, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2100/2884 (75%)]\tLoss: 1.659408, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2200/2884 (79%)]\tLoss: 1.549140, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2300/2884 (82%)]\tLoss: 1.584941, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2400/2884 (86%)]\tLoss: 1.762939, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2500/2884 (89%)]\tLoss: 1.515985, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2600/2884 (93%)]\tLoss: 1.620239, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2700/2884 (96%)]\tLoss: 1.672964, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5713, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6167857142857143\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5661, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64), tensor(1.5073, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/23817 (0%)]\tLoss: 1.525782, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [1000/23817 (4%)]\tLoss: 1.464912, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [2000/23817 (8%)]\tLoss: 1.499620, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [3000/23817 (13%)]\tLoss: 1.535659, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [4000/23817 (17%)]\tLoss: 1.486394, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [5000/23817 (21%)]\tLoss: 1.475865, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [6000/23817 (25%)]\tLoss: 1.458259, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [7000/23817 (29%)]\tLoss: 1.498321, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [8000/23817 (34%)]\tLoss: 1.467291, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [9000/23817 (38%)]\tLoss: 1.512913, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [10000/23817 (42%)]\tLoss: 1.493608, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [11000/23817 (46%)]\tLoss: 1.450239, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [12000/23817 (50%)]\tLoss: 1.508099, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [13000/23817 (55%)]\tLoss: 1.488990, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [14000/23817 (59%)]\tLoss: 1.478370, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [15000/23817 (63%)]\tLoss: 1.535774, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [16000/23817 (67%)]\tLoss: 1.516693, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [17000/23817 (71%)]\tLoss: 1.444244, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [18000/23817 (76%)]\tLoss: 1.469016, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [19000/23817 (80%)]\tLoss: 1.490999, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [20000/23817 (84%)]\tLoss: 1.556766, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [21000/23817 (88%)]\tLoss: 1.482656, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [22000/23817 (92%)]\tLoss: 1.398165, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [23000/23817 (97%)]\tLoss: 1.494452, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.4893, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6750420168067227\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5661, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64), tensor(1.5073, device='cuda:0', dtype=torch.float64), tensor(1.4893, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 8 [0/2884 (0%)]\tLoss: 1.519615, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [100/2884 (4%)]\tLoss: 1.475363, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [200/2884 (7%)]\tLoss: 1.564896, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [300/2884 (11%)]\tLoss: 1.447034, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [400/2884 (14%)]\tLoss: 1.628354, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [500/2884 (18%)]\tLoss: 1.584852, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [600/2884 (21%)]\tLoss: 1.403630, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [700/2884 (25%)]\tLoss: 1.470230, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [800/2884 (29%)]\tLoss: 1.693533, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [900/2884 (32%)]\tLoss: 1.596582, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1000/2884 (36%)]\tLoss: 1.517394, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1100/2884 (39%)]\tLoss: 1.682691, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1200/2884 (43%)]\tLoss: 1.742930, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1300/2884 (46%)]\tLoss: 1.473801, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1400/2884 (50%)]\tLoss: 1.474532, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1500/2884 (54%)]\tLoss: 1.514765, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1600/2884 (57%)]\tLoss: 1.522253, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1700/2884 (61%)]\tLoss: 1.452171, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1800/2884 (64%)]\tLoss: 1.573212, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1900/2884 (68%)]\tLoss: 1.559324, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2000/2884 (71%)]\tLoss: 1.597829, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2100/2884 (75%)]\tLoss: 1.649709, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2200/2884 (79%)]\tLoss: 1.548711, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2300/2884 (82%)]\tLoss: 1.586597, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2400/2884 (86%)]\tLoss: 1.777978, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2500/2884 (89%)]\tLoss: 1.511351, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2600/2884 (93%)]\tLoss: 1.606279, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [2700/2884 (96%)]\tLoss: 1.640847, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5649, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6235714285714286\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5661, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5649, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64), tensor(1.5073, device='cuda:0', dtype=torch.float64), tensor(1.4893, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/23817 (0%)]\tLoss: 1.462866, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [1000/23817 (4%)]\tLoss: 1.466082, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [2000/23817 (8%)]\tLoss: 1.481956, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [3000/23817 (13%)]\tLoss: 1.412384, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [4000/23817 (17%)]\tLoss: 1.504960, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [5000/23817 (21%)]\tLoss: 1.473521, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [6000/23817 (25%)]\tLoss: 1.480615, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [7000/23817 (29%)]\tLoss: 1.474638, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [8000/23817 (34%)]\tLoss: 1.443301, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [9000/23817 (38%)]\tLoss: 1.475324, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [10000/23817 (42%)]\tLoss: 1.468712, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [11000/23817 (46%)]\tLoss: 1.516734, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [12000/23817 (50%)]\tLoss: 1.499316, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [13000/23817 (55%)]\tLoss: 1.463452, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [14000/23817 (59%)]\tLoss: 1.453444, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [15000/23817 (63%)]\tLoss: 1.521510, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [16000/23817 (67%)]\tLoss: 1.465399, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [17000/23817 (71%)]\tLoss: 1.503659, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [18000/23817 (76%)]\tLoss: 1.459112, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [19000/23817 (80%)]\tLoss: 1.460705, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [20000/23817 (84%)]\tLoss: 1.502846, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [21000/23817 (88%)]\tLoss: 1.458342, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [22000/23817 (92%)]\tLoss: 1.459920, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [23000/23817 (97%)]\tLoss: 1.448096, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.4718, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6824369747899159\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5661, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5649, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64), tensor(1.5073, device='cuda:0', dtype=torch.float64), tensor(1.4893, device='cuda:0', dtype=torch.float64), tensor(1.4718, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 9 [0/2884 (0%)]\tLoss: 1.465191, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [100/2884 (4%)]\tLoss: 1.483000, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [200/2884 (7%)]\tLoss: 1.569167, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [300/2884 (11%)]\tLoss: 1.469586, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [400/2884 (14%)]\tLoss: 1.638119, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [500/2884 (18%)]\tLoss: 1.597009, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [600/2884 (21%)]\tLoss: 1.394699, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [700/2884 (25%)]\tLoss: 1.482737, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [800/2884 (29%)]\tLoss: 1.702878, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [900/2884 (32%)]\tLoss: 1.603122, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1000/2884 (36%)]\tLoss: 1.487595, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1100/2884 (39%)]\tLoss: 1.688327, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1200/2884 (43%)]\tLoss: 1.741069, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1300/2884 (46%)]\tLoss: 1.422030, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1400/2884 (50%)]\tLoss: 1.464309, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1500/2884 (54%)]\tLoss: 1.534191, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1600/2884 (57%)]\tLoss: 1.563009, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1700/2884 (61%)]\tLoss: 1.460293, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1800/2884 (64%)]\tLoss: 1.578491, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1900/2884 (68%)]\tLoss: 1.520587, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2000/2884 (71%)]\tLoss: 1.632352, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2100/2884 (75%)]\tLoss: 1.704882, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2200/2884 (79%)]\tLoss: 1.546388, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2300/2884 (82%)]\tLoss: 1.593653, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2400/2884 (86%)]\tLoss: 1.794598, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2500/2884 (89%)]\tLoss: 1.525446, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2600/2884 (93%)]\tLoss: 1.659500, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [2700/2884 (96%)]\tLoss: 1.651672, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5705, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6110714285714286\n",
            "losses: [tensor(1.6361, device='cuda:0', dtype=torch.float64), tensor(1.6319, device='cuda:0', dtype=torch.float64), tensor(1.6197, device='cuda:0', dtype=torch.float64), tensor(1.5834, device='cuda:0', dtype=torch.float64), tensor(1.5737, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5661, device='cuda:0', dtype=torch.float64), tensor(1.5713, device='cuda:0', dtype=torch.float64), tensor(1.5649, device='cuda:0', dtype=torch.float64), tensor(1.5705, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6334, device='cuda:0', dtype=torch.float64), tensor(1.6251, device='cuda:0', dtype=torch.float64), tensor(1.5870, device='cuda:0', dtype=torch.float64), tensor(1.5566, device='cuda:0', dtype=torch.float64), tensor(1.5402, device='cuda:0', dtype=torch.float64), tensor(1.5246, device='cuda:0', dtype=torch.float64), tensor(1.5073, device='cuda:0', dtype=torch.float64), tensor(1.4893, device='cuda:0', dtype=torch.float64), tensor(1.4718, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.635629, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 1.635683, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 1.635643, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 1.634822, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 1.635579, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 1.635187, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/12119 (50%)]\tLoss: 1.635643, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/12119 (58%)]\tLoss: 1.636495, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/12119 (66%)]\tLoss: 1.634632, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/12119 (74%)]\tLoss: 1.635089, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/12119 (83%)]\tLoss: 1.635131, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/12119 (91%)]\tLoss: 1.635538, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/12119 (99%)]\tLoss: 1.635404, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6356, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5000826446280991\n",
            "losses: []\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 1.633313, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/1507 (7%)]\tLoss: 1.633366, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/1507 (13%)]\tLoss: 1.634666, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/1507 (20%)]\tLoss: 1.634449, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/1507 (27%)]\tLoss: 1.632670, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/1507 (33%)]\tLoss: 1.634558, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/1507 (40%)]\tLoss: 1.634504, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/1507 (47%)]\tLoss: 1.633653, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/1507 (53%)]\tLoss: 1.632873, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/1507 (60%)]\tLoss: 1.635107, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/1507 (67%)]\tLoss: 1.637780, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/1507 (73%)]\tLoss: 1.637709, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/1507 (80%)]\tLoss: 1.636730, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/1507 (87%)]\tLoss: 1.636935, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/1507 (93%)]\tLoss: 1.638577, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6351, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.556\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 1.635304, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/12119 (8%)]\tLoss: 1.635467, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/12119 (17%)]\tLoss: 1.635760, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/12119 (25%)]\tLoss: 1.635448, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/12119 (33%)]\tLoss: 1.635399, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/12119 (41%)]\tLoss: 1.635385, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/12119 (50%)]\tLoss: 1.634998, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/12119 (58%)]\tLoss: 1.635103, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/12119 (66%)]\tLoss: 1.636116, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/12119 (74%)]\tLoss: 1.636213, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/12119 (83%)]\tLoss: 1.635442, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/12119 (91%)]\tLoss: 1.635237, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/12119 (99%)]\tLoss: 1.635478, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6356, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.4993388429752066\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 1.633766, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/1507 (7%)]\tLoss: 1.633940, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/1507 (13%)]\tLoss: 1.634819, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/1507 (20%)]\tLoss: 1.634956, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/1507 (27%)]\tLoss: 1.632548, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/1507 (33%)]\tLoss: 1.634688, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/1507 (40%)]\tLoss: 1.635053, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/1507 (47%)]\tLoss: 1.634606, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/1507 (53%)]\tLoss: 1.633988, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/1507 (60%)]\tLoss: 1.634939, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/1507 (67%)]\tLoss: 1.637437, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/1507 (73%)]\tLoss: 1.638378, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/1507 (80%)]\tLoss: 1.636063, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/1507 (87%)]\tLoss: 1.636531, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/1507 (93%)]\tLoss: 1.637665, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6353, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5406666666666666\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 1.635245, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/12119 (8%)]\tLoss: 1.634996, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/12119 (17%)]\tLoss: 1.635730, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/12119 (25%)]\tLoss: 1.635503, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/12119 (33%)]\tLoss: 1.634811, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/12119 (41%)]\tLoss: 1.635323, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/12119 (50%)]\tLoss: 1.635413, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/12119 (58%)]\tLoss: 1.635883, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/12119 (66%)]\tLoss: 1.635896, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/12119 (74%)]\tLoss: 1.635340, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/12119 (83%)]\tLoss: 1.635743, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/12119 (91%)]\tLoss: 1.635462, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/12119 (99%)]\tLoss: 1.634704, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6354, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5102479338842976\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 1.634620, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/1507 (7%)]\tLoss: 1.633354, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/1507 (13%)]\tLoss: 1.634560, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/1507 (20%)]\tLoss: 1.634990, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/1507 (27%)]\tLoss: 1.633875, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/1507 (33%)]\tLoss: 1.634524, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/1507 (40%)]\tLoss: 1.634750, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/1507 (47%)]\tLoss: 1.634667, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/1507 (53%)]\tLoss: 1.634176, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/1507 (60%)]\tLoss: 1.635472, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/1507 (67%)]\tLoss: 1.636519, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/1507 (73%)]\tLoss: 1.637210, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/1507 (80%)]\tLoss: 1.635675, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/1507 (87%)]\tLoss: 1.636045, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/1507 (93%)]\tLoss: 1.636749, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6351, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5373333333333333\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/12119 (0%)]\tLoss: 1.635520, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/12119 (8%)]\tLoss: 1.634694, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/12119 (17%)]\tLoss: 1.635286, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/12119 (25%)]\tLoss: 1.635414, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/12119 (33%)]\tLoss: 1.635000, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/12119 (41%)]\tLoss: 1.634706, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/12119 (50%)]\tLoss: 1.634745, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/12119 (58%)]\tLoss: 1.634929, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/12119 (66%)]\tLoss: 1.634911, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/12119 (74%)]\tLoss: 1.635067, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/12119 (83%)]\tLoss: 1.634731, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/12119 (91%)]\tLoss: 1.634694, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/12119 (99%)]\tLoss: 1.634957, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6349, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5200826446280992\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/1507 (0%)]\tLoss: 1.634865, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/1507 (7%)]\tLoss: 1.634579, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/1507 (13%)]\tLoss: 1.635781, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/1507 (20%)]\tLoss: 1.634732, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/1507 (27%)]\tLoss: 1.635623, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/1507 (33%)]\tLoss: 1.634713, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/1507 (40%)]\tLoss: 1.635020, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/1507 (47%)]\tLoss: 1.634884, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/1507 (53%)]\tLoss: 1.635274, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/1507 (60%)]\tLoss: 1.635293, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/1507 (67%)]\tLoss: 1.634743, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/1507 (73%)]\tLoss: 1.634613, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/1507 (80%)]\tLoss: 1.635062, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/1507 (87%)]\tLoss: 1.634273, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/1507 (93%)]\tLoss: 1.634967, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6350, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5013333333333333\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/12119 (0%)]\tLoss: 1.635022, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/12119 (8%)]\tLoss: 1.633816, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/12119 (17%)]\tLoss: 1.634134, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/12119 (25%)]\tLoss: 1.633649, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/12119 (33%)]\tLoss: 1.633512, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/12119 (41%)]\tLoss: 1.632847, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/12119 (50%)]\tLoss: 1.632975, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/12119 (58%)]\tLoss: 1.631743, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/12119 (66%)]\tLoss: 1.631022, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/12119 (74%)]\tLoss: 1.631482, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/12119 (83%)]\tLoss: 1.630440, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/12119 (91%)]\tLoss: 1.631090, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/12119 (99%)]\tLoss: 1.631285, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6327, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5278512396694215\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/1507 (0%)]\tLoss: 1.635674, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/1507 (7%)]\tLoss: 1.635338, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/1507 (13%)]\tLoss: 1.635048, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/1507 (20%)]\tLoss: 1.633817, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/1507 (27%)]\tLoss: 1.638334, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/1507 (33%)]\tLoss: 1.632831, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/1507 (40%)]\tLoss: 1.635417, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/1507 (47%)]\tLoss: 1.635216, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/1507 (53%)]\tLoss: 1.637032, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/1507 (60%)]\tLoss: 1.633814, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/1507 (67%)]\tLoss: 1.630013, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/1507 (73%)]\tLoss: 1.629493, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/1507 (80%)]\tLoss: 1.631880, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/1507 (87%)]\tLoss: 1.630515, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/1507 (93%)]\tLoss: 1.628932, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6336, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.502\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/12119 (0%)]\tLoss: 1.630759, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/12119 (8%)]\tLoss: 1.631162, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/12119 (17%)]\tLoss: 1.629802, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/12119 (25%)]\tLoss: 1.630162, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/12119 (33%)]\tLoss: 1.627823, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/12119 (41%)]\tLoss: 1.627978, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/12119 (50%)]\tLoss: 1.629943, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/12119 (58%)]\tLoss: 1.627956, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/12119 (66%)]\tLoss: 1.626368, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/12119 (74%)]\tLoss: 1.627977, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/12119 (83%)]\tLoss: 1.626480, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/12119 (91%)]\tLoss: 1.625022, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/12119 (99%)]\tLoss: 1.626024, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6286, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.532396694214876\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/1507 (0%)]\tLoss: 1.636298, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/1507 (7%)]\tLoss: 1.632393, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/1507 (13%)]\tLoss: 1.633974, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/1507 (20%)]\tLoss: 1.630953, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/1507 (27%)]\tLoss: 1.640122, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/1507 (33%)]\tLoss: 1.629552, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/1507 (40%)]\tLoss: 1.632893, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/1507 (47%)]\tLoss: 1.633024, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/1507 (53%)]\tLoss: 1.635133, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/1507 (60%)]\tLoss: 1.630728, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/1507 (67%)]\tLoss: 1.625393, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/1507 (73%)]\tLoss: 1.625241, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/1507 (80%)]\tLoss: 1.625471, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/1507 (87%)]\tLoss: 1.627076, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/1507 (93%)]\tLoss: 1.627239, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6310, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.504\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/12119 (0%)]\tLoss: 1.627870, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/12119 (8%)]\tLoss: 1.626349, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/12119 (17%)]\tLoss: 1.623941, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/12119 (25%)]\tLoss: 1.624144, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/12119 (33%)]\tLoss: 1.620087, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/12119 (41%)]\tLoss: 1.623657, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/12119 (50%)]\tLoss: 1.617469, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/12119 (58%)]\tLoss: 1.617735, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/12119 (66%)]\tLoss: 1.611990, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/12119 (74%)]\tLoss: 1.614240, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/12119 (83%)]\tLoss: 1.615289, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/12119 (91%)]\tLoss: 1.608393, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/12119 (99%)]\tLoss: 1.602984, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6174, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5691735537190082\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/1507 (0%)]\tLoss: 1.633517, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/1507 (7%)]\tLoss: 1.623996, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/1507 (13%)]\tLoss: 1.635267, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/1507 (20%)]\tLoss: 1.617475, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/1507 (27%)]\tLoss: 1.647698, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/1507 (33%)]\tLoss: 1.604797, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/1507 (40%)]\tLoss: 1.626546, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/1507 (47%)]\tLoss: 1.627552, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/1507 (53%)]\tLoss: 1.631751, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/1507 (60%)]\tLoss: 1.616500, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/1507 (67%)]\tLoss: 1.593230, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/1507 (73%)]\tLoss: 1.593315, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/1507 (80%)]\tLoss: 1.595205, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/1507 (87%)]\tLoss: 1.596204, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/1507 (93%)]\tLoss: 1.592343, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6157, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5213333333333333\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64), tensor(1.6157, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/12119 (0%)]\tLoss: 1.608637, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [1000/12119 (8%)]\tLoss: 1.593769, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [2000/12119 (17%)]\tLoss: 1.596307, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [3000/12119 (25%)]\tLoss: 1.595497, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [4000/12119 (33%)]\tLoss: 1.577065, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [5000/12119 (41%)]\tLoss: 1.582833, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [6000/12119 (50%)]\tLoss: 1.579465, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [7000/12119 (58%)]\tLoss: 1.564966, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [8000/12119 (66%)]\tLoss: 1.573500, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [9000/12119 (74%)]\tLoss: 1.582906, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [10000/12119 (83%)]\tLoss: 1.555352, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [11000/12119 (91%)]\tLoss: 1.581998, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [12000/12119 (99%)]\tLoss: 1.562767, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5800, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6207438016528926\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64), tensor(1.6157, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64), tensor(1.5800, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/1507 (0%)]\tLoss: 1.566045, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [100/1507 (7%)]\tLoss: 1.546079, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [200/1507 (13%)]\tLoss: 1.633009, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [300/1507 (20%)]\tLoss: 1.592490, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [400/1507 (27%)]\tLoss: 1.644010, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [500/1507 (33%)]\tLoss: 1.517572, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [600/1507 (40%)]\tLoss: 1.601472, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [700/1507 (47%)]\tLoss: 1.578922, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [800/1507 (53%)]\tLoss: 1.570863, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [900/1507 (60%)]\tLoss: 1.569127, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1000/1507 (67%)]\tLoss: 1.588864, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1100/1507 (73%)]\tLoss: 1.607116, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1200/1507 (80%)]\tLoss: 1.540143, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1300/1507 (87%)]\tLoss: 1.573074, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1400/1507 (93%)]\tLoss: 1.599112, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5819, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.608\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64), tensor(1.6157, device='cuda:0', dtype=torch.float64), tensor(1.5819, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64), tensor(1.5800, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/12119 (0%)]\tLoss: 1.562886, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [1000/12119 (8%)]\tLoss: 1.532957, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [2000/12119 (17%)]\tLoss: 1.554211, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [3000/12119 (25%)]\tLoss: 1.539849, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [4000/12119 (33%)]\tLoss: 1.540402, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [5000/12119 (41%)]\tLoss: 1.550927, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [6000/12119 (50%)]\tLoss: 1.554473, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [7000/12119 (58%)]\tLoss: 1.562415, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [8000/12119 (66%)]\tLoss: 1.523773, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [9000/12119 (74%)]\tLoss: 1.577668, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [10000/12119 (83%)]\tLoss: 1.531481, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [11000/12119 (91%)]\tLoss: 1.543697, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 8 [12000/12119 (99%)]\tLoss: 1.547464, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5468, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6470247933884298\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64), tensor(1.6157, device='cuda:0', dtype=torch.float64), tensor(1.5819, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64), tensor(1.5800, device='cuda:0', dtype=torch.float64), tensor(1.5468, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 8 [0/1507 (0%)]\tLoss: 1.553230, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [100/1507 (7%)]\tLoss: 1.530462, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [200/1507 (13%)]\tLoss: 1.670429, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [300/1507 (20%)]\tLoss: 1.582277, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [400/1507 (27%)]\tLoss: 1.682274, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [500/1507 (33%)]\tLoss: 1.473329, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [600/1507 (40%)]\tLoss: 1.604379, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [700/1507 (47%)]\tLoss: 1.591170, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [800/1507 (53%)]\tLoss: 1.584247, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [900/1507 (60%)]\tLoss: 1.568613, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1000/1507 (67%)]\tLoss: 1.563653, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1100/1507 (73%)]\tLoss: 1.584020, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1200/1507 (80%)]\tLoss: 1.506194, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1300/1507 (87%)]\tLoss: 1.544301, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 8 [1400/1507 (93%)]\tLoss: 1.578828, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5745, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.604\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64), tensor(1.6157, device='cuda:0', dtype=torch.float64), tensor(1.5819, device='cuda:0', dtype=torch.float64), tensor(1.5745, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64), tensor(1.5800, device='cuda:0', dtype=torch.float64), tensor(1.5468, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/12119 (0%)]\tLoss: 1.525707, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [1000/12119 (8%)]\tLoss: 1.544540, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [2000/12119 (17%)]\tLoss: 1.517221, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [3000/12119 (25%)]\tLoss: 1.533609, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [4000/12119 (33%)]\tLoss: 1.539397, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [5000/12119 (41%)]\tLoss: 1.490915, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [6000/12119 (50%)]\tLoss: 1.512822, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [7000/12119 (58%)]\tLoss: 1.535475, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [8000/12119 (66%)]\tLoss: 1.480084, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [9000/12119 (74%)]\tLoss: 1.536452, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [10000/12119 (83%)]\tLoss: 1.514109, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [11000/12119 (91%)]\tLoss: 1.500426, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 9 [12000/12119 (99%)]\tLoss: 1.488110, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5257, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6671900826446281\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64), tensor(1.6157, device='cuda:0', dtype=torch.float64), tensor(1.5819, device='cuda:0', dtype=torch.float64), tensor(1.5745, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64), tensor(1.5800, device='cuda:0', dtype=torch.float64), tensor(1.5468, device='cuda:0', dtype=torch.float64), tensor(1.5257, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 9 [0/1507 (0%)]\tLoss: 1.516780, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [100/1507 (7%)]\tLoss: 1.482086, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [200/1507 (13%)]\tLoss: 1.680772, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [300/1507 (20%)]\tLoss: 1.575704, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [400/1507 (27%)]\tLoss: 1.653845, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [500/1507 (33%)]\tLoss: 1.443244, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [600/1507 (40%)]\tLoss: 1.597519, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [700/1507 (47%)]\tLoss: 1.559715, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [800/1507 (53%)]\tLoss: 1.540803, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [900/1507 (60%)]\tLoss: 1.548139, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1000/1507 (67%)]\tLoss: 1.601299, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1100/1507 (73%)]\tLoss: 1.618070, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1200/1507 (80%)]\tLoss: 1.477609, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1300/1507 (87%)]\tLoss: 1.555121, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 9 [1400/1507 (93%)]\tLoss: 1.601441, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5635, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6133333333333333\n",
            "losses: [tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6353, device='cuda:0', dtype=torch.float64), tensor(1.6351, device='cuda:0', dtype=torch.float64), tensor(1.6350, device='cuda:0', dtype=torch.float64), tensor(1.6336, device='cuda:0', dtype=torch.float64), tensor(1.6310, device='cuda:0', dtype=torch.float64), tensor(1.6157, device='cuda:0', dtype=torch.float64), tensor(1.5819, device='cuda:0', dtype=torch.float64), tensor(1.5745, device='cuda:0', dtype=torch.float64), tensor(1.5635, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6354, device='cuda:0', dtype=torch.float64), tensor(1.6349, device='cuda:0', dtype=torch.float64), tensor(1.6327, device='cuda:0', dtype=torch.float64), tensor(1.6286, device='cuda:0', dtype=torch.float64), tensor(1.6174, device='cuda:0', dtype=torch.float64), tensor(1.5800, device='cuda:0', dtype=torch.float64), tensor(1.5468, device='cuda:0', dtype=torch.float64), tensor(1.5257, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAogAAAEbCAYAAABZU3XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFBEREZECCogiIiIiUkABUUREREQK/H80fhjwyB4LEQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "877db738-69b5-49e0-9731-f725e069ce26"
      },
      "source": [
        "evaluation_summary(\"sheena model\", sheena_predicted_ys.cpu(), datasets[\"politifact\"].val_data)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: sheena model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-578d4096a171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sheena model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheena_predicted_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"politifact\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-c11f08302229>\u001b[0m in \u001b[0;36mevaluation_summary\u001b[0;34m(description, predictions, true_labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluation_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation for: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1670\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1533, 2884]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "##OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFA1PNpzA8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeclareParameters:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0.5\n",
        "  epochs = 10\n",
        "  C = 0\n",
        "  is_debug = True\n",
        "  lr=0.0002\n",
        "  decay = 0.0009\n",
        "  grad_clip = False\n",
        "  early_stopping = 4\n",
        "  use_early_stopping = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "outputId": "60a282fa-0c5f-4f3a-9ecd-fdf69a7aaf76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"real_declare\"], datasets[\"snopes\"], DeclareParameters)\n",
        "run_model(models[\"real_declare\"], datasets[\"politifact\"], DeclareParameters)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 0.699827, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 0.697094, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 0.694691, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 0.702312, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 0.693329, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 0.681124, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/12119 (50%)]\tLoss: 0.695653, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/12119 (58%)]\tLoss: 0.694296, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/12119 (66%)]\tLoss: 0.685706, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/12119 (74%)]\tLoss: 0.694656, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/12119 (83%)]\tLoss: 0.688081, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/12119 (91%)]\tLoss: 0.693859, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/12119 (99%)]\tLoss: 0.693776, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6949, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.4990909090909091\n",
            "losses: []\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 0.666455, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/1507 (7%)]\tLoss: 0.670523, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/1507 (13%)]\tLoss: 0.682082, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/1507 (20%)]\tLoss: 0.683867, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/1507 (27%)]\tLoss: 0.658457, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/1507 (33%)]\tLoss: 0.681776, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/1507 (40%)]\tLoss: 0.682655, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/1507 (47%)]\tLoss: 0.674668, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/1507 (53%)]\tLoss: 0.663513, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/1507 (60%)]\tLoss: 0.688728, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/1507 (67%)]\tLoss: 0.723566, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/1507 (73%)]\tLoss: 0.724279, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/1507 (80%)]\tLoss: 0.703563, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/1507 (87%)]\tLoss: 0.707720, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/1507 (93%)]\tLoss: 0.725232, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6891, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.558\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 0.698694, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/12119 (8%)]\tLoss: 0.694392, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/12119 (17%)]\tLoss: 0.694187, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/12119 (25%)]\tLoss: 0.690509, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/12119 (33%)]\tLoss: 0.688635, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/12119 (41%)]\tLoss: 0.687605, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/12119 (50%)]\tLoss: 0.691127, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/12119 (58%)]\tLoss: 0.694557, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/12119 (66%)]\tLoss: 0.690337, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/12119 (74%)]\tLoss: 0.695111, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/12119 (83%)]\tLoss: 0.697030, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/12119 (91%)]\tLoss: 0.689895, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/12119 (99%)]\tLoss: 0.692644, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6932, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.4996694214876033\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 0.680439, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/1507 (7%)]\tLoss: 0.682396, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/1507 (13%)]\tLoss: 0.688502, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/1507 (20%)]\tLoss: 0.687991, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/1507 (27%)]\tLoss: 0.678304, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/1507 (33%)]\tLoss: 0.685834, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/1507 (40%)]\tLoss: 0.687903, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/1507 (47%)]\tLoss: 0.684678, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/1507 (53%)]\tLoss: 0.678999, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/1507 (60%)]\tLoss: 0.689564, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/1507 (67%)]\tLoss: 0.705411, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/1507 (73%)]\tLoss: 0.705669, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/1507 (80%)]\tLoss: 0.695824, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/1507 (87%)]\tLoss: 0.698062, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/1507 (93%)]\tLoss: 0.705744, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6904, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.558\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 0.690733, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/12119 (8%)]\tLoss: 0.685217, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/12119 (17%)]\tLoss: 0.692202, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/12119 (25%)]\tLoss: 0.691658, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/12119 (33%)]\tLoss: 0.693579, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/12119 (41%)]\tLoss: 0.690693, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/12119 (50%)]\tLoss: 0.692743, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/12119 (58%)]\tLoss: 0.683422, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/12119 (66%)]\tLoss: 0.681785, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/12119 (74%)]\tLoss: 0.688805, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/12119 (83%)]\tLoss: 0.679477, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/12119 (91%)]\tLoss: 0.693966, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/12119 (99%)]\tLoss: 0.675683, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6879, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5613223140495868\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 0.697494, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/1507 (7%)]\tLoss: 0.703167, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/1507 (13%)]\tLoss: 0.705592, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/1507 (20%)]\tLoss: 0.691914, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/1507 (27%)]\tLoss: 0.724765, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/1507 (33%)]\tLoss: 0.678694, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/1507 (40%)]\tLoss: 0.698774, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/1507 (47%)]\tLoss: 0.704959, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/1507 (53%)]\tLoss: 0.703822, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/1507 (60%)]\tLoss: 0.683681, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/1507 (67%)]\tLoss: 0.663561, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/1507 (73%)]\tLoss: 0.664437, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/1507 (80%)]\tLoss: 0.671253, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/1507 (87%)]\tLoss: 0.666868, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/1507 (93%)]\tLoss: 0.657024, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6877, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5533333333333333\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/12119 (0%)]\tLoss: 0.679725, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/12119 (8%)]\tLoss: 0.671982, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/12119 (17%)]\tLoss: 0.667763, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/12119 (25%)]\tLoss: 0.663960, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/12119 (33%)]\tLoss: 0.668579, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/12119 (41%)]\tLoss: 0.646067, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/12119 (50%)]\tLoss: 0.655883, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/12119 (58%)]\tLoss: 0.645347, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/12119 (66%)]\tLoss: 0.658523, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/12119 (74%)]\tLoss: 0.646843, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/12119 (83%)]\tLoss: 0.653780, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/12119 (91%)]\tLoss: 0.650752, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/12119 (99%)]\tLoss: 0.602276, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6543, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6773553719008264\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/1507 (0%)]\tLoss: 0.655927, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/1507 (7%)]\tLoss: 0.645702, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/1507 (13%)]\tLoss: 0.728350, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/1507 (20%)]\tLoss: 0.651060, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/1507 (27%)]\tLoss: 0.776654, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/1507 (33%)]\tLoss: 0.592252, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/1507 (40%)]\tLoss: 0.706682, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/1507 (47%)]\tLoss: 0.681696, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/1507 (53%)]\tLoss: 0.697925, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/1507 (60%)]\tLoss: 0.645945, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/1507 (67%)]\tLoss: 0.617472, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/1507 (73%)]\tLoss: 0.675365, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/1507 (80%)]\tLoss: 0.615659, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/1507 (87%)]\tLoss: 0.626556, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/1507 (93%)]\tLoss: 0.618424, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6624, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6306666666666667\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/12119 (0%)]\tLoss: 0.611825, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/12119 (8%)]\tLoss: 0.608652, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/12119 (17%)]\tLoss: 0.630184, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/12119 (25%)]\tLoss: 0.592266, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/12119 (33%)]\tLoss: 0.556053, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/12119 (41%)]\tLoss: 0.581349, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/12119 (50%)]\tLoss: 0.540845, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/12119 (58%)]\tLoss: 0.567234, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/12119 (66%)]\tLoss: 0.565408, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/12119 (74%)]\tLoss: 0.622660, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/12119 (83%)]\tLoss: 0.474070, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/12119 (91%)]\tLoss: 0.534721, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/12119 (99%)]\tLoss: 0.543910, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.5796, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.7390082644628099\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/1507 (0%)]\tLoss: 0.720498, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/1507 (7%)]\tLoss: 0.748577, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/1507 (13%)]\tLoss: 0.845223, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/1507 (20%)]\tLoss: 0.683759, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/1507 (27%)]\tLoss: 0.996448, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/1507 (33%)]\tLoss: 0.576851, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/1507 (40%)]\tLoss: 0.786085, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/1507 (47%)]\tLoss: 0.794977, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/1507 (53%)]\tLoss: 0.774320, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/1507 (60%)]\tLoss: 0.649175, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/1507 (67%)]\tLoss: 0.501235, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/1507 (73%)]\tLoss: 0.592191, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/1507 (80%)]\tLoss: 0.592712, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/1507 (87%)]\tLoss: 0.598314, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/1507 (93%)]\tLoss: 0.537876, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6932, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.602\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/12119 (0%)]\tLoss: 0.576950, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/12119 (8%)]\tLoss: 0.555200, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/12119 (17%)]\tLoss: 0.455352, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/12119 (25%)]\tLoss: 0.482139, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/12119 (33%)]\tLoss: 0.502975, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/12119 (41%)]\tLoss: 0.529737, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/12119 (50%)]\tLoss: 0.410669, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/12119 (58%)]\tLoss: 0.536502, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/12119 (66%)]\tLoss: 0.623375, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/12119 (74%)]\tLoss: 0.499457, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/12119 (83%)]\tLoss: 0.558822, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/12119 (91%)]\tLoss: 0.484665, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/12119 (99%)]\tLoss: 0.594298, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.5176, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.7773553719008265\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/1507 (0%)]\tLoss: 0.630600, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/1507 (7%)]\tLoss: 0.667634, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/1507 (13%)]\tLoss: 0.854913, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/1507 (20%)]\tLoss: 0.628807, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/1507 (27%)]\tLoss: 0.957212, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/1507 (33%)]\tLoss: 0.536429, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/1507 (40%)]\tLoss: 0.787925, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/1507 (47%)]\tLoss: 0.751408, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/1507 (53%)]\tLoss: 0.750200, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/1507 (60%)]\tLoss: 0.618515, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/1507 (67%)]\tLoss: 0.610297, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/1507 (73%)]\tLoss: 0.745648, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/1507 (80%)]\tLoss: 0.639157, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/1507 (87%)]\tLoss: 0.665193, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/1507 (93%)]\tLoss: 0.658493, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.7002, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6066666666666667\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/12119 (0%)]\tLoss: 0.498063, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/12119 (8%)]\tLoss: 0.472260, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/12119 (17%)]\tLoss: 0.413358, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/12119 (25%)]\tLoss: 0.491294, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/12119 (33%)]\tLoss: 0.464174, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/12119 (41%)]\tLoss: 0.446821, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/12119 (50%)]\tLoss: 0.512348, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/12119 (58%)]\tLoss: 0.515351, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/12119 (66%)]\tLoss: 0.365489, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/12119 (74%)]\tLoss: 0.503914, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/12119 (83%)]\tLoss: 0.440118, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/12119 (91%)]\tLoss: 0.468644, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/12119 (99%)]\tLoss: 0.445495, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.4808, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.8034710743801653\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/1507 (0%)]\tLoss: 0.646152, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/1507 (7%)]\tLoss: 0.676712, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/1507 (13%)]\tLoss: 0.899916, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/1507 (20%)]\tLoss: 0.620046, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/1507 (27%)]\tLoss: 0.963634, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/1507 (33%)]\tLoss: 0.545241, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/1507 (40%)]\tLoss: 0.809977, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/1507 (47%)]\tLoss: 0.775594, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/1507 (53%)]\tLoss: 0.773866, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/1507 (60%)]\tLoss: 0.609407, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/1507 (67%)]\tLoss: 0.614967, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/1507 (73%)]\tLoss: 0.790264, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/1507 (80%)]\tLoss: 0.666969, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/1507 (87%)]\tLoss: 0.693228, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/1507 (93%)]\tLoss: 0.669099, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.7170, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.612\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/12119 (0%)]\tLoss: 0.389869, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [1000/12119 (8%)]\tLoss: 0.429734, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [2000/12119 (17%)]\tLoss: 0.479572, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [3000/12119 (25%)]\tLoss: 0.410558, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [4000/12119 (33%)]\tLoss: 0.385864, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [5000/12119 (41%)]\tLoss: 0.442638, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [6000/12119 (50%)]\tLoss: 0.371564, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [7000/12119 (58%)]\tLoss: 0.463393, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [8000/12119 (66%)]\tLoss: 0.414937, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [9000/12119 (74%)]\tLoss: 0.519634, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [10000/12119 (83%)]\tLoss: 0.385039, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [11000/12119 (91%)]\tLoss: 0.443368, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [12000/12119 (99%)]\tLoss: 0.422608, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4351, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.8320661157024793\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64), tensor(0.4351, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/1507 (0%)]\tLoss: 0.646152, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [100/1507 (7%)]\tLoss: 0.676712, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [200/1507 (13%)]\tLoss: 0.899916, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [300/1507 (20%)]\tLoss: 0.620046, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [400/1507 (27%)]\tLoss: 0.963634, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [500/1507 (33%)]\tLoss: 0.545241, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [600/1507 (40%)]\tLoss: 0.809977, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [700/1507 (47%)]\tLoss: 0.775594, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [800/1507 (53%)]\tLoss: 0.773866, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [900/1507 (60%)]\tLoss: 0.609407, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1000/1507 (67%)]\tLoss: 0.614967, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1100/1507 (73%)]\tLoss: 0.790264, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1200/1507 (80%)]\tLoss: 0.666969, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1300/1507 (87%)]\tLoss: 0.693228, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1400/1507 (93%)]\tLoss: 0.669099, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.7170, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.612\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64), tensor(0.4351, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/12119 (0%)]\tLoss: 0.365813, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [1000/12119 (8%)]\tLoss: 0.526694, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [2000/12119 (17%)]\tLoss: 0.397205, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [3000/12119 (25%)]\tLoss: 0.442288, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [4000/12119 (33%)]\tLoss: 0.369680, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [5000/12119 (41%)]\tLoss: 0.421620, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [6000/12119 (50%)]\tLoss: 0.402008, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [7000/12119 (58%)]\tLoss: 0.375944, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [8000/12119 (66%)]\tLoss: 0.519195, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [9000/12119 (74%)]\tLoss: 0.402408, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [10000/12119 (83%)]\tLoss: 0.444555, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [11000/12119 (91%)]\tLoss: 0.409407, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [12000/12119 (99%)]\tLoss: 0.498122, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4352, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.8319834710743802\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64), tensor(0.4351, device='cuda:0', dtype=torch.float64), tensor(0.4352, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 8 [0/1507 (0%)]\tLoss: 0.646152, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [100/1507 (7%)]\tLoss: 0.676712, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [200/1507 (13%)]\tLoss: 0.899916, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [300/1507 (20%)]\tLoss: 0.620046, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [400/1507 (27%)]\tLoss: 0.963634, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [500/1507 (33%)]\tLoss: 0.545241, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [600/1507 (40%)]\tLoss: 0.809977, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [700/1507 (47%)]\tLoss: 0.775594, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [800/1507 (53%)]\tLoss: 0.773866, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [900/1507 (60%)]\tLoss: 0.609407, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1000/1507 (67%)]\tLoss: 0.614967, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1100/1507 (73%)]\tLoss: 0.790264, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1200/1507 (80%)]\tLoss: 0.666969, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1300/1507 (87%)]\tLoss: 0.693228, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1400/1507 (93%)]\tLoss: 0.669099, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.7170, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.612\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64), tensor(0.4351, device='cuda:0', dtype=torch.float64), tensor(0.4352, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/12119 (0%)]\tLoss: 0.434670, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [1000/12119 (8%)]\tLoss: 0.557457, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [2000/12119 (17%)]\tLoss: 0.377872, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [3000/12119 (25%)]\tLoss: 0.456102, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [4000/12119 (33%)]\tLoss: 0.448755, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [5000/12119 (41%)]\tLoss: 0.492174, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [6000/12119 (50%)]\tLoss: 0.454416, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [7000/12119 (58%)]\tLoss: 0.398235, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [8000/12119 (66%)]\tLoss: 0.449138, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [9000/12119 (74%)]\tLoss: 0.466087, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [10000/12119 (83%)]\tLoss: 0.488111, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [11000/12119 (91%)]\tLoss: 0.475707, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [12000/12119 (99%)]\tLoss: 0.397371, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4350, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.8320661157024793\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64), tensor(0.4351, device='cuda:0', dtype=torch.float64), tensor(0.4352, device='cuda:0', dtype=torch.float64), tensor(0.4350, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 9 [0/1507 (0%)]\tLoss: 0.646152, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [100/1507 (7%)]\tLoss: 0.676712, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [200/1507 (13%)]\tLoss: 0.899916, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [300/1507 (20%)]\tLoss: 0.620046, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [400/1507 (27%)]\tLoss: 0.963634, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [500/1507 (33%)]\tLoss: 0.545241, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [600/1507 (40%)]\tLoss: 0.809977, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [700/1507 (47%)]\tLoss: 0.775594, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [800/1507 (53%)]\tLoss: 0.773866, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [900/1507 (60%)]\tLoss: 0.609407, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1000/1507 (67%)]\tLoss: 0.614967, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1100/1507 (73%)]\tLoss: 0.790264, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1200/1507 (80%)]\tLoss: 0.666969, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1300/1507 (87%)]\tLoss: 0.693228, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1400/1507 (93%)]\tLoss: 0.669099, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.7170, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.612\n",
            "losses: [tensor(0.6891, device='cuda:0', dtype=torch.float64), tensor(0.6904, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6624, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.7002, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64), tensor(0.7170, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6949, device='cuda:0', dtype=torch.float64), tensor(0.6932, device='cuda:0', dtype=torch.float64), tensor(0.6879, device='cuda:0', dtype=torch.float64), tensor(0.6543, device='cuda:0', dtype=torch.float64), tensor(0.5796, device='cuda:0', dtype=torch.float64), tensor(0.5176, device='cuda:0', dtype=torch.float64), tensor(0.4808, device='cuda:0', dtype=torch.float64), tensor(0.4351, device='cuda:0', dtype=torch.float64), tensor(0.4352, device='cuda:0', dtype=torch.float64), tensor(0.4350, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 0.711011, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [1000/23817 (4%)]\tLoss: 0.696508, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/23817 (8%)]\tLoss: 0.721229, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/23817 (13%)]\tLoss: 0.694501, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/23817 (17%)]\tLoss: 0.711436, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/23817 (21%)]\tLoss: 0.683001, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/23817 (25%)]\tLoss: 0.700042, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/23817 (29%)]\tLoss: 0.718789, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/23817 (34%)]\tLoss: 0.706967, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/23817 (38%)]\tLoss: 0.698906, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/23817 (42%)]\tLoss: 0.705629, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/23817 (46%)]\tLoss: 0.689177, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/23817 (50%)]\tLoss: 0.695709, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [13000/23817 (55%)]\tLoss: 0.706229, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [14000/23817 (59%)]\tLoss: 0.686306, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [15000/23817 (63%)]\tLoss: 0.691554, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [16000/23817 (67%)]\tLoss: 0.710861, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [17000/23817 (71%)]\tLoss: 0.687592, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [18000/23817 (76%)]\tLoss: 0.710056, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [19000/23817 (80%)]\tLoss: 0.694254, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [20000/23817 (84%)]\tLoss: 0.696389, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [21000/23817 (88%)]\tLoss: 0.703583, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [22000/23817 (92%)]\tLoss: 0.689758, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [23000/23817 (97%)]\tLoss: 0.704038, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6989, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.4977310924369748\n",
            "losses: []\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 0.673381, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/2884 (4%)]\tLoss: 0.703847, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/2884 (7%)]\tLoss: 0.683886, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/2884 (11%)]\tLoss: 0.722728, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/2884 (14%)]\tLoss: 0.703819, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/2884 (18%)]\tLoss: 0.683901, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/2884 (21%)]\tLoss: 0.698507, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/2884 (25%)]\tLoss: 0.689642, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/2884 (29%)]\tLoss: 0.720160, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/2884 (32%)]\tLoss: 0.693818, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/2884 (36%)]\tLoss: 0.702455, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/2884 (39%)]\tLoss: 0.695678, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/2884 (43%)]\tLoss: 0.685179, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/2884 (46%)]\tLoss: 0.648805, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/2884 (50%)]\tLoss: 0.702543, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1500/2884 (54%)]\tLoss: 0.721853, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1600/2884 (57%)]\tLoss: 0.723653, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1700/2884 (61%)]\tLoss: 0.700714, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1800/2884 (64%)]\tLoss: 0.676681, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1900/2884 (68%)]\tLoss: 0.670430, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2000/2884 (71%)]\tLoss: 0.723869, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2100/2884 (75%)]\tLoss: 0.719837, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2200/2884 (79%)]\tLoss: 0.683110, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2300/2884 (82%)]\tLoss: 0.679127, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2400/2884 (86%)]\tLoss: 0.686208, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2500/2884 (89%)]\tLoss: 0.688935, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2600/2884 (93%)]\tLoss: 0.692013, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2700/2884 (96%)]\tLoss: 0.713105, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6960, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.4810714285714286\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 0.699658, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/23817 (4%)]\tLoss: 0.684427, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/23817 (8%)]\tLoss: 0.690175, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/23817 (13%)]\tLoss: 0.694577, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/23817 (17%)]\tLoss: 0.691735, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/23817 (21%)]\tLoss: 0.694059, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/23817 (25%)]\tLoss: 0.690800, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/23817 (29%)]\tLoss: 0.700512, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/23817 (34%)]\tLoss: 0.693344, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/23817 (38%)]\tLoss: 0.692012, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/23817 (42%)]\tLoss: 0.698739, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/23817 (46%)]\tLoss: 0.694143, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/23817 (50%)]\tLoss: 0.683420, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [13000/23817 (55%)]\tLoss: 0.693485, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [14000/23817 (59%)]\tLoss: 0.688631, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [15000/23817 (63%)]\tLoss: 0.693521, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [16000/23817 (67%)]\tLoss: 0.693641, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [17000/23817 (71%)]\tLoss: 0.687935, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [18000/23817 (76%)]\tLoss: 0.687961, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [19000/23817 (80%)]\tLoss: 0.686323, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [20000/23817 (84%)]\tLoss: 0.691637, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [21000/23817 (88%)]\tLoss: 0.682986, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [22000/23817 (92%)]\tLoss: 0.678599, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [23000/23817 (97%)]\tLoss: 0.687368, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6912, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.529327731092437\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 0.670509, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/2884 (4%)]\tLoss: 0.690220, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/2884 (7%)]\tLoss: 0.682788, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/2884 (11%)]\tLoss: 0.683652, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/2884 (14%)]\tLoss: 0.702269, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/2884 (18%)]\tLoss: 0.669659, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/2884 (21%)]\tLoss: 0.677683, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/2884 (25%)]\tLoss: 0.670551, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/2884 (29%)]\tLoss: 0.721268, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/2884 (32%)]\tLoss: 0.681958, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/2884 (36%)]\tLoss: 0.681190, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/2884 (39%)]\tLoss: 0.694227, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/2884 (43%)]\tLoss: 0.694935, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/2884 (46%)]\tLoss: 0.648170, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/2884 (50%)]\tLoss: 0.690105, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1500/2884 (54%)]\tLoss: 0.693421, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1600/2884 (57%)]\tLoss: 0.701193, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1700/2884 (61%)]\tLoss: 0.672338, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1800/2884 (64%)]\tLoss: 0.669467, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1900/2884 (68%)]\tLoss: 0.685927, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2000/2884 (71%)]\tLoss: 0.719792, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2100/2884 (75%)]\tLoss: 0.702942, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2200/2884 (79%)]\tLoss: 0.674980, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2300/2884 (82%)]\tLoss: 0.675612, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2400/2884 (86%)]\tLoss: 0.701263, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2500/2884 (89%)]\tLoss: 0.674479, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2600/2884 (93%)]\tLoss: 0.684315, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2700/2884 (96%)]\tLoss: 0.717538, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6869, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5478571428571428\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 0.681341, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/23817 (4%)]\tLoss: 0.682782, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/23817 (8%)]\tLoss: 0.679734, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/23817 (13%)]\tLoss: 0.666022, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/23817 (17%)]\tLoss: 0.673629, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/23817 (21%)]\tLoss: 0.672201, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/23817 (25%)]\tLoss: 0.683353, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/23817 (29%)]\tLoss: 0.676280, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/23817 (34%)]\tLoss: 0.660794, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/23817 (38%)]\tLoss: 0.651037, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/23817 (42%)]\tLoss: 0.656631, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/23817 (46%)]\tLoss: 0.676605, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/23817 (50%)]\tLoss: 0.681586, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [13000/23817 (55%)]\tLoss: 0.669878, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [14000/23817 (59%)]\tLoss: 0.670862, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [15000/23817 (63%)]\tLoss: 0.650153, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [16000/23817 (67%)]\tLoss: 0.687530, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [17000/23817 (71%)]\tLoss: 0.655064, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [18000/23817 (76%)]\tLoss: 0.671702, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [19000/23817 (80%)]\tLoss: 0.671604, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [20000/23817 (84%)]\tLoss: 0.652497, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [21000/23817 (88%)]\tLoss: 0.652291, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [22000/23817 (92%)]\tLoss: 0.652250, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [23000/23817 (97%)]\tLoss: 0.706472, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6677, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.613655462184874\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 0.643500, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/2884 (4%)]\tLoss: 0.645303, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/2884 (7%)]\tLoss: 0.673651, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/2884 (11%)]\tLoss: 0.602968, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/2884 (14%)]\tLoss: 0.702421, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/2884 (18%)]\tLoss: 0.615015, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/2884 (21%)]\tLoss: 0.590727, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/2884 (25%)]\tLoss: 0.661311, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/2884 (29%)]\tLoss: 0.718705, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/2884 (32%)]\tLoss: 0.694178, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/2884 (36%)]\tLoss: 0.599596, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/2884 (39%)]\tLoss: 0.717448, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/2884 (43%)]\tLoss: 0.726523, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/2884 (46%)]\tLoss: 0.615029, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/2884 (50%)]\tLoss: 0.637279, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1500/2884 (54%)]\tLoss: 0.660312, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1600/2884 (57%)]\tLoss: 0.662412, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1700/2884 (61%)]\tLoss: 0.637553, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1800/2884 (64%)]\tLoss: 0.658806, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1900/2884 (68%)]\tLoss: 0.690952, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2000/2884 (71%)]\tLoss: 0.721747, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2100/2884 (75%)]\tLoss: 0.695828, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2200/2884 (79%)]\tLoss: 0.653137, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2300/2884 (82%)]\tLoss: 0.669145, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2400/2884 (86%)]\tLoss: 0.762805, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2500/2884 (89%)]\tLoss: 0.653300, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2600/2884 (93%)]\tLoss: 0.685710, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2700/2884 (96%)]\tLoss: 0.740588, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6691, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5960714285714286\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23817 (0%)]\tLoss: 0.622249, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/23817 (4%)]\tLoss: 0.628894, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/23817 (8%)]\tLoss: 0.624258, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/23817 (13%)]\tLoss: 0.642221, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/23817 (17%)]\tLoss: 0.590029, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/23817 (21%)]\tLoss: 0.614816, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/23817 (25%)]\tLoss: 0.643662, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/23817 (29%)]\tLoss: 0.632413, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/23817 (34%)]\tLoss: 0.639943, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/23817 (38%)]\tLoss: 0.657071, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/23817 (42%)]\tLoss: 0.631094, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/23817 (46%)]\tLoss: 0.641482, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/23817 (50%)]\tLoss: 0.621960, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [13000/23817 (55%)]\tLoss: 0.618385, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [14000/23817 (59%)]\tLoss: 0.610004, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [15000/23817 (63%)]\tLoss: 0.615766, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [16000/23817 (67%)]\tLoss: 0.599416, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [17000/23817 (71%)]\tLoss: 0.640220, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [18000/23817 (76%)]\tLoss: 0.593125, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [19000/23817 (80%)]\tLoss: 0.641024, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [20000/23817 (84%)]\tLoss: 0.662765, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [21000/23817 (88%)]\tLoss: 0.632536, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [22000/23817 (92%)]\tLoss: 0.647322, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [23000/23817 (97%)]\tLoss: 0.610012, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6338, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6677310924369748\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/2884 (0%)]\tLoss: 0.624522, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/2884 (4%)]\tLoss: 0.643840, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/2884 (7%)]\tLoss: 0.675297, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/2884 (11%)]\tLoss: 0.618153, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/2884 (14%)]\tLoss: 0.700007, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/2884 (18%)]\tLoss: 0.589577, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/2884 (21%)]\tLoss: 0.527183, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/2884 (25%)]\tLoss: 0.659002, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/2884 (29%)]\tLoss: 0.707983, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/2884 (32%)]\tLoss: 0.713400, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/2884 (36%)]\tLoss: 0.568371, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/2884 (39%)]\tLoss: 0.750509, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/2884 (43%)]\tLoss: 0.749409, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/2884 (46%)]\tLoss: 0.578558, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/2884 (50%)]\tLoss: 0.626349, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1500/2884 (54%)]\tLoss: 0.680041, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1600/2884 (57%)]\tLoss: 0.653038, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1700/2884 (61%)]\tLoss: 0.639960, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1800/2884 (64%)]\tLoss: 0.652841, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1900/2884 (68%)]\tLoss: 0.650232, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2000/2884 (71%)]\tLoss: 0.741911, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2100/2884 (75%)]\tLoss: 0.698829, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2200/2884 (79%)]\tLoss: 0.668482, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2300/2884 (82%)]\tLoss: 0.664785, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2400/2884 (86%)]\tLoss: 0.804593, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2500/2884 (89%)]\tLoss: 0.650930, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2600/2884 (93%)]\tLoss: 0.699493, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2700/2884 (96%)]\tLoss: 0.751849, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6675, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5975\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/23817 (0%)]\tLoss: 0.644295, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/23817 (4%)]\tLoss: 0.595913, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/23817 (8%)]\tLoss: 0.685099, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/23817 (13%)]\tLoss: 0.630329, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/23817 (17%)]\tLoss: 0.648780, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/23817 (21%)]\tLoss: 0.595900, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/23817 (25%)]\tLoss: 0.613995, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/23817 (29%)]\tLoss: 0.582388, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/23817 (34%)]\tLoss: 0.594584, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/23817 (38%)]\tLoss: 0.618711, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/23817 (42%)]\tLoss: 0.535679, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/23817 (46%)]\tLoss: 0.610867, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/23817 (50%)]\tLoss: 0.599944, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [13000/23817 (55%)]\tLoss: 0.598206, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [14000/23817 (59%)]\tLoss: 0.576523, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [15000/23817 (63%)]\tLoss: 0.533969, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [16000/23817 (67%)]\tLoss: 0.608797, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [17000/23817 (71%)]\tLoss: 0.644910, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [18000/23817 (76%)]\tLoss: 0.580386, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [19000/23817 (80%)]\tLoss: 0.609820, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [20000/23817 (84%)]\tLoss: 0.605399, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [21000/23817 (88%)]\tLoss: 0.574313, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [22000/23817 (92%)]\tLoss: 0.585396, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [23000/23817 (97%)]\tLoss: 0.535940, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6001, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.7020168067226891\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/2884 (0%)]\tLoss: 0.663306, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/2884 (4%)]\tLoss: 0.670002, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/2884 (7%)]\tLoss: 0.701599, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/2884 (11%)]\tLoss: 0.583162, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/2884 (14%)]\tLoss: 0.666587, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/2884 (18%)]\tLoss: 0.609383, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/2884 (21%)]\tLoss: 0.488861, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/2884 (25%)]\tLoss: 0.647165, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/2884 (29%)]\tLoss: 0.692572, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/2884 (32%)]\tLoss: 0.748331, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/2884 (36%)]\tLoss: 0.540457, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/2884 (39%)]\tLoss: 0.795202, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/2884 (43%)]\tLoss: 0.799916, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/2884 (46%)]\tLoss: 0.607331, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/2884 (50%)]\tLoss: 0.619956, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1500/2884 (54%)]\tLoss: 0.695248, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1600/2884 (57%)]\tLoss: 0.599106, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1700/2884 (61%)]\tLoss: 0.610506, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1800/2884 (64%)]\tLoss: 0.659314, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1900/2884 (68%)]\tLoss: 0.655903, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2000/2884 (71%)]\tLoss: 0.727785, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2100/2884 (75%)]\tLoss: 0.672017, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2200/2884 (79%)]\tLoss: 0.746753, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2300/2884 (82%)]\tLoss: 0.683083, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2400/2884 (86%)]\tLoss: 0.900539, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2500/2884 (89%)]\tLoss: 0.649649, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2600/2884 (93%)]\tLoss: 0.746018, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2700/2884 (96%)]\tLoss: 0.729373, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6753, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6003571428571428\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/23817 (0%)]\tLoss: 0.538803, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/23817 (4%)]\tLoss: 0.546705, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/23817 (8%)]\tLoss: 0.554928, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/23817 (13%)]\tLoss: 0.616207, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/23817 (17%)]\tLoss: 0.556685, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/23817 (21%)]\tLoss: 0.549296, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/23817 (25%)]\tLoss: 0.520994, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/23817 (29%)]\tLoss: 0.560486, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/23817 (34%)]\tLoss: 0.613576, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/23817 (38%)]\tLoss: 0.563844, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/23817 (42%)]\tLoss: 0.546830, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/23817 (46%)]\tLoss: 0.597766, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/23817 (50%)]\tLoss: 0.593820, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [13000/23817 (55%)]\tLoss: 0.557303, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [14000/23817 (59%)]\tLoss: 0.539405, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [15000/23817 (63%)]\tLoss: 0.588964, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [16000/23817 (67%)]\tLoss: 0.560448, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [17000/23817 (71%)]\tLoss: 0.612187, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [18000/23817 (76%)]\tLoss: 0.616482, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [19000/23817 (80%)]\tLoss: 0.600995, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [20000/23817 (84%)]\tLoss: 0.523389, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [21000/23817 (88%)]\tLoss: 0.587608, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [22000/23817 (92%)]\tLoss: 0.590049, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [23000/23817 (97%)]\tLoss: 0.578841, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.5713, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.7294957983193278\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/2884 (0%)]\tLoss: 0.682878, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/2884 (4%)]\tLoss: 0.656791, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/2884 (7%)]\tLoss: 0.725155, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/2884 (11%)]\tLoss: 0.580480, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/2884 (14%)]\tLoss: 0.679767, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/2884 (18%)]\tLoss: 0.611417, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/2884 (21%)]\tLoss: 0.446220, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/2884 (25%)]\tLoss: 0.633357, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/2884 (29%)]\tLoss: 0.697307, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/2884 (32%)]\tLoss: 0.787402, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/2884 (36%)]\tLoss: 0.538392, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/2884 (39%)]\tLoss: 0.841442, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/2884 (43%)]\tLoss: 0.833591, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/2884 (46%)]\tLoss: 0.647100, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/2884 (50%)]\tLoss: 0.609167, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1500/2884 (54%)]\tLoss: 0.718953, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1600/2884 (57%)]\tLoss: 0.585859, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1700/2884 (61%)]\tLoss: 0.610464, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1800/2884 (64%)]\tLoss: 0.656106, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1900/2884 (68%)]\tLoss: 0.669598, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2000/2884 (71%)]\tLoss: 0.712923, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2100/2884 (75%)]\tLoss: 0.677752, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2200/2884 (79%)]\tLoss: 0.803299, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2300/2884 (82%)]\tLoss: 0.676280, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2400/2884 (86%)]\tLoss: 0.927115, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2500/2884 (89%)]\tLoss: 0.636960, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2600/2884 (93%)]\tLoss: 0.749790, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2700/2884 (96%)]\tLoss: 0.746299, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6836, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6060714285714286\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/23817 (0%)]\tLoss: 0.525281, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/23817 (4%)]\tLoss: 0.547813, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/23817 (8%)]\tLoss: 0.518603, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/23817 (13%)]\tLoss: 0.493975, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/23817 (17%)]\tLoss: 0.504702, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/23817 (21%)]\tLoss: 0.579205, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/23817 (25%)]\tLoss: 0.524456, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/23817 (29%)]\tLoss: 0.539441, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/23817 (34%)]\tLoss: 0.494724, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/23817 (38%)]\tLoss: 0.507350, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/23817 (42%)]\tLoss: 0.558700, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/23817 (46%)]\tLoss: 0.620783, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/23817 (50%)]\tLoss: 0.557630, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [13000/23817 (55%)]\tLoss: 0.531871, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [14000/23817 (59%)]\tLoss: 0.571156, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [15000/23817 (63%)]\tLoss: 0.542223, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [16000/23817 (67%)]\tLoss: 0.473272, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [17000/23817 (71%)]\tLoss: 0.597515, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [18000/23817 (76%)]\tLoss: 0.496692, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [19000/23817 (80%)]\tLoss: 0.516353, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [20000/23817 (84%)]\tLoss: 0.583892, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [21000/23817 (88%)]\tLoss: 0.531027, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [22000/23817 (92%)]\tLoss: 0.459511, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [23000/23817 (97%)]\tLoss: 0.590376, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.5462, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.7484873949579832\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/2884 (0%)]\tLoss: 0.693230, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/2884 (4%)]\tLoss: 0.648394, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/2884 (7%)]\tLoss: 0.752912, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/2884 (11%)]\tLoss: 0.588483, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/2884 (14%)]\tLoss: 0.674199, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/2884 (18%)]\tLoss: 0.598035, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/2884 (21%)]\tLoss: 0.438643, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/2884 (25%)]\tLoss: 0.606128, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/2884 (29%)]\tLoss: 0.713286, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/2884 (32%)]\tLoss: 0.841173, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/2884 (36%)]\tLoss: 0.534644, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/2884 (39%)]\tLoss: 0.866755, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/2884 (43%)]\tLoss: 0.885204, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/2884 (46%)]\tLoss: 0.662634, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/2884 (50%)]\tLoss: 0.617397, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1500/2884 (54%)]\tLoss: 0.744951, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1600/2884 (57%)]\tLoss: 0.586081, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1700/2884 (61%)]\tLoss: 0.613431, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1800/2884 (64%)]\tLoss: 0.685591, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1900/2884 (68%)]\tLoss: 0.672556, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2000/2884 (71%)]\tLoss: 0.739206, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2100/2884 (75%)]\tLoss: 0.677686, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2200/2884 (79%)]\tLoss: 0.807299, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2300/2884 (82%)]\tLoss: 0.703927, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2400/2884 (86%)]\tLoss: 0.985318, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2500/2884 (89%)]\tLoss: 0.642299, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2600/2884 (93%)]\tLoss: 0.757568, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2700/2884 (96%)]\tLoss: 0.777526, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6969, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6075\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/23817 (0%)]\tLoss: 0.552171, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [1000/23817 (4%)]\tLoss: 0.507694, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [2000/23817 (8%)]\tLoss: 0.426522, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [3000/23817 (13%)]\tLoss: 0.506503, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [4000/23817 (17%)]\tLoss: 0.558518, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [5000/23817 (21%)]\tLoss: 0.486013, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [6000/23817 (25%)]\tLoss: 0.542017, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [7000/23817 (29%)]\tLoss: 0.526630, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [8000/23817 (34%)]\tLoss: 0.538341, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [9000/23817 (38%)]\tLoss: 0.492364, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [10000/23817 (42%)]\tLoss: 0.504922, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [11000/23817 (46%)]\tLoss: 0.379054, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [12000/23817 (50%)]\tLoss: 0.511852, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [13000/23817 (55%)]\tLoss: 0.636282, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [14000/23817 (59%)]\tLoss: 0.464103, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [15000/23817 (63%)]\tLoss: 0.494314, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [16000/23817 (67%)]\tLoss: 0.501180, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [17000/23817 (71%)]\tLoss: 0.488978, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [18000/23817 (76%)]\tLoss: 0.506890, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [19000/23817 (80%)]\tLoss: 0.496796, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [20000/23817 (84%)]\tLoss: 0.490362, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [21000/23817 (88%)]\tLoss: 0.479741, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [22000/23817 (92%)]\tLoss: 0.558415, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [23000/23817 (97%)]\tLoss: 0.453770, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5075, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.7828151260504201\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/2884 (0%)]\tLoss: 0.693230, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [100/2884 (4%)]\tLoss: 0.648394, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [200/2884 (7%)]\tLoss: 0.752912, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [300/2884 (11%)]\tLoss: 0.588483, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [400/2884 (14%)]\tLoss: 0.674199, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [500/2884 (18%)]\tLoss: 0.598035, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [600/2884 (21%)]\tLoss: 0.438643, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [700/2884 (25%)]\tLoss: 0.606128, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [800/2884 (29%)]\tLoss: 0.713286, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [900/2884 (32%)]\tLoss: 0.841173, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1000/2884 (36%)]\tLoss: 0.534644, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1100/2884 (39%)]\tLoss: 0.866755, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1200/2884 (43%)]\tLoss: 0.885204, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1300/2884 (46%)]\tLoss: 0.662634, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1400/2884 (50%)]\tLoss: 0.617397, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1500/2884 (54%)]\tLoss: 0.744951, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1600/2884 (57%)]\tLoss: 0.586081, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1700/2884 (61%)]\tLoss: 0.613431, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1800/2884 (64%)]\tLoss: 0.685591, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1900/2884 (68%)]\tLoss: 0.672556, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2000/2884 (71%)]\tLoss: 0.739206, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2100/2884 (75%)]\tLoss: 0.677686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2200/2884 (79%)]\tLoss: 0.807299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2300/2884 (82%)]\tLoss: 0.703927, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2400/2884 (86%)]\tLoss: 0.985318, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2500/2884 (89%)]\tLoss: 0.642299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2600/2884 (93%)]\tLoss: 0.757568, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2700/2884 (96%)]\tLoss: 0.777526, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6969, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6075\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/23817 (0%)]\tLoss: 0.499732, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [1000/23817 (4%)]\tLoss: 0.542732, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [2000/23817 (8%)]\tLoss: 0.522787, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [3000/23817 (13%)]\tLoss: 0.578350, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [4000/23817 (17%)]\tLoss: 0.518044, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [5000/23817 (21%)]\tLoss: 0.538369, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [6000/23817 (25%)]\tLoss: 0.462044, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [7000/23817 (29%)]\tLoss: 0.442287, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [8000/23817 (34%)]\tLoss: 0.485818, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [9000/23817 (38%)]\tLoss: 0.443070, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [10000/23817 (42%)]\tLoss: 0.497598, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [11000/23817 (46%)]\tLoss: 0.493545, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [12000/23817 (50%)]\tLoss: 0.559766, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [13000/23817 (55%)]\tLoss: 0.534355, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [14000/23817 (59%)]\tLoss: 0.467236, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [15000/23817 (63%)]\tLoss: 0.537934, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [16000/23817 (67%)]\tLoss: 0.519855, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [17000/23817 (71%)]\tLoss: 0.493468, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [18000/23817 (76%)]\tLoss: 0.520082, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [19000/23817 (80%)]\tLoss: 0.550234, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [20000/23817 (84%)]\tLoss: 0.532632, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [21000/23817 (88%)]\tLoss: 0.476828, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [22000/23817 (92%)]\tLoss: 0.529447, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [23000/23817 (97%)]\tLoss: 0.593990, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5075, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.7827731092436975\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 8 [0/2884 (0%)]\tLoss: 0.693230, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [100/2884 (4%)]\tLoss: 0.648394, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [200/2884 (7%)]\tLoss: 0.752912, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [300/2884 (11%)]\tLoss: 0.588483, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [400/2884 (14%)]\tLoss: 0.674199, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [500/2884 (18%)]\tLoss: 0.598035, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [600/2884 (21%)]\tLoss: 0.438643, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [700/2884 (25%)]\tLoss: 0.606128, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [800/2884 (29%)]\tLoss: 0.713286, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [900/2884 (32%)]\tLoss: 0.841173, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1000/2884 (36%)]\tLoss: 0.534644, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1100/2884 (39%)]\tLoss: 0.866755, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1200/2884 (43%)]\tLoss: 0.885204, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1300/2884 (46%)]\tLoss: 0.662634, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1400/2884 (50%)]\tLoss: 0.617397, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1500/2884 (54%)]\tLoss: 0.744951, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1600/2884 (57%)]\tLoss: 0.586081, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1700/2884 (61%)]\tLoss: 0.613431, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1800/2884 (64%)]\tLoss: 0.685591, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1900/2884 (68%)]\tLoss: 0.672556, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2000/2884 (71%)]\tLoss: 0.739206, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2100/2884 (75%)]\tLoss: 0.677686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2200/2884 (79%)]\tLoss: 0.807299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2300/2884 (82%)]\tLoss: 0.703927, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2400/2884 (86%)]\tLoss: 0.985318, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2500/2884 (89%)]\tLoss: 0.642299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2600/2884 (93%)]\tLoss: 0.757568, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2700/2884 (96%)]\tLoss: 0.777526, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6969, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6075\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/23817 (0%)]\tLoss: 0.396719, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [1000/23817 (4%)]\tLoss: 0.516575, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [2000/23817 (8%)]\tLoss: 0.492910, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [3000/23817 (13%)]\tLoss: 0.469151, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [4000/23817 (17%)]\tLoss: 0.479325, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [5000/23817 (21%)]\tLoss: 0.512640, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [6000/23817 (25%)]\tLoss: 0.516675, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [7000/23817 (29%)]\tLoss: 0.534847, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [8000/23817 (34%)]\tLoss: 0.551495, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [9000/23817 (38%)]\tLoss: 0.514234, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [10000/23817 (42%)]\tLoss: 0.473323, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [11000/23817 (46%)]\tLoss: 0.510861, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [12000/23817 (50%)]\tLoss: 0.498394, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [13000/23817 (55%)]\tLoss: 0.448988, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [14000/23817 (59%)]\tLoss: 0.517886, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [15000/23817 (63%)]\tLoss: 0.535884, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [16000/23817 (67%)]\tLoss: 0.498266, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [17000/23817 (71%)]\tLoss: 0.557600, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [18000/23817 (76%)]\tLoss: 0.640624, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [19000/23817 (80%)]\tLoss: 0.588369, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [20000/23817 (84%)]\tLoss: 0.477624, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [21000/23817 (88%)]\tLoss: 0.391442, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [22000/23817 (92%)]\tLoss: 0.479401, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [23000/23817 (97%)]\tLoss: 0.524177, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5074, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status True\n",
            "Accuracy of the model 0.7828991596638656\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64), tensor(0.5074, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 9 [0/2884 (0%)]\tLoss: 0.693230, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [100/2884 (4%)]\tLoss: 0.648394, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [200/2884 (7%)]\tLoss: 0.752912, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [300/2884 (11%)]\tLoss: 0.588483, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [400/2884 (14%)]\tLoss: 0.674199, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [500/2884 (18%)]\tLoss: 0.598035, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [600/2884 (21%)]\tLoss: 0.438643, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [700/2884 (25%)]\tLoss: 0.606128, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [800/2884 (29%)]\tLoss: 0.713286, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [900/2884 (32%)]\tLoss: 0.841173, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1000/2884 (36%)]\tLoss: 0.534644, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1100/2884 (39%)]\tLoss: 0.866755, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1200/2884 (43%)]\tLoss: 0.885204, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1300/2884 (46%)]\tLoss: 0.662634, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1400/2884 (50%)]\tLoss: 0.617397, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1500/2884 (54%)]\tLoss: 0.744951, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1600/2884 (57%)]\tLoss: 0.586081, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1700/2884 (61%)]\tLoss: 0.613431, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1800/2884 (64%)]\tLoss: 0.685591, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1900/2884 (68%)]\tLoss: 0.672556, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2000/2884 (71%)]\tLoss: 0.739206, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2100/2884 (75%)]\tLoss: 0.677686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2200/2884 (79%)]\tLoss: 0.807299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2300/2884 (82%)]\tLoss: 0.703927, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2400/2884 (86%)]\tLoss: 0.985318, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2500/2884 (89%)]\tLoss: 0.642299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2600/2884 (93%)]\tLoss: 0.757568, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2700/2884 (96%)]\tLoss: 0.777526, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6969, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status True\n",
            "Accuracy of the model 0.6075\n",
            "losses: [tensor(0.6960, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64), tensor(0.6691, device='cuda:0', dtype=torch.float64), tensor(0.6675, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6836, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6989, device='cuda:0', dtype=torch.float64), tensor(0.6912, device='cuda:0', dtype=torch.float64), tensor(0.6677, device='cuda:0', dtype=torch.float64), tensor(0.6338, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5462, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64), tensor(0.5074, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 0.,  ..., 1., 0., 1.], device='cuda:0', dtype=torch.float64),\n",
              " RealDeclare(\n",
              "   (premise_embeddings): Embedding(100, 50)\n",
              "   (hypothesis_processor): SequenceProcessor(\n",
              "     (embeddings): Embedding(100, 50)\n",
              "     (normaliser): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (cool_lstm): LSTM(50, 64, batch_first=True, bidirectional=True)\n",
              "     (dropout): Dropout(p=0, inplace=False)\n",
              "   )\n",
              "   (premise_linear): Linear(in_features=10000, out_features=128, bias=True)\n",
              "   (linear_penultimate): Linear(in_features=100, out_features=36, bias=True)\n",
              "   (linear_almost_there): Linear(in_features=36, out_features=8, bias=True)\n",
              "   (dropout): Dropout(p=0.5, inplace=False)\n",
              "   (linear_final): Linear(in_features=8, out_features=1, bias=True)\n",
              " ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAEbCAYAAABUTQWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDEB+X/9eh/eO6Na5wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAEbCAYAAABUTQWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JetqyT7Imn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "declare_real_results, declare_predicted_ys =  run_model(models[\"real_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"real declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZugTi6AEHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_params = {\n",
        "    \"my_model\": Hyperparameters,\n",
        "    \"my_model_better\": Hyperparameters_b\n",
        "    \"sheena_model\": SheenaParameters,\n",
        "    \"sheena_model_better\": SheenaParameters_b\n",
        "    \"broke_declare\": DeclareParameters,\n",
        "    \"real_declare\": DeclareParameters\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWBCcIDk674v",
        "colab_type": "text"
      },
      "source": [
        "##VALIDATION LAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    some_results = []\n",
        "  \n",
        "    for per_avg_i in range(per_avg_amount):\n",
        "      predicted_ys, model = run_model(models[model_name], datasets[data_name], all_params[model_name])\n",
        "      full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "    processed_results.append(process_results(list_to_dict(some_results)))\n",
        "    print(processed_results)\n",
        "      #avg_results.append(get_avgs(some_results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for result in full_results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWgiTDHE2tP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}