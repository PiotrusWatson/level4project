{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4NbGwIC9-z",
        "colab_type": "text"
      },
      "source": [
        "##HAHA ITS TIME TO SPEND 5 HOURS DOWNLOADING THINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "5877bd4f-8671-4eee-a01d-6ba1de200229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:17:58--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  21.2MB/s    in 7.5s    \n",
            "\n",
            "2020-02-19 22:18:06 (12.1 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "ba4536af-62f9-4abf-f05f-5fd3be508bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:18:16--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-19 22:18:17--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-19 22:18:17--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.88MB/s    in 6m 30s  \n",
            "\n",
            "2020-02-19 22:24:48 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "72391a64-ed8c-447a-a772-4034391c36a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:25:16--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  2.15MB/s    in 2.2s    \n",
            "\n",
            "2020-02-19 22:25:20 (2.15 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "f1a7ed23-a2ed-4742-cb1e-2f288aa86d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "16d0cf98-2cca-466a-b63f-3a19e395b955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 22:25:29--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  2.44MB/s    in 2.2s    \n",
            "\n",
            "2020-02-19 22:25:32 (2.44 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "outputId": "b1bda0f5-23c9-4a1a-d556-8d823daf8006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJXKPPODFqp",
        "colab_type": "text"
      },
      "source": [
        "##LOOK AT ALL THIS CODE TO IMPORT DATA GOD THERE MUST BE SOMETHING WRONG WITH ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "outputId": "62da3783-e85d-40bf-d61e-c493a552f1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "5c7d1fc9-5eb7-4bf0-86be-784fffa1c873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "7f8d5580-0709-404c-aac5-dc3668b93ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "train_challenge, val_challenge = train_test_split(train_challenge, test_size=0.1, random_state=8)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39952</th>\n",
              "      <td>2109</td>\n",
              "      <td>A SCHOOLBOY who was almost killed when he was ...</td>\n",
              "      <td>Islamic State claims it executed American phot...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20359</th>\n",
              "      <td>1212</td>\n",
              "      <td>Tiger Woods divorced Swedish model Elin Nordeg...</td>\n",
              "      <td>Sources: Guns N' Roses Frontman Axl Rose Found...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11075</th>\n",
              "      <td>682</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>Wife and child of Islamic State leader Baghdad...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47329</th>\n",
              "      <td>2413</td>\n",
              "      <td>You may have a seen a story going around Faceb...</td>\n",
              "      <td>Armed U.S. drones spotted flying over Syria in...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15650</th>\n",
              "      <td>942</td>\n",
              "      <td>Ok, this is all still rumor, but there’s a cha...</td>\n",
              "      <td>UPDATE: BATMAN v SUPERMAN Batmobile Reportedly...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Body ID  ... Stance\n",
              "39952     2109  ...      2\n",
              "20359     1212  ...      2\n",
              "11075      682  ...      2\n",
              "47329     2413  ...      2\n",
              "15650      942  ...      1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "fake_news_challenge_mapping = {}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None, is_folding=False):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_text\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  \n",
        "  if is_folding:\n",
        "    results = []\n",
        "    folded = KFold(n_splits=10, shuffle=True)\n",
        "    splitted_object = folded.split(unique)\n",
        "    for train_result, test_result in splitted_object:\n",
        "      train_ilocs = unique.iloc[train_result][\"claim_text\"]\n",
        "      test_ilocs = unique.iloc[test_result][\"claim_text\"]\n",
        "      results.append((facts[facts[\"claim_text\"].isin(train_ilocs)], facts[facts[\"claim_text\"].isin(test_ilocs)]))\n",
        "\n",
        "    return results\n",
        "\n",
        "  train_unique, big_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "  val_unique, test_unique = train_test_split(big_unique, test_size=0.5, random_state=8)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_text\"].isin(test_unique[\"claim_text\"])]\n",
        "  val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "  train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "  return train_facts, test_facts, val_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts, val_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes, val_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "f35b916d-f66b-4145-fbde-629cc6d96e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>vice news we dont have a timeline on the decis...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>a schedule i narcotic along with heroin and ec...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>now do you think you were maybe talking just a...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>made to the new yorker that marijuana is no mo...</td>\n",
              "      <td>time.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jun_27_donald-trump_white-house-criticism...</td>\n",
              "      <td>obamacare signed law cbo estimated 23 million ...</td>\n",
              "      <td>donald trump</td>\n",
              "      <td>about the affordable health care act in its da...</td>\n",
              "      <td>eugeneweekly.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4849</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama to ban guns from 42 million social secur...</td>\n",
              "      <td>bearingarms.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama is looking to ban social security recipi...</td>\n",
              "      <td>rightwingnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4851</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>main navigation recent posts obama to ban 42 m...</td>\n",
              "      <td>downtrend.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4852</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>get news like this in your facebook news feed ...</td>\n",
              "      <td>thegatewaypundit.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>to prove everyone wrong yet again this time it...</td>\n",
              "      <td>zerohedge.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...        article_source\n",
              "187            1  ...            reason.com\n",
              "188            1  ...            reason.com\n",
              "189            1  ...               cnn.com\n",
              "190            1  ...              time.com\n",
              "526            1  ...      eugeneweekly.com\n",
              "...          ...  ...                   ...\n",
              "4849           0  ...       bearingarms.com\n",
              "4850           0  ...     rightwingnews.com\n",
              "4851           0  ...         downtrend.com\n",
              "4852           0  ...  thegatewaypundit.com\n",
              "4853           0  ...         zerohedge.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Beq65oTgcBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, val_loader, test_data, val_data, tokeniser):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.test_data = test_data\n",
        "    self.val_data = val_data\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4lYd83j-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\"claim_text\", \"article\"]\n",
        "big_labels = [\"claim_text\", \"article\", \"article_source\"]\n",
        "challenge_labels = [\"articleBody\", \"Headline\"]\n",
        "\n",
        "def get_list(panda, labels):\n",
        "  label_to_data = {}\n",
        "  for label in labels:\n",
        "    label_to_data[label] = panda[label]\n",
        "  \n",
        "  x_list = convert_to_lists(label_to_data)\n",
        "  if labels[0] == \"claim_text\":\n",
        "    y_list = panda[\"cred_label\"].tolist()\n",
        "  else:\n",
        "    y_list = panda[\"Stance\"].tolist()\n",
        "  return x_list, y_list\n",
        "\n",
        "def get_loader(x, y, vocab_size, max_length, batch_size, name, training=True, drop_last=True):\n",
        "  stuff = []\n",
        "  for key in x:\n",
        "    stuff.append(torch.from_numpy(x[key]).type(torch.LongTensor))\n",
        "  stuff.append(torch.from_numpy(y).type(torch.DoubleTensor))\n",
        "\n",
        "  tensorset = data_utils.TensorDataset(*stuff)\n",
        "  loader = data_utils.DataLoader(tensorset, batch_size=batch_size, drop_last=drop_last, shuffle=training)\n",
        "  loader.name = name\n",
        "  return loader\n",
        "\n",
        "  \n",
        "def get_dataset(train, test, val, vocab_size, max_length, batch_size, labels, name):\n",
        "  is_challenge = labels[0] == \"articleBody\"\n",
        "  train_list_x, train_list_y = get_list(train, labels)\n",
        "  if not is_challenge:\n",
        "    test_list_x, test_list_y = get_list(test, labels)\n",
        "  val_list_x, val_list_y = get_list(val, labels)\n",
        "\n",
        "\n",
        "\n",
        "  #tokenising various stuff, setting up numpy dictionaries :)\n",
        "  tokeniser = Tokeniser(train_list_x, vocab_size, max_length)\n",
        "  x_train = tokeniser.do_everything(train_list_x)\n",
        "  x_test = tokeniser.do_everything(test_list_x)\n",
        "  x_val = tokeniser.do_everything(val_list_x)\n",
        "  y_train = np.array(train_list_y, dtype=np.float32)\n",
        "  y_test = np.array(test_list_y, dtype=np.float32)\n",
        "  y_val = np.array(val_list_y, dtype=np.float32)\n",
        "  \n",
        "  #datasets/loaders\n",
        "  train_loader = get_loader(x_train, y_train, vocab_size, max_length, batch_size, name, True)\n",
        "  test_loader = get_loader(x_test, y_test, vocab_size, max_length, batch_size, name, False, drop_last=False)\n",
        "  val_loader = get_loader(x_val, y_val, vocab_size, max_length, batch_size, name, False)\n",
        "  return Dataset(train_loader, test_loader, val_loader,  y_test, y_val, tokeniser)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MMrKlu6_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "outputId": "1d6e3077-d611-4dcf-c5d4-dcea3d1c69d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "\n",
        "snopes_dataset = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "fact_dataset = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "big_snopes = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "big_fact = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39093\n",
            "33766\n",
            "44623\n",
            "37174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    \n",
        "    self.embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.normaliser = torch.nn.BatchNorm1d(self.embedding_size)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x[:, 0:self.hp.max_length])\n",
        "    #embedding = self.normaliser(embedding.transpose(1,2))\n",
        "    embedding = self.dropout(embedding)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    tanh_W_H = self.dropout(tanh_W_H)\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.premise_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_dropout = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    premise_factor = self.premise_dropout(premise_factor)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    hypothesis_factor = self.hypothesis_dropout(hypothesis_factor)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=50)\n",
        "    self.linear2 = torch.nn.Linear(50, 20)\n",
        "    self.dropout1 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(p=hp.inner_dropout)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "      x = self.dropout1(x)\n",
        "    else:\n",
        "      x = torch.relu(self.linear1(x.reshape(self.hp.batch_size, -1)))\n",
        "      x = self.dropout1(x)\n",
        "      x = torch.relu(self.linear2(x))\n",
        "      x = self.dropout2(x)\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    print(word_embeddings.shape)\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout),\n",
        "                     torch.nn.Dropout(p=hp.outer_dropout)]\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    self.dropouts[1](premise_embedding)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    self.dropouts[3](hypothesis_embedding)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    self.dropouts[4](factorised_mush)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention*premise_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWRHhOxCjFl",
        "colab_type": "text"
      },
      "source": [
        "##EVAL SUMMARY :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "    self.dropouts = [torch.nn.Dropout(p=hp.outer_dropout)]*5\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    processed_premise = self.dropouts[0](processed_premise)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    premise_embedding = self.dropouts[1](premise_embedding)\n",
        "    \n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    processed_hypothesis = self.dropouts[2](processed_hypothesis)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    hypothesis_embedding = self.dropouts[3](hypothesis_embedding)\n",
        "    combined = premise_embedding * hypothesis_embedding\n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    avg = self.dropouts[4](avg)\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(hp.max_length, word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(10000, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 36)\n",
        "    self.linear_almost_there = torch.nn.Linear(36, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.dropout = torch.nn.Dropout(p=hp.outer_dropout)\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    lengths = self.hp.max_length - (premise == 0).sum(dim=1)\n",
        "    lengths = lengths.repeat(50, 1).transpose(0,1)\n",
        "    summed_embeddings = torch.sum(embeddings, 1)\n",
        "    mean_embeddings = torch.unsqueeze(summed_embeddings / lengths, 1) #change to accurate size of lenfgth\n",
        "    flattened_embeddings = mean_embeddings.reshape(self.hp.batch_size, -1)\n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100]).reshape(self.hp.batch_size, -1)\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((flattened_embeddings.repeat(1, 100), added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    attention_weights = softmax(torch.tanh(self.premise_linear(main_embeddings)))#TODO: turn into row vector\n",
        "\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, attention_weights.unsqueeze(2))\n",
        "    \n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    new_lengths = lengths.repeat(1, 2)\n",
        "\n",
        "    avg = torch.sum(combined, 1)/new_lengths #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    smaller = self.dropout(smaller)\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    output = torch.sigmoid(self.linear_final(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def should_stop(losses, train_losses, limit):\n",
        "  shouldnt_stop = False \n",
        "  is_learning = False\n",
        "  print(\"losses:\",losses)\n",
        "  print(\"train_losses:\", train_losses)\n",
        "  \n",
        "  if len(losses) < limit:\n",
        "    return False\n",
        "  last = losses[-1]\n",
        "  last_train = train_losses[-1]\n",
        "\n",
        "  if limit == 2:\n",
        "    return last >= losses[-2]\n",
        "  \n",
        "  for i in range(-2,- limit-1,-1):\n",
        "    shouldnt_stop = (shouldnt_stop or last < losses[i])\n",
        "    last = losses[i]\n",
        "  return not shouldnt_stop\n",
        "\n",
        "\n",
        "def train(model=None, \n",
        "          dataset = None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "\n",
        "  is_binary = hp.num_classes == 1\n",
        "  is_validating = False\n",
        "  has_stopped = False\n",
        "  \n",
        "  if dataset.train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif dataset.train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "\n",
        "    for i in range(2):\n",
        "      \n",
        "      total_loss = 0\n",
        "      batch_count = 0\n",
        "      correct = 0\n",
        "      penal = 0\n",
        "\n",
        "      if is_validating:\n",
        "        model.train(False)\n",
        "        model.eval()\n",
        "        loader = dataset.val_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 1\n",
        "      elif has_stopped:\n",
        "        model.train(False)\n",
        "        loader = dataset.train_loader\n",
        "        torch.no_grad()\n",
        "        debug_amount = 10\n",
        "      else:\n",
        "        model.train(True)\n",
        "        loader = dataset.train_loader\n",
        "        torch.enable_grad()\n",
        "        debug_amount = 10\n",
        "      \n",
        "      for batch_index, train_data in enumerate(loader):\n",
        "      #setting everything up\n",
        "        model.hidden_state = model.reset_for_testing(train_data[0].shape[0])\n",
        "        train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "        predicted_y, attention = model(*train_data[:-1])\n",
        "        actual_y = train_data[-1]\n",
        "        squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "        if hp.C > 0:\n",
        "          attentionT = attention.transpose(1,2)\n",
        "          identity = torch.eye(attention.size(1))\n",
        "          identity = Variable(identity.unsqueeze(0).expand(loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "          penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "        if is_binary:\n",
        "          loss = loss_function(squeezed_y, actual_y.double())\n",
        "          loss += hp.C * penal/loader.batch_size\n",
        "          correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "        else:\n",
        "          loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/loader.batch_size)\n",
        "          correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if using_gradient_clipping:\n",
        "          torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      \n",
        "      #cleaning up regularisation\n",
        "        if hp.C > 0:\n",
        "          del(penal)\n",
        "          del(identity)\n",
        "          del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "        optimiser.zero_grad()\n",
        "        if not is_validating and not has_stopped:\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "        if hp.is_debug and batch_index % debug_amount == 0:\n",
        "          print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, ISvAL: {}, HasStopped: {}\".format(\n",
        "              epoch, batch_index * len(train_data[0]), len(loader.dataset),\n",
        "              100. * batch_index / len(loader), loss.item(), is_validating, has_stopped\n",
        "          ))\n",
        "\n",
        "      \n",
        "        batch_count += 1\n",
        "      \n",
        "        free_data(train_data)\n",
        "\n",
        "      print(\"Average loss is:\",total_loss/batch_count, \"while validation_status:\", is_validating, \"and stopping_status\", has_stopped)\n",
        "      correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "      accuracy = correct_but_numpy / float(batch_count * loader.batch_size)\n",
        "      print(\"Accuracy of the model\", accuracy)\n",
        "      if (not is_validating):\n",
        "        losses.append(total_loss/batch_count)\n",
        "        accuracies.append(accuracy)\n",
        "      else:\n",
        "        val_losses.append(total_loss/batch_count)\n",
        "        val_accuracies.append(accuracy)\n",
        "      if hp.use_early_stopping:\n",
        "        has_stopped = should_stop(val_losses,losses, hp.early_stopping)\n",
        "\n",
        "      is_validating = not is_validating\n",
        "  return losses, val_losses, accuracies, val_accuracies "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, val_losses, accuracies, val_accuracies, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  \n",
        "  plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Train Accuracy\")\n",
        "  plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_accuracies, scalex=True, scaley=True, label=\"Val Accuracy\")\n",
        "  plt.annotate(str(val_accuracies[-1]), xy=(epochs,val_accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Train Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.plot(range(1, epochs+1), val_losses,scalex=True, scaley=True, label=\"Val Loss\")\n",
        "  plt.annotate(str(val_losses[-1]), xy=(epochs,val_losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYfImtudskLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp, is_plot=True):\n",
        "  runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "  bce_loss = torch.nn.BCELoss()\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(runnable_model.parameters(), lr=hp.lr, weight_decay=hp.decay)\n",
        "  losses, val_losses, accuracies, val_accuracies = train(model=runnable_model,\n",
        "                       dataset = dataset,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=hp.grad_clip)\n",
        "  if is_plot:\n",
        "    plot_stuff(hp.epochs, losses,val_losses, accuracies, val_accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  check_loader = dataset.test_loader\n",
        "  predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         check_loader,\n",
        "         hp)\n",
        "  return predicted_ys, runnable_model\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc_full}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2DLo4OoUO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc_full))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oMDNooNrMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "##RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": fact_dataset,\n",
        "    \"snopes\": snopes_dataset\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 40\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 10\n",
        "  inner_dropout = 0.5\n",
        "  outer_dropout = 0.5\n",
        "  C = 0.3\n",
        "  decay = 0\n",
        "  is_debug = True\n",
        "  lr=0.00007\n",
        "  grad_clip = True\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wRSAvtABCT",
        "colab_type": "text"
      },
      "source": [
        "##CROSS VALIDATION ZONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "7792c43d-4409-47ca-cdb2-3498070cb062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "NUM_OF_AVGS = 5\n",
        "avgs = []\n",
        "\"\"\"\n",
        "for i in range(NUM_OF_AVGS):\n",
        "  \n",
        "  \n",
        "  \n",
        "  avgs.append(results)\n",
        "\n",
        "print(process_results(list_to_dict(avgs)))\n",
        "\"\"\"\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"snopes\"], Hyperparameters)\n",
        "predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([39093, 50])\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.648738, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 1.645830, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 1.652951, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 1.648662, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 1.642927, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 1.627337, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-cfcc529f0804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snopes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"politifact\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-183-1a3c21a92b33>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, dataset, hp, is_plot)\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        using_gradient_clipping=hp.grad_clip)\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_plot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplot_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-98fae21d5d5e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;31m#get y values - do forward pass and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpredicted_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mactual_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0msqueezed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-aeb8314fa739>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpremise_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremise_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_premise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothesis_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mhypothesis_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothesis_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-42611970826d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden_layer)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     return self.cool_lstm(embedding,\n\u001b[0;32m---> 26\u001b[0;31m                           hidden_layer)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JkXr8fcoee",
        "colab_type": "text"
      },
      "source": [
        "WOAH WERE DOIN SOME VALIDATION\n",
        "PLEASE VALIDATE ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu() ,datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zfw_4Acupe",
        "colab_type": "text"
      },
      "source": [
        "JUST TESTIN HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsngYK8wcvc8",
        "colab_type": "code",
        "outputId": "b6de5f47-bdb6-4a75-d8ae-a318e55cc82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "test_results= batch_wise_evaluate(text_model, datasets[\"politifact\"].test_loader, Hyperparameters)\n",
        "evaluation_summary(\"textual entailment test model\", test_results.cpu(), datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment test model\n",
            "Classifier 'textual entailment test model' has Acc=0.631 P=0.614 R=0.636 F1=0.606 AUC=0.614\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.417     0.648     0.507       836\n",
            "         1.0      0.811     0.624     0.705      2019\n",
            "\n",
            "    accuracy                          0.631      2855\n",
            "   macro avg      0.614     0.636     0.606      2855\n",
            "weighted avg      0.695     0.631     0.647      2855\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 542  759]\n",
            " [ 294 1260]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6137067120925691,\n",
              " 0.6361983406442623,\n",
              " 0.6311733800350263,\n",
              " 0.6062714155888395)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "##running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ngDR0NMc26u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 150\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 8\n",
        "  inner_dropout=0.5\n",
        "  outer_dropout=0.5\n",
        "  C = 0.3\n",
        "  is_debug = True\n",
        "  grad_clip = True\n",
        "  lr=0.00008\n",
        "  decay = 0\n",
        "  early_stopping = 3\n",
        "  use_early_stopping = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "outputId": "b5bafa9e-3f4b-4b20-cfe7-15beab8af57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters)\n",
        "sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"snopes\"], SheenaParameters)\n"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.635040, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/23817 (4%)]\tLoss: 1.636341, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/23817 (8%)]\tLoss: 1.631181, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/23817 (13%)]\tLoss: 1.634871, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/23817 (17%)]\tLoss: 1.635460, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/23817 (21%)]\tLoss: 1.636772, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/23817 (25%)]\tLoss: 1.635798, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/23817 (29%)]\tLoss: 1.634752, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/23817 (34%)]\tLoss: 1.638183, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/23817 (38%)]\tLoss: 1.635914, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/23817 (42%)]\tLoss: 1.632843, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/23817 (46%)]\tLoss: 1.637441, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/23817 (50%)]\tLoss: 1.636854, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [13000/23817 (55%)]\tLoss: 1.633266, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [14000/23817 (59%)]\tLoss: 1.634763, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [15000/23817 (63%)]\tLoss: 1.632688, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [16000/23817 (67%)]\tLoss: 1.637544, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [17000/23817 (71%)]\tLoss: 1.635597, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [18000/23817 (76%)]\tLoss: 1.633024, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [19000/23817 (80%)]\tLoss: 1.634824, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [20000/23817 (84%)]\tLoss: 1.632168, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [21000/23817 (88%)]\tLoss: 1.636209, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [22000/23817 (92%)]\tLoss: 1.636809, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [23000/23817 (97%)]\tLoss: 1.634424, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6355, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5021008403361344\n",
            "losses: []\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 1.641284, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/2884 (4%)]\tLoss: 1.632435, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/2884 (7%)]\tLoss: 1.638824, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/2884 (11%)]\tLoss: 1.626627, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/2884 (14%)]\tLoss: 1.632434, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/2884 (18%)]\tLoss: 1.637758, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/2884 (21%)]\tLoss: 1.633460, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/2884 (25%)]\tLoss: 1.635622, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/2884 (29%)]\tLoss: 1.628488, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/2884 (32%)]\tLoss: 1.635166, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/2884 (36%)]\tLoss: 1.632764, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/2884 (39%)]\tLoss: 1.636215, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/2884 (43%)]\tLoss: 1.638937, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/2884 (46%)]\tLoss: 1.648429, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/2884 (50%)]\tLoss: 1.633061, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1500/2884 (54%)]\tLoss: 1.626774, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1600/2884 (57%)]\tLoss: 1.626939, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1700/2884 (61%)]\tLoss: 1.632592, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1800/2884 (64%)]\tLoss: 1.640186, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1900/2884 (68%)]\tLoss: 1.642217, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2000/2884 (71%)]\tLoss: 1.626991, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2100/2884 (75%)]\tLoss: 1.627661, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2200/2884 (79%)]\tLoss: 1.638692, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2300/2884 (82%)]\tLoss: 1.638984, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2400/2884 (86%)]\tLoss: 1.638433, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2500/2884 (89%)]\tLoss: 1.635859, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2600/2884 (93%)]\tLoss: 1.635641, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2700/2884 (96%)]\tLoss: 1.630860, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6348, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5189285714285714\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.636651, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/23817 (4%)]\tLoss: 1.633358, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/23817 (8%)]\tLoss: 1.638255, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/23817 (13%)]\tLoss: 1.634498, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/23817 (17%)]\tLoss: 1.636002, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/23817 (21%)]\tLoss: 1.635561, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/23817 (25%)]\tLoss: 1.634065, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/23817 (29%)]\tLoss: 1.629972, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/23817 (34%)]\tLoss: 1.633744, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/23817 (38%)]\tLoss: 1.633089, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/23817 (42%)]\tLoss: 1.634578, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/23817 (46%)]\tLoss: 1.634139, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/23817 (50%)]\tLoss: 1.630401, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [13000/23817 (55%)]\tLoss: 1.630582, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [14000/23817 (59%)]\tLoss: 1.629382, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [15000/23817 (63%)]\tLoss: 1.630776, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [16000/23817 (67%)]\tLoss: 1.631100, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [17000/23817 (71%)]\tLoss: 1.627160, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [18000/23817 (76%)]\tLoss: 1.632034, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [19000/23817 (80%)]\tLoss: 1.625464, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [20000/23817 (84%)]\tLoss: 1.625012, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [21000/23817 (88%)]\tLoss: 1.630499, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [22000/23817 (92%)]\tLoss: 1.625821, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [23000/23817 (97%)]\tLoss: 1.622067, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6307, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5083193277310925\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 1.638939, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/2884 (4%)]\tLoss: 1.621468, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/2884 (7%)]\tLoss: 1.636602, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/2884 (11%)]\tLoss: 1.606406, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/2884 (14%)]\tLoss: 1.626596, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/2884 (18%)]\tLoss: 1.632722, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/2884 (21%)]\tLoss: 1.621644, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/2884 (25%)]\tLoss: 1.622519, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/2884 (29%)]\tLoss: 1.623501, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/2884 (32%)]\tLoss: 1.627979, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/2884 (36%)]\tLoss: 1.625716, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/2884 (39%)]\tLoss: 1.633605, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/2884 (43%)]\tLoss: 1.642378, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/2884 (46%)]\tLoss: 1.648070, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/2884 (50%)]\tLoss: 1.624220, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1500/2884 (54%)]\tLoss: 1.609888, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1600/2884 (57%)]\tLoss: 1.613779, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1700/2884 (61%)]\tLoss: 1.614554, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1800/2884 (64%)]\tLoss: 1.633055, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1900/2884 (68%)]\tLoss: 1.647397, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2000/2884 (71%)]\tLoss: 1.621902, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2100/2884 (75%)]\tLoss: 1.619372, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2200/2884 (79%)]\tLoss: 1.630878, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2300/2884 (82%)]\tLoss: 1.633473, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2400/2884 (86%)]\tLoss: 1.638913, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2500/2884 (89%)]\tLoss: 1.625944, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2600/2884 (93%)]\tLoss: 1.627474, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2700/2884 (96%)]\tLoss: 1.629203, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6278, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5389285714285714\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.620982, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/23817 (4%)]\tLoss: 1.621655, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/23817 (8%)]\tLoss: 1.621985, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/23817 (13%)]\tLoss: 1.624671, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/23817 (17%)]\tLoss: 1.631699, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/23817 (21%)]\tLoss: 1.614120, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/23817 (25%)]\tLoss: 1.621079, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/23817 (29%)]\tLoss: 1.614675, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/23817 (34%)]\tLoss: 1.605479, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/23817 (38%)]\tLoss: 1.603994, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/23817 (42%)]\tLoss: 1.620335, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/23817 (46%)]\tLoss: 1.605854, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/23817 (50%)]\tLoss: 1.585920, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [13000/23817 (55%)]\tLoss: 1.600728, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [14000/23817 (59%)]\tLoss: 1.574892, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [15000/23817 (63%)]\tLoss: 1.594805, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [16000/23817 (67%)]\tLoss: 1.591573, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [17000/23817 (71%)]\tLoss: 1.603540, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [18000/23817 (76%)]\tLoss: 1.576426, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [19000/23817 (80%)]\tLoss: 1.568383, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [20000/23817 (84%)]\tLoss: 1.578692, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [21000/23817 (88%)]\tLoss: 1.572075, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [22000/23817 (92%)]\tLoss: 1.567165, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [23000/23817 (97%)]\tLoss: 1.560076, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5987, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5865126050420169\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 1.582807, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/2884 (4%)]\tLoss: 1.576378, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/2884 (7%)]\tLoss: 1.599268, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/2884 (11%)]\tLoss: 1.509066, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/2884 (14%)]\tLoss: 1.635257, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/2884 (18%)]\tLoss: 1.539154, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/2884 (21%)]\tLoss: 1.552410, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/2884 (25%)]\tLoss: 1.515483, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/2884 (29%)]\tLoss: 1.666204, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/2884 (32%)]\tLoss: 1.581010, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/2884 (36%)]\tLoss: 1.576847, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/2884 (39%)]\tLoss: 1.649539, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/2884 (43%)]\tLoss: 1.682218, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/2884 (46%)]\tLoss: 1.539406, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/2884 (50%)]\tLoss: 1.570944, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1500/2884 (54%)]\tLoss: 1.548830, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1600/2884 (57%)]\tLoss: 1.598105, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1700/2884 (61%)]\tLoss: 1.502231, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1800/2884 (64%)]\tLoss: 1.579944, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1900/2884 (68%)]\tLoss: 1.640492, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2000/2884 (71%)]\tLoss: 1.655697, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2100/2884 (75%)]\tLoss: 1.601437, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2200/2884 (79%)]\tLoss: 1.585794, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2300/2884 (82%)]\tLoss: 1.579125, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2400/2884 (86%)]\tLoss: 1.670060, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2500/2884 (89%)]\tLoss: 1.535186, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2600/2884 (93%)]\tLoss: 1.548404, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2700/2884 (96%)]\tLoss: 1.663979, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5888, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5828571428571429\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23817 (0%)]\tLoss: 1.587121, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/23817 (4%)]\tLoss: 1.588727, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/23817 (8%)]\tLoss: 1.567444, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/23817 (13%)]\tLoss: 1.573748, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/23817 (17%)]\tLoss: 1.568931, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/23817 (21%)]\tLoss: 1.580476, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/23817 (25%)]\tLoss: 1.578202, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/23817 (29%)]\tLoss: 1.561384, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/23817 (34%)]\tLoss: 1.546142, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/23817 (38%)]\tLoss: 1.568659, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/23817 (42%)]\tLoss: 1.566201, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/23817 (46%)]\tLoss: 1.572170, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/23817 (50%)]\tLoss: 1.549314, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [13000/23817 (55%)]\tLoss: 1.520142, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [14000/23817 (59%)]\tLoss: 1.551738, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [15000/23817 (63%)]\tLoss: 1.574543, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [16000/23817 (67%)]\tLoss: 1.569224, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [17000/23817 (71%)]\tLoss: 1.586743, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [18000/23817 (76%)]\tLoss: 1.580768, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [19000/23817 (80%)]\tLoss: 1.523413, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [20000/23817 (84%)]\tLoss: 1.542307, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [21000/23817 (88%)]\tLoss: 1.542419, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [22000/23817 (92%)]\tLoss: 1.557896, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [23000/23817 (97%)]\tLoss: 1.577634, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5542, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.620546218487395\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/2884 (0%)]\tLoss: 1.543098, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/2884 (4%)]\tLoss: 1.522942, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/2884 (7%)]\tLoss: 1.578222, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/2884 (11%)]\tLoss: 1.487544, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/2884 (14%)]\tLoss: 1.644368, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/2884 (18%)]\tLoss: 1.548471, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/2884 (21%)]\tLoss: 1.510616, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/2884 (25%)]\tLoss: 1.482618, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/2884 (29%)]\tLoss: 1.660732, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/2884 (32%)]\tLoss: 1.581355, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/2884 (36%)]\tLoss: 1.535294, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/2884 (39%)]\tLoss: 1.642098, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/2884 (43%)]\tLoss: 1.689349, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/2884 (46%)]\tLoss: 1.480885, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/2884 (50%)]\tLoss: 1.543463, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1500/2884 (54%)]\tLoss: 1.521274, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1600/2884 (57%)]\tLoss: 1.581228, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1700/2884 (61%)]\tLoss: 1.483842, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1800/2884 (64%)]\tLoss: 1.551816, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1900/2884 (68%)]\tLoss: 1.644177, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2000/2884 (71%)]\tLoss: 1.641683, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2100/2884 (75%)]\tLoss: 1.597219, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2200/2884 (79%)]\tLoss: 1.573054, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2300/2884 (82%)]\tLoss: 1.557816, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2400/2884 (86%)]\tLoss: 1.673238, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2500/2884 (89%)]\tLoss: 1.494285, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2600/2884 (93%)]\tLoss: 1.530595, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [2700/2884 (96%)]\tLoss: 1.666189, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5703, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5917857142857142\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/23817 (0%)]\tLoss: 1.578452, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/23817 (4%)]\tLoss: 1.535462, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/23817 (8%)]\tLoss: 1.551169, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/23817 (13%)]\tLoss: 1.582961, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/23817 (17%)]\tLoss: 1.528820, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/23817 (21%)]\tLoss: 1.568256, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/23817 (25%)]\tLoss: 1.513899, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/23817 (29%)]\tLoss: 1.547069, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/23817 (34%)]\tLoss: 1.506572, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/23817 (38%)]\tLoss: 1.481163, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/23817 (42%)]\tLoss: 1.506012, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/23817 (46%)]\tLoss: 1.566818, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/23817 (50%)]\tLoss: 1.585438, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [13000/23817 (55%)]\tLoss: 1.520160, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [14000/23817 (59%)]\tLoss: 1.487221, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [15000/23817 (63%)]\tLoss: 1.491944, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [16000/23817 (67%)]\tLoss: 1.511607, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [17000/23817 (71%)]\tLoss: 1.480004, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [18000/23817 (76%)]\tLoss: 1.563763, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [19000/23817 (80%)]\tLoss: 1.522359, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [20000/23817 (84%)]\tLoss: 1.500537, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [21000/23817 (88%)]\tLoss: 1.547752, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [22000/23817 (92%)]\tLoss: 1.500022, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [23000/23817 (97%)]\tLoss: 1.493669, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5332, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6363865546218488\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/2884 (0%)]\tLoss: 1.516156, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/2884 (4%)]\tLoss: 1.514450, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/2884 (7%)]\tLoss: 1.553283, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/2884 (11%)]\tLoss: 1.475323, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/2884 (14%)]\tLoss: 1.650524, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/2884 (18%)]\tLoss: 1.522992, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/2884 (21%)]\tLoss: 1.466293, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/2884 (25%)]\tLoss: 1.482300, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/2884 (29%)]\tLoss: 1.648004, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/2884 (32%)]\tLoss: 1.568333, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/2884 (36%)]\tLoss: 1.509295, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/2884 (39%)]\tLoss: 1.651699, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/2884 (43%)]\tLoss: 1.716601, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/2884 (46%)]\tLoss: 1.429284, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/2884 (50%)]\tLoss: 1.500852, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1500/2884 (54%)]\tLoss: 1.519892, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1600/2884 (57%)]\tLoss: 1.604268, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1700/2884 (61%)]\tLoss: 1.466420, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1800/2884 (64%)]\tLoss: 1.552160, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1900/2884 (68%)]\tLoss: 1.640494, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2000/2884 (71%)]\tLoss: 1.637022, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2100/2884 (75%)]\tLoss: 1.630752, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2200/2884 (79%)]\tLoss: 1.566506, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2300/2884 (82%)]\tLoss: 1.566559, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2400/2884 (86%)]\tLoss: 1.703777, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2500/2884 (89%)]\tLoss: 1.468509, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2600/2884 (93%)]\tLoss: 1.528063, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [2700/2884 (96%)]\tLoss: 1.671318, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5629, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6103571428571428\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/23817 (0%)]\tLoss: 1.542900, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/23817 (4%)]\tLoss: 1.563470, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/23817 (8%)]\tLoss: 1.552792, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/23817 (13%)]\tLoss: 1.548550, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/23817 (17%)]\tLoss: 1.528937, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/23817 (21%)]\tLoss: 1.544291, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/23817 (25%)]\tLoss: 1.565416, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/23817 (29%)]\tLoss: 1.591209, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/23817 (34%)]\tLoss: 1.505473, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/23817 (38%)]\tLoss: 1.509200, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/23817 (42%)]\tLoss: 1.496564, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/23817 (46%)]\tLoss: 1.503639, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/23817 (50%)]\tLoss: 1.534607, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [13000/23817 (55%)]\tLoss: 1.502362, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [14000/23817 (59%)]\tLoss: 1.528499, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [15000/23817 (63%)]\tLoss: 1.542815, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [16000/23817 (67%)]\tLoss: 1.493244, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [17000/23817 (71%)]\tLoss: 1.486562, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [18000/23817 (76%)]\tLoss: 1.507813, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [19000/23817 (80%)]\tLoss: 1.578372, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [20000/23817 (84%)]\tLoss: 1.544139, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [21000/23817 (88%)]\tLoss: 1.531944, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [22000/23817 (92%)]\tLoss: 1.462667, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [23000/23817 (97%)]\tLoss: 1.548849, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5156, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6531932773109244\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/2884 (0%)]\tLoss: 1.517541, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/2884 (4%)]\tLoss: 1.474854, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/2884 (7%)]\tLoss: 1.569889, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/2884 (11%)]\tLoss: 1.463639, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/2884 (14%)]\tLoss: 1.633835, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/2884 (18%)]\tLoss: 1.541250, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/2884 (21%)]\tLoss: 1.427755, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/2884 (25%)]\tLoss: 1.479936, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/2884 (29%)]\tLoss: 1.639947, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/2884 (32%)]\tLoss: 1.585257, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/2884 (36%)]\tLoss: 1.504177, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/2884 (39%)]\tLoss: 1.668761, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/2884 (43%)]\tLoss: 1.708445, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/2884 (46%)]\tLoss: 1.408802, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/2884 (50%)]\tLoss: 1.484500, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1500/2884 (54%)]\tLoss: 1.537730, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1600/2884 (57%)]\tLoss: 1.618701, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1700/2884 (61%)]\tLoss: 1.450139, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1800/2884 (64%)]\tLoss: 1.562175, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1900/2884 (68%)]\tLoss: 1.619664, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2000/2884 (71%)]\tLoss: 1.645276, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2100/2884 (75%)]\tLoss: 1.670304, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2200/2884 (79%)]\tLoss: 1.581960, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2300/2884 (82%)]\tLoss: 1.550662, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2400/2884 (86%)]\tLoss: 1.688025, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2500/2884 (89%)]\tLoss: 1.457093, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2600/2884 (93%)]\tLoss: 1.519289, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [2700/2884 (96%)]\tLoss: 1.677897, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5603, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.615\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/23817 (0%)]\tLoss: 1.524993, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/23817 (4%)]\tLoss: 1.493712, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/23817 (8%)]\tLoss: 1.490648, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/23817 (13%)]\tLoss: 1.465686, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/23817 (17%)]\tLoss: 1.575511, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/23817 (21%)]\tLoss: 1.481722, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/23817 (25%)]\tLoss: 1.496617, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/23817 (29%)]\tLoss: 1.525703, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/23817 (34%)]\tLoss: 1.493689, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/23817 (38%)]\tLoss: 1.482394, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/23817 (42%)]\tLoss: 1.464542, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/23817 (46%)]\tLoss: 1.441012, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/23817 (50%)]\tLoss: 1.536699, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [13000/23817 (55%)]\tLoss: 1.466415, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [14000/23817 (59%)]\tLoss: 1.519974, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [15000/23817 (63%)]\tLoss: 1.468828, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [16000/23817 (67%)]\tLoss: 1.492384, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [17000/23817 (71%)]\tLoss: 1.524345, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [18000/23817 (76%)]\tLoss: 1.502909, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [19000/23817 (80%)]\tLoss: 1.473381, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [20000/23817 (84%)]\tLoss: 1.478107, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [21000/23817 (88%)]\tLoss: 1.491710, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [22000/23817 (92%)]\tLoss: 1.499304, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [23000/23817 (97%)]\tLoss: 1.515547, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.4970, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6671008403361345\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/2884 (0%)]\tLoss: 1.495033, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/2884 (4%)]\tLoss: 1.441044, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/2884 (7%)]\tLoss: 1.549486, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/2884 (11%)]\tLoss: 1.466627, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/2884 (14%)]\tLoss: 1.634473, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/2884 (18%)]\tLoss: 1.548737, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/2884 (21%)]\tLoss: 1.398990, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/2884 (25%)]\tLoss: 1.514108, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/2884 (29%)]\tLoss: 1.627218, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/2884 (32%)]\tLoss: 1.624032, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/2884 (36%)]\tLoss: 1.475735, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/2884 (39%)]\tLoss: 1.699050, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/2884 (43%)]\tLoss: 1.729660, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/2884 (46%)]\tLoss: 1.434673, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/2884 (50%)]\tLoss: 1.468608, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1500/2884 (54%)]\tLoss: 1.558070, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1600/2884 (57%)]\tLoss: 1.587002, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1700/2884 (61%)]\tLoss: 1.451245, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1800/2884 (64%)]\tLoss: 1.535527, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1900/2884 (68%)]\tLoss: 1.625401, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2000/2884 (71%)]\tLoss: 1.661654, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2100/2884 (75%)]\tLoss: 1.679966, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2200/2884 (79%)]\tLoss: 1.636259, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2300/2884 (82%)]\tLoss: 1.579775, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2400/2884 (86%)]\tLoss: 1.739586, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2500/2884 (89%)]\tLoss: 1.446624, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2600/2884 (93%)]\tLoss: 1.516229, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [2700/2884 (96%)]\tLoss: 1.646398, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5633, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6135714285714285\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64), tensor(1.5633, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/23817 (0%)]\tLoss: 1.524544, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [1000/23817 (4%)]\tLoss: 1.481832, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [2000/23817 (8%)]\tLoss: 1.468990, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [3000/23817 (13%)]\tLoss: 1.512928, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [4000/23817 (17%)]\tLoss: 1.477186, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [5000/23817 (21%)]\tLoss: 1.546695, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [6000/23817 (25%)]\tLoss: 1.444508, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [7000/23817 (29%)]\tLoss: 1.427380, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [8000/23817 (34%)]\tLoss: 1.488049, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [9000/23817 (38%)]\tLoss: 1.495904, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [10000/23817 (42%)]\tLoss: 1.511340, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [11000/23817 (46%)]\tLoss: 1.522071, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [12000/23817 (50%)]\tLoss: 1.515147, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [13000/23817 (55%)]\tLoss: 1.454190, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [14000/23817 (59%)]\tLoss: 1.490942, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [15000/23817 (63%)]\tLoss: 1.539240, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [16000/23817 (67%)]\tLoss: 1.492200, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [17000/23817 (71%)]\tLoss: 1.492442, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [18000/23817 (76%)]\tLoss: 1.451821, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [19000/23817 (80%)]\tLoss: 1.391969, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [20000/23817 (84%)]\tLoss: 1.437224, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [21000/23817 (88%)]\tLoss: 1.484536, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [22000/23817 (92%)]\tLoss: 1.510754, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [23000/23817 (97%)]\tLoss: 1.478962, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.4760, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6840756302521008\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64), tensor(1.5633, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64), tensor(1.4760, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/2884 (0%)]\tLoss: 1.475102, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [100/2884 (4%)]\tLoss: 1.431083, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [200/2884 (7%)]\tLoss: 1.559200, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [300/2884 (11%)]\tLoss: 1.480770, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [400/2884 (14%)]\tLoss: 1.634205, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [500/2884 (18%)]\tLoss: 1.590686, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [600/2884 (21%)]\tLoss: 1.368303, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [700/2884 (25%)]\tLoss: 1.479216, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [800/2884 (29%)]\tLoss: 1.612908, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [900/2884 (32%)]\tLoss: 1.683690, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1000/2884 (36%)]\tLoss: 1.447002, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1100/2884 (39%)]\tLoss: 1.677390, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1200/2884 (43%)]\tLoss: 1.717632, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1300/2884 (46%)]\tLoss: 1.364109, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1400/2884 (50%)]\tLoss: 1.461842, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1500/2884 (54%)]\tLoss: 1.618379, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1600/2884 (57%)]\tLoss: 1.624082, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1700/2884 (61%)]\tLoss: 1.478874, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1800/2884 (64%)]\tLoss: 1.554178, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1900/2884 (68%)]\tLoss: 1.557442, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2000/2884 (71%)]\tLoss: 1.660386, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2100/2884 (75%)]\tLoss: 1.748695, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2200/2884 (79%)]\tLoss: 1.642878, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2300/2884 (82%)]\tLoss: 1.573568, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2400/2884 (86%)]\tLoss: 1.770727, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2500/2884 (89%)]\tLoss: 1.423732, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2600/2884 (93%)]\tLoss: 1.514365, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [2700/2884 (96%)]\tLoss: 1.678647, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5653, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6089285714285714\n",
            "losses: [tensor(1.6348, device='cuda:0', dtype=torch.float64), tensor(1.6278, device='cuda:0', dtype=torch.float64), tensor(1.5888, device='cuda:0', dtype=torch.float64), tensor(1.5703, device='cuda:0', dtype=torch.float64), tensor(1.5629, device='cuda:0', dtype=torch.float64), tensor(1.5603, device='cuda:0', dtype=torch.float64), tensor(1.5633, device='cuda:0', dtype=torch.float64), tensor(1.5653, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6307, device='cuda:0', dtype=torch.float64), tensor(1.5987, device='cuda:0', dtype=torch.float64), tensor(1.5542, device='cuda:0', dtype=torch.float64), tensor(1.5332, device='cuda:0', dtype=torch.float64), tensor(1.5156, device='cuda:0', dtype=torch.float64), tensor(1.4970, device='cuda:0', dtype=torch.float64), tensor(1.4760, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 1.636064, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 1.635740, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 1.635554, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 1.635724, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 1.635692, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 1.635371, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/12119 (50%)]\tLoss: 1.635385, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/12119 (58%)]\tLoss: 1.635483, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/12119 (66%)]\tLoss: 1.636134, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/12119 (74%)]\tLoss: 1.635961, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/12119 (83%)]\tLoss: 1.636217, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/12119 (91%)]\tLoss: 1.635440, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/12119 (99%)]\tLoss: 1.636157, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6356, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.49950413223140494\n",
            "losses: []\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 1.635525, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/1507 (7%)]\tLoss: 1.636089, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/1507 (13%)]\tLoss: 1.636263, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/1507 (20%)]\tLoss: 1.635591, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/1507 (27%)]\tLoss: 1.636291, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/1507 (33%)]\tLoss: 1.635912, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/1507 (40%)]\tLoss: 1.636065, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/1507 (47%)]\tLoss: 1.635811, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/1507 (53%)]\tLoss: 1.636025, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/1507 (60%)]\tLoss: 1.635864, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/1507 (67%)]\tLoss: 1.635144, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/1507 (73%)]\tLoss: 1.635223, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/1507 (80%)]\tLoss: 1.635737, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/1507 (87%)]\tLoss: 1.636119, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/1507 (93%)]\tLoss: 1.635179, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6358, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.492\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 1.635339, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/12119 (8%)]\tLoss: 1.635906, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/12119 (17%)]\tLoss: 1.635473, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/12119 (25%)]\tLoss: 1.635295, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/12119 (33%)]\tLoss: 1.635977, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/12119 (41%)]\tLoss: 1.635797, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/12119 (50%)]\tLoss: 1.635579, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/12119 (58%)]\tLoss: 1.635222, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/12119 (66%)]\tLoss: 1.635440, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/12119 (74%)]\tLoss: 1.635627, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/12119 (83%)]\tLoss: 1.635152, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/12119 (91%)]\tLoss: 1.635089, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/12119 (99%)]\tLoss: 1.635788, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6355, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5108264462809917\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 1.636149, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/1507 (7%)]\tLoss: 1.636165, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/1507 (13%)]\tLoss: 1.635627, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/1507 (20%)]\tLoss: 1.635391, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/1507 (27%)]\tLoss: 1.636524, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/1507 (33%)]\tLoss: 1.635951, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/1507 (40%)]\tLoss: 1.635260, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/1507 (47%)]\tLoss: 1.636029, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/1507 (53%)]\tLoss: 1.636029, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/1507 (60%)]\tLoss: 1.635751, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/1507 (67%)]\tLoss: 1.634789, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/1507 (73%)]\tLoss: 1.634656, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/1507 (80%)]\tLoss: 1.635149, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/1507 (87%)]\tLoss: 1.634773, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/1507 (93%)]\tLoss: 1.634926, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6355, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.51\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 1.635578, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/12119 (8%)]\tLoss: 1.635271, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/12119 (17%)]\tLoss: 1.634756, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/12119 (25%)]\tLoss: 1.634731, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/12119 (33%)]\tLoss: 1.634836, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/12119 (41%)]\tLoss: 1.635210, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/12119 (50%)]\tLoss: 1.634758, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/12119 (58%)]\tLoss: 1.634702, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/12119 (66%)]\tLoss: 1.634403, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/12119 (74%)]\tLoss: 1.634168, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/12119 (83%)]\tLoss: 1.633776, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/12119 (91%)]\tLoss: 1.633058, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/12119 (99%)]\tLoss: 1.632936, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6347, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5132231404958678\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 1.634181, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/1507 (7%)]\tLoss: 1.634346, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/1507 (13%)]\tLoss: 1.634950, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/1507 (20%)]\tLoss: 1.634574, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/1507 (27%)]\tLoss: 1.635044, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/1507 (33%)]\tLoss: 1.633009, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/1507 (40%)]\tLoss: 1.634678, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/1507 (47%)]\tLoss: 1.634478, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/1507 (53%)]\tLoss: 1.634240, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/1507 (60%)]\tLoss: 1.634849, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/1507 (67%)]\tLoss: 1.633195, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/1507 (73%)]\tLoss: 1.634142, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/1507 (80%)]\tLoss: 1.633851, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/1507 (87%)]\tLoss: 1.634903, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/1507 (93%)]\tLoss: 1.633606, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6343, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5186666666666667\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/12119 (0%)]\tLoss: 1.633420, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/12119 (8%)]\tLoss: 1.633064, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/12119 (17%)]\tLoss: 1.632238, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/12119 (25%)]\tLoss: 1.631861, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/12119 (33%)]\tLoss: 1.631607, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/12119 (41%)]\tLoss: 1.631011, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/12119 (50%)]\tLoss: 1.630736, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/12119 (58%)]\tLoss: 1.629777, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/12119 (66%)]\tLoss: 1.628647, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/12119 (74%)]\tLoss: 1.628500, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/12119 (83%)]\tLoss: 1.628130, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/12119 (91%)]\tLoss: 1.627824, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/12119 (99%)]\tLoss: 1.625967, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6302, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5306611570247934\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/1507 (0%)]\tLoss: 1.634317, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/1507 (7%)]\tLoss: 1.634043, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/1507 (13%)]\tLoss: 1.634657, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/1507 (20%)]\tLoss: 1.633681, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/1507 (27%)]\tLoss: 1.637844, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/1507 (33%)]\tLoss: 1.631343, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/1507 (40%)]\tLoss: 1.632861, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/1507 (47%)]\tLoss: 1.634963, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/1507 (53%)]\tLoss: 1.636275, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/1507 (60%)]\tLoss: 1.632375, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/1507 (67%)]\tLoss: 1.626056, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/1507 (73%)]\tLoss: 1.627624, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/1507 (80%)]\tLoss: 1.629719, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/1507 (87%)]\tLoss: 1.627424, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/1507 (93%)]\tLoss: 1.627174, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6320, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.49733333333333335\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/12119 (0%)]\tLoss: 1.626647, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/12119 (8%)]\tLoss: 1.625217, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/12119 (17%)]\tLoss: 1.625997, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/12119 (25%)]\tLoss: 1.625190, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/12119 (33%)]\tLoss: 1.624165, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/12119 (41%)]\tLoss: 1.624708, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/12119 (50%)]\tLoss: 1.624027, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/12119 (58%)]\tLoss: 1.622714, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/12119 (66%)]\tLoss: 1.617902, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/12119 (74%)]\tLoss: 1.619067, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/12119 (83%)]\tLoss: 1.618708, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/12119 (91%)]\tLoss: 1.613890, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/12119 (99%)]\tLoss: 1.612713, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.6216, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5297520661157025\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/1507 (0%)]\tLoss: 1.623064, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/1507 (7%)]\tLoss: 1.620902, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/1507 (13%)]\tLoss: 1.626905, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/1507 (20%)]\tLoss: 1.623484, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/1507 (27%)]\tLoss: 1.626971, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/1507 (33%)]\tLoss: 1.618639, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/1507 (40%)]\tLoss: 1.624861, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/1507 (47%)]\tLoss: 1.624162, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/1507 (53%)]\tLoss: 1.622435, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/1507 (60%)]\tLoss: 1.623495, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/1507 (67%)]\tLoss: 1.618512, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/1507 (73%)]\tLoss: 1.621763, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/1507 (80%)]\tLoss: 1.621145, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/1507 (87%)]\tLoss: 1.620411, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/1507 (93%)]\tLoss: 1.621257, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.6225, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.56\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/12119 (0%)]\tLoss: 1.612772, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [1000/12119 (8%)]\tLoss: 1.607184, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [2000/12119 (17%)]\tLoss: 1.606167, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [3000/12119 (25%)]\tLoss: 1.602604, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [4000/12119 (33%)]\tLoss: 1.603064, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [5000/12119 (41%)]\tLoss: 1.598748, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [6000/12119 (50%)]\tLoss: 1.598401, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [7000/12119 (58%)]\tLoss: 1.591024, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [8000/12119 (66%)]\tLoss: 1.590137, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [9000/12119 (74%)]\tLoss: 1.582976, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [10000/12119 (83%)]\tLoss: 1.572386, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [11000/12119 (91%)]\tLoss: 1.581548, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 5 [12000/12119 (99%)]\tLoss: 1.585395, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5958, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5750413223140496\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/1507 (0%)]\tLoss: 1.601754, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [100/1507 (7%)]\tLoss: 1.584058, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [200/1507 (13%)]\tLoss: 1.630426, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [300/1507 (20%)]\tLoss: 1.601797, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [400/1507 (27%)]\tLoss: 1.640542, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [500/1507 (33%)]\tLoss: 1.581077, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [600/1507 (40%)]\tLoss: 1.608676, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [700/1507 (47%)]\tLoss: 1.613073, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [800/1507 (53%)]\tLoss: 1.607928, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [900/1507 (60%)]\tLoss: 1.587898, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1000/1507 (67%)]\tLoss: 1.581525, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1100/1507 (73%)]\tLoss: 1.579834, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1200/1507 (80%)]\tLoss: 1.578470, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1300/1507 (87%)]\tLoss: 1.576381, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 5 [1400/1507 (93%)]\tLoss: 1.582323, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5971, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5966666666666667\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/12119 (0%)]\tLoss: 1.569945, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [1000/12119 (8%)]\tLoss: 1.571486, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [2000/12119 (17%)]\tLoss: 1.567821, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [3000/12119 (25%)]\tLoss: 1.575180, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [4000/12119 (33%)]\tLoss: 1.565251, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [5000/12119 (41%)]\tLoss: 1.550450, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [6000/12119 (50%)]\tLoss: 1.562456, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [7000/12119 (58%)]\tLoss: 1.553054, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [8000/12119 (66%)]\tLoss: 1.547018, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [9000/12119 (74%)]\tLoss: 1.546705, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [10000/12119 (83%)]\tLoss: 1.546238, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [11000/12119 (91%)]\tLoss: 1.540497, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 6 [12000/12119 (99%)]\tLoss: 1.566315, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5572, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6447107438016529\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/1507 (0%)]\tLoss: 1.526728, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [100/1507 (7%)]\tLoss: 1.511388, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [200/1507 (13%)]\tLoss: 1.651672, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [300/1507 (20%)]\tLoss: 1.576657, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [400/1507 (27%)]\tLoss: 1.617264, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [500/1507 (33%)]\tLoss: 1.503359, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [600/1507 (40%)]\tLoss: 1.593126, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [700/1507 (47%)]\tLoss: 1.579722, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [800/1507 (53%)]\tLoss: 1.552129, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [900/1507 (60%)]\tLoss: 1.548448, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1000/1507 (67%)]\tLoss: 1.572192, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1100/1507 (73%)]\tLoss: 1.587989, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1200/1507 (80%)]\tLoss: 1.521265, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1300/1507 (87%)]\tLoss: 1.543996, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 6 [1400/1507 (93%)]\tLoss: 1.584008, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5647, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.632\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64), tensor(1.5647, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/12119 (0%)]\tLoss: 1.531014, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [1000/12119 (8%)]\tLoss: 1.533155, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [2000/12119 (17%)]\tLoss: 1.547877, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [3000/12119 (25%)]\tLoss: 1.512851, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [4000/12119 (33%)]\tLoss: 1.517492, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [5000/12119 (41%)]\tLoss: 1.552986, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [6000/12119 (50%)]\tLoss: 1.513241, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [7000/12119 (58%)]\tLoss: 1.529474, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [8000/12119 (66%)]\tLoss: 1.515810, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [9000/12119 (74%)]\tLoss: 1.521979, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [10000/12119 (83%)]\tLoss: 1.504715, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [11000/12119 (91%)]\tLoss: 1.485536, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 7 [12000/12119 (99%)]\tLoss: 1.538966, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(1.5238, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6713223140495868\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64), tensor(1.5647, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64), tensor(1.5238, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/1507 (0%)]\tLoss: 1.527882, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [100/1507 (7%)]\tLoss: 1.485338, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [200/1507 (13%)]\tLoss: 1.707263, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [300/1507 (20%)]\tLoss: 1.594489, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [400/1507 (27%)]\tLoss: 1.673835, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [500/1507 (33%)]\tLoss: 1.468600, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [600/1507 (40%)]\tLoss: 1.615026, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [700/1507 (47%)]\tLoss: 1.590585, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [800/1507 (53%)]\tLoss: 1.556100, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [900/1507 (60%)]\tLoss: 1.533276, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1000/1507 (67%)]\tLoss: 1.546457, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1100/1507 (73%)]\tLoss: 1.546198, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1200/1507 (80%)]\tLoss: 1.481157, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1300/1507 (87%)]\tLoss: 1.496178, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 7 [1400/1507 (93%)]\tLoss: 1.565735, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(1.5592, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6246666666666667\n",
            "losses: [tensor(1.6358, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6343, device='cuda:0', dtype=torch.float64), tensor(1.6320, device='cuda:0', dtype=torch.float64), tensor(1.6225, device='cuda:0', dtype=torch.float64), tensor(1.5971, device='cuda:0', dtype=torch.float64), tensor(1.5647, device='cuda:0', dtype=torch.float64), tensor(1.5592, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(1.6356, device='cuda:0', dtype=torch.float64), tensor(1.6355, device='cuda:0', dtype=torch.float64), tensor(1.6347, device='cuda:0', dtype=torch.float64), tensor(1.6302, device='cuda:0', dtype=torch.float64), tensor(1.6216, device='cuda:0', dtype=torch.float64), tensor(1.5958, device='cuda:0', dtype=torch.float64), tensor(1.5572, device='cuda:0', dtype=torch.float64), tensor(1.5238, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAogAAAEbCAYAAABZU3XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIIqIiIhIEQVEERERESmigCgiIiIiRf4/WI9udoA9bxsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"sheena model\", sheena_predicted_ys.cpu(), datasets[\"politifact\"].val_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "##OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFA1PNpzA8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeclareParameters:\n",
        "  lstm_hidden_size = 64\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = 100\n",
        "  num_classes = 1\n",
        "  inner_dropout=0\n",
        "  outer_dropout=0.5\n",
        "  epochs = 10\n",
        "  C = 0\n",
        "  is_debug = True\n",
        "  lr=0.0002\n",
        "  decay = 0.0001\n",
        "  grad_clip = False\n",
        "  early_stopping = 2\n",
        "  use_early_stopping = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ca842cc-57eb-49c1-e92c-0711f0a5f5d7"
      },
      "source": [
        "predicted_ys, text_model= run_model(models[\"real_declare\"], datasets[\"snopes\"], DeclareParameters)\n",
        "run_model(models[\"real_declare\"], datasets[\"politifact\"], DeclareParameters)"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/12119 (0%)]\tLoss: 0.692838, ISvAL: False, HasStopped: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1000/12119 (8%)]\tLoss: 0.701797, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/12119 (17%)]\tLoss: 0.711365, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/12119 (25%)]\tLoss: 0.699694, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/12119 (33%)]\tLoss: 0.701368, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/12119 (41%)]\tLoss: 0.681560, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/12119 (50%)]\tLoss: 0.713944, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/12119 (58%)]\tLoss: 0.690849, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/12119 (66%)]\tLoss: 0.704042, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/12119 (74%)]\tLoss: 0.698090, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/12119 (83%)]\tLoss: 0.694971, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/12119 (91%)]\tLoss: 0.701305, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/12119 (99%)]\tLoss: 0.691886, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6985, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5011570247933884\n",
            "losses: []\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/1507 (0%)]\tLoss: 0.753689, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/1507 (7%)]\tLoss: 0.745086, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/1507 (13%)]\tLoss: 0.721893, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/1507 (20%)]\tLoss: 0.717209, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/1507 (27%)]\tLoss: 0.771165, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/1507 (33%)]\tLoss: 0.721166, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/1507 (40%)]\tLoss: 0.720166, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/1507 (47%)]\tLoss: 0.736797, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/1507 (53%)]\tLoss: 0.759648, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/1507 (60%)]\tLoss: 0.706957, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/1507 (67%)]\tLoss: 0.634177, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/1507 (73%)]\tLoss: 0.632806, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/1507 (80%)]\tLoss: 0.675674, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/1507 (87%)]\tLoss: 0.667394, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/1507 (93%)]\tLoss: 0.630692, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.7063, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.442\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/12119 (0%)]\tLoss: 0.691796, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/12119 (8%)]\tLoss: 0.698550, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/12119 (17%)]\tLoss: 0.696274, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/12119 (25%)]\tLoss: 0.702883, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/12119 (33%)]\tLoss: 0.696642, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/12119 (41%)]\tLoss: 0.696271, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/12119 (50%)]\tLoss: 0.701055, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/12119 (58%)]\tLoss: 0.690991, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/12119 (66%)]\tLoss: 0.694551, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/12119 (74%)]\tLoss: 0.692402, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/12119 (83%)]\tLoss: 0.695335, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/12119 (91%)]\tLoss: 0.689985, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/12119 (99%)]\tLoss: 0.682561, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6942, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5036363636363637\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/1507 (0%)]\tLoss: 0.704799, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/1507 (7%)]\tLoss: 0.699215, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/1507 (13%)]\tLoss: 0.701164, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/1507 (20%)]\tLoss: 0.697886, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/1507 (27%)]\tLoss: 0.718117, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/1507 (33%)]\tLoss: 0.687204, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/1507 (40%)]\tLoss: 0.698452, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/1507 (47%)]\tLoss: 0.701989, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/1507 (53%)]\tLoss: 0.708462, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/1507 (60%)]\tLoss: 0.690765, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/1507 (67%)]\tLoss: 0.669415, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/1507 (73%)]\tLoss: 0.673779, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/1507 (80%)]\tLoss: 0.675878, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/1507 (87%)]\tLoss: 0.677209, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/1507 (93%)]\tLoss: 0.668903, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6915, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.4493333333333333\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/12119 (0%)]\tLoss: 0.688100, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/12119 (8%)]\tLoss: 0.681187, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/12119 (17%)]\tLoss: 0.679636, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/12119 (25%)]\tLoss: 0.671846, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/12119 (33%)]\tLoss: 0.682216, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/12119 (41%)]\tLoss: 0.673468, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/12119 (50%)]\tLoss: 0.672172, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/12119 (58%)]\tLoss: 0.658749, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/12119 (66%)]\tLoss: 0.656509, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/12119 (74%)]\tLoss: 0.640246, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/12119 (83%)]\tLoss: 0.650465, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/12119 (91%)]\tLoss: 0.674115, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/12119 (99%)]\tLoss: 0.663722, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6678, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.6236363636363637\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/1507 (0%)]\tLoss: 0.563020, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/1507 (7%)]\tLoss: 0.578236, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/1507 (13%)]\tLoss: 0.694565, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/1507 (20%)]\tLoss: 0.640916, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/1507 (27%)]\tLoss: 0.632764, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/1507 (33%)]\tLoss: 0.558550, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/1507 (40%)]\tLoss: 0.657392, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/1507 (47%)]\tLoss: 0.635182, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/1507 (53%)]\tLoss: 0.574805, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/1507 (60%)]\tLoss: 0.618824, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/1507 (67%)]\tLoss: 0.731862, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/1507 (73%)]\tLoss: 0.757941, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/1507 (80%)]\tLoss: 0.655525, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/1507 (87%)]\tLoss: 0.673396, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/1507 (93%)]\tLoss: 0.722411, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6464, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.63\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/12119 (0%)]\tLoss: 0.614480, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [1000/12119 (8%)]\tLoss: 0.643309, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [2000/12119 (17%)]\tLoss: 0.618890, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [3000/12119 (25%)]\tLoss: 0.617784, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [4000/12119 (33%)]\tLoss: 0.614370, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [5000/12119 (41%)]\tLoss: 0.585891, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [6000/12119 (50%)]\tLoss: 0.591626, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [7000/12119 (58%)]\tLoss: 0.578936, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [8000/12119 (66%)]\tLoss: 0.549435, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [9000/12119 (74%)]\tLoss: 0.646782, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [10000/12119 (83%)]\tLoss: 0.596738, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [11000/12119 (91%)]\tLoss: 0.590483, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 3 [12000/12119 (99%)]\tLoss: 0.554570, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6067, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.7315702479338843\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/1507 (0%)]\tLoss: 0.527608, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [100/1507 (7%)]\tLoss: 0.591649, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [200/1507 (13%)]\tLoss: 0.724923, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [300/1507 (20%)]\tLoss: 0.637331, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [400/1507 (27%)]\tLoss: 0.654734, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [500/1507 (33%)]\tLoss: 0.554952, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [600/1507 (40%)]\tLoss: 0.649750, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [700/1507 (47%)]\tLoss: 0.677400, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [800/1507 (53%)]\tLoss: 0.558875, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [900/1507 (60%)]\tLoss: 0.610333, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1000/1507 (67%)]\tLoss: 0.675955, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1100/1507 (73%)]\tLoss: 0.728621, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1200/1507 (80%)]\tLoss: 0.647098, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1300/1507 (87%)]\tLoss: 0.672224, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 3 [1400/1507 (93%)]\tLoss: 0.685008, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6398, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.622\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/12119 (0%)]\tLoss: 0.540863, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [1000/12119 (8%)]\tLoss: 0.582240, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [2000/12119 (17%)]\tLoss: 0.524375, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [3000/12119 (25%)]\tLoss: 0.588200, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [4000/12119 (33%)]\tLoss: 0.506259, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [5000/12119 (41%)]\tLoss: 0.565848, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [6000/12119 (50%)]\tLoss: 0.490911, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [7000/12119 (58%)]\tLoss: 0.606558, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [8000/12119 (66%)]\tLoss: 0.481058, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [9000/12119 (74%)]\tLoss: 0.519880, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [10000/12119 (83%)]\tLoss: 0.521593, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [11000/12119 (91%)]\tLoss: 0.549483, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 4 [12000/12119 (99%)]\tLoss: 0.626687, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.5455, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.7848760330578513\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/1507 (0%)]\tLoss: 0.562174, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [100/1507 (7%)]\tLoss: 0.657017, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [200/1507 (13%)]\tLoss: 0.743803, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [300/1507 (20%)]\tLoss: 0.658506, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [400/1507 (27%)]\tLoss: 0.750858, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [500/1507 (33%)]\tLoss: 0.591820, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [600/1507 (40%)]\tLoss: 0.655856, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [700/1507 (47%)]\tLoss: 0.729299, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [800/1507 (53%)]\tLoss: 0.629137, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [900/1507 (60%)]\tLoss: 0.617451, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1000/1507 (67%)]\tLoss: 0.593528, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1100/1507 (73%)]\tLoss: 0.631390, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1200/1507 (80%)]\tLoss: 0.619007, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1300/1507 (87%)]\tLoss: 0.626309, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 4 [1400/1507 (93%)]\tLoss: 0.600399, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6444, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.614\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/12119 (0%)]\tLoss: 0.501018, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [1000/12119 (8%)]\tLoss: 0.492082, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [2000/12119 (17%)]\tLoss: 0.483910, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [3000/12119 (25%)]\tLoss: 0.475669, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [4000/12119 (33%)]\tLoss: 0.504498, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [5000/12119 (41%)]\tLoss: 0.534954, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [6000/12119 (50%)]\tLoss: 0.501462, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [7000/12119 (58%)]\tLoss: 0.462260, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [8000/12119 (66%)]\tLoss: 0.477047, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [9000/12119 (74%)]\tLoss: 0.481846, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [10000/12119 (83%)]\tLoss: 0.449444, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [11000/12119 (91%)]\tLoss: 0.494188, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [12000/12119 (99%)]\tLoss: 0.479453, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4925, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.8341322314049586\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/1507 (0%)]\tLoss: 0.562174, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [100/1507 (7%)]\tLoss: 0.657017, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [200/1507 (13%)]\tLoss: 0.743803, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [300/1507 (20%)]\tLoss: 0.658506, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [400/1507 (27%)]\tLoss: 0.750858, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [500/1507 (33%)]\tLoss: 0.591820, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [600/1507 (40%)]\tLoss: 0.655856, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [700/1507 (47%)]\tLoss: 0.729299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [800/1507 (53%)]\tLoss: 0.629137, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [900/1507 (60%)]\tLoss: 0.617451, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1000/1507 (67%)]\tLoss: 0.593528, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1100/1507 (73%)]\tLoss: 0.631390, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1200/1507 (80%)]\tLoss: 0.619007, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1300/1507 (87%)]\tLoss: 0.626309, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1400/1507 (93%)]\tLoss: 0.600399, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6444, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.614\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/12119 (0%)]\tLoss: 0.501360, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [1000/12119 (8%)]\tLoss: 0.463826, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [2000/12119 (17%)]\tLoss: 0.497050, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [3000/12119 (25%)]\tLoss: 0.489372, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [4000/12119 (33%)]\tLoss: 0.499261, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [5000/12119 (41%)]\tLoss: 0.507189, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [6000/12119 (50%)]\tLoss: 0.505411, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [7000/12119 (58%)]\tLoss: 0.474380, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [8000/12119 (66%)]\tLoss: 0.506733, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [9000/12119 (74%)]\tLoss: 0.516540, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [10000/12119 (83%)]\tLoss: 0.538044, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [11000/12119 (91%)]\tLoss: 0.497639, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [12000/12119 (99%)]\tLoss: 0.447375, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4926, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.8341322314049586\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/1507 (0%)]\tLoss: 0.562174, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [100/1507 (7%)]\tLoss: 0.657017, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [200/1507 (13%)]\tLoss: 0.743803, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [300/1507 (20%)]\tLoss: 0.658506, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [400/1507 (27%)]\tLoss: 0.750858, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [500/1507 (33%)]\tLoss: 0.591820, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [600/1507 (40%)]\tLoss: 0.655856, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [700/1507 (47%)]\tLoss: 0.729299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [800/1507 (53%)]\tLoss: 0.629137, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [900/1507 (60%)]\tLoss: 0.617451, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1000/1507 (67%)]\tLoss: 0.593528, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1100/1507 (73%)]\tLoss: 0.631390, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1200/1507 (80%)]\tLoss: 0.619007, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1300/1507 (87%)]\tLoss: 0.626309, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1400/1507 (93%)]\tLoss: 0.600399, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6444, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.614\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/12119 (0%)]\tLoss: 0.505385, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [1000/12119 (8%)]\tLoss: 0.555822, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [2000/12119 (17%)]\tLoss: 0.502219, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [3000/12119 (25%)]\tLoss: 0.556975, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [4000/12119 (33%)]\tLoss: 0.464948, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [5000/12119 (41%)]\tLoss: 0.555486, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [6000/12119 (50%)]\tLoss: 0.471179, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [7000/12119 (58%)]\tLoss: 0.514336, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [8000/12119 (66%)]\tLoss: 0.455178, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [9000/12119 (74%)]\tLoss: 0.464276, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [10000/12119 (83%)]\tLoss: 0.496336, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [11000/12119 (91%)]\tLoss: 0.517181, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [12000/12119 (99%)]\tLoss: 0.514338, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4927, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.8340495867768595\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64), tensor(0.4927, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/1507 (0%)]\tLoss: 0.562174, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [100/1507 (7%)]\tLoss: 0.657017, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [200/1507 (13%)]\tLoss: 0.743803, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [300/1507 (20%)]\tLoss: 0.658506, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [400/1507 (27%)]\tLoss: 0.750858, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [500/1507 (33%)]\tLoss: 0.591820, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [600/1507 (40%)]\tLoss: 0.655856, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [700/1507 (47%)]\tLoss: 0.729299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [800/1507 (53%)]\tLoss: 0.629137, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [900/1507 (60%)]\tLoss: 0.617451, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1000/1507 (67%)]\tLoss: 0.593528, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1100/1507 (73%)]\tLoss: 0.631390, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1200/1507 (80%)]\tLoss: 0.619007, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1300/1507 (87%)]\tLoss: 0.626309, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1400/1507 (93%)]\tLoss: 0.600399, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6444, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.614\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64), tensor(0.4927, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/12119 (0%)]\tLoss: 0.440554, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [1000/12119 (8%)]\tLoss: 0.473661, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [2000/12119 (17%)]\tLoss: 0.530893, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [3000/12119 (25%)]\tLoss: 0.485361, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [4000/12119 (33%)]\tLoss: 0.486228, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [5000/12119 (41%)]\tLoss: 0.499238, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [6000/12119 (50%)]\tLoss: 0.528393, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [7000/12119 (58%)]\tLoss: 0.490256, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [8000/12119 (66%)]\tLoss: 0.485005, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [9000/12119 (74%)]\tLoss: 0.509945, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [10000/12119 (83%)]\tLoss: 0.486379, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [11000/12119 (91%)]\tLoss: 0.466160, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [12000/12119 (99%)]\tLoss: 0.515071, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4925, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.8342148760330579\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64), tensor(0.4927, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 8 [0/1507 (0%)]\tLoss: 0.562174, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [100/1507 (7%)]\tLoss: 0.657017, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [200/1507 (13%)]\tLoss: 0.743803, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [300/1507 (20%)]\tLoss: 0.658506, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [400/1507 (27%)]\tLoss: 0.750858, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [500/1507 (33%)]\tLoss: 0.591820, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [600/1507 (40%)]\tLoss: 0.655856, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [700/1507 (47%)]\tLoss: 0.729299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [800/1507 (53%)]\tLoss: 0.629137, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [900/1507 (60%)]\tLoss: 0.617451, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1000/1507 (67%)]\tLoss: 0.593528, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1100/1507 (73%)]\tLoss: 0.631390, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1200/1507 (80%)]\tLoss: 0.619007, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1300/1507 (87%)]\tLoss: 0.626309, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1400/1507 (93%)]\tLoss: 0.600399, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6444, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.614\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64), tensor(0.4927, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/12119 (0%)]\tLoss: 0.512069, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [1000/12119 (8%)]\tLoss: 0.470867, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [2000/12119 (17%)]\tLoss: 0.513878, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [3000/12119 (25%)]\tLoss: 0.470578, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [4000/12119 (33%)]\tLoss: 0.534094, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [5000/12119 (41%)]\tLoss: 0.490549, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [6000/12119 (50%)]\tLoss: 0.481327, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [7000/12119 (58%)]\tLoss: 0.546505, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [8000/12119 (66%)]\tLoss: 0.441965, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [9000/12119 (74%)]\tLoss: 0.507008, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [10000/12119 (83%)]\tLoss: 0.500964, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [11000/12119 (91%)]\tLoss: 0.491699, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [12000/12119 (99%)]\tLoss: 0.525834, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.4928, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.8340495867768595\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64), tensor(0.4927, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4928, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 9 [0/1507 (0%)]\tLoss: 0.562174, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [100/1507 (7%)]\tLoss: 0.657017, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [200/1507 (13%)]\tLoss: 0.743803, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [300/1507 (20%)]\tLoss: 0.658506, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [400/1507 (27%)]\tLoss: 0.750858, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [500/1507 (33%)]\tLoss: 0.591820, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [600/1507 (40%)]\tLoss: 0.655856, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [700/1507 (47%)]\tLoss: 0.729299, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [800/1507 (53%)]\tLoss: 0.629137, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [900/1507 (60%)]\tLoss: 0.617451, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1000/1507 (67%)]\tLoss: 0.593528, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1100/1507 (73%)]\tLoss: 0.631390, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1200/1507 (80%)]\tLoss: 0.619007, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1300/1507 (87%)]\tLoss: 0.626309, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1400/1507 (93%)]\tLoss: 0.600399, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6444, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.614\n",
            "losses: [tensor(0.7063, device='cuda:0', dtype=torch.float64), tensor(0.6915, device='cuda:0', dtype=torch.float64), tensor(0.6464, device='cuda:0', dtype=torch.float64), tensor(0.6398, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64), tensor(0.6444, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6985, device='cuda:0', dtype=torch.float64), tensor(0.6942, device='cuda:0', dtype=torch.float64), tensor(0.6678, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.5455, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4926, device='cuda:0', dtype=torch.float64), tensor(0.4927, device='cuda:0', dtype=torch.float64), tensor(0.4925, device='cuda:0', dtype=torch.float64), tensor(0.4928, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 0.687283, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [1000/23817 (4%)]\tLoss: 0.698788, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [2000/23817 (8%)]\tLoss: 0.691814, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [3000/23817 (13%)]\tLoss: 0.692267, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [4000/23817 (17%)]\tLoss: 0.693431, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [5000/23817 (21%)]\tLoss: 0.695820, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [6000/23817 (25%)]\tLoss: 0.691254, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [7000/23817 (29%)]\tLoss: 0.689308, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [8000/23817 (34%)]\tLoss: 0.689676, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [9000/23817 (38%)]\tLoss: 0.692085, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [10000/23817 (42%)]\tLoss: 0.694800, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [11000/23817 (46%)]\tLoss: 0.687850, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [12000/23817 (50%)]\tLoss: 0.689931, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [13000/23817 (55%)]\tLoss: 0.696568, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [14000/23817 (59%)]\tLoss: 0.685157, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [15000/23817 (63%)]\tLoss: 0.693675, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [16000/23817 (67%)]\tLoss: 0.688816, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [17000/23817 (71%)]\tLoss: 0.691502, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [18000/23817 (76%)]\tLoss: 0.682391, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [19000/23817 (80%)]\tLoss: 0.689433, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [20000/23817 (84%)]\tLoss: 0.683938, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [21000/23817 (88%)]\tLoss: 0.672428, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [22000/23817 (92%)]\tLoss: 0.678118, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 0 [23000/23817 (97%)]\tLoss: 0.691569, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6892, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.5325210084033614\n",
            "losses: []\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 0 [0/2884 (0%)]\tLoss: 0.646364, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [100/2884 (4%)]\tLoss: 0.681948, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [200/2884 (7%)]\tLoss: 0.669871, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [300/2884 (11%)]\tLoss: 0.669587, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [400/2884 (14%)]\tLoss: 0.700640, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [500/2884 (18%)]\tLoss: 0.637890, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [600/2884 (21%)]\tLoss: 0.651815, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [700/2884 (25%)]\tLoss: 0.658320, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [800/2884 (29%)]\tLoss: 0.739744, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [900/2884 (32%)]\tLoss: 0.676192, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1000/2884 (36%)]\tLoss: 0.647938, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1100/2884 (39%)]\tLoss: 0.700591, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1200/2884 (43%)]\tLoss: 0.695011, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1300/2884 (46%)]\tLoss: 0.596494, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1400/2884 (50%)]\tLoss: 0.675209, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1500/2884 (54%)]\tLoss: 0.692581, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1600/2884 (57%)]\tLoss: 0.709851, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1700/2884 (61%)]\tLoss: 0.660587, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1800/2884 (64%)]\tLoss: 0.645803, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [1900/2884 (68%)]\tLoss: 0.677105, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2000/2884 (71%)]\tLoss: 0.750887, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2100/2884 (75%)]\tLoss: 0.712109, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2200/2884 (79%)]\tLoss: 0.658909, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2300/2884 (82%)]\tLoss: 0.661665, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2400/2884 (86%)]\tLoss: 0.717108, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2500/2884 (89%)]\tLoss: 0.659527, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2600/2884 (93%)]\tLoss: 0.693832, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 0 [2700/2884 (96%)]\tLoss: 0.746543, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6798, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.5721428571428572\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 0.666997, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [1000/23817 (4%)]\tLoss: 0.669143, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [2000/23817 (8%)]\tLoss: 0.660681, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [3000/23817 (13%)]\tLoss: 0.664612, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [4000/23817 (17%)]\tLoss: 0.659751, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [5000/23817 (21%)]\tLoss: 0.617727, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [6000/23817 (25%)]\tLoss: 0.671473, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [7000/23817 (29%)]\tLoss: 0.665926, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [8000/23817 (34%)]\tLoss: 0.639983, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [9000/23817 (38%)]\tLoss: 0.641455, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [10000/23817 (42%)]\tLoss: 0.671159, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [11000/23817 (46%)]\tLoss: 0.687102, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [12000/23817 (50%)]\tLoss: 0.645183, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [13000/23817 (55%)]\tLoss: 0.650098, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [14000/23817 (59%)]\tLoss: 0.613556, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [15000/23817 (63%)]\tLoss: 0.663293, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [16000/23817 (67%)]\tLoss: 0.627096, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [17000/23817 (71%)]\tLoss: 0.641989, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [18000/23817 (76%)]\tLoss: 0.671135, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [19000/23817 (80%)]\tLoss: 0.594350, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [20000/23817 (84%)]\tLoss: 0.672631, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [21000/23817 (88%)]\tLoss: 0.679915, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [22000/23817 (92%)]\tLoss: 0.655086, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 1 [23000/23817 (97%)]\tLoss: 0.630561, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.6458, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status False\n",
            "Accuracy of the model 0.6500840336134454\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 1 [0/2884 (0%)]\tLoss: 0.655747, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [100/2884 (4%)]\tLoss: 0.628990, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [200/2884 (7%)]\tLoss: 0.695430, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [300/2884 (11%)]\tLoss: 0.582162, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [400/2884 (14%)]\tLoss: 0.705130, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [500/2884 (18%)]\tLoss: 0.626595, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [600/2884 (21%)]\tLoss: 0.543366, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [700/2884 (25%)]\tLoss: 0.653197, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [800/2884 (29%)]\tLoss: 0.692632, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [900/2884 (32%)]\tLoss: 0.682096, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1000/2884 (36%)]\tLoss: 0.566587, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1100/2884 (39%)]\tLoss: 0.733755, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1200/2884 (43%)]\tLoss: 0.738793, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1300/2884 (46%)]\tLoss: 0.517066, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1400/2884 (50%)]\tLoss: 0.599042, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1500/2884 (54%)]\tLoss: 0.646726, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1600/2884 (57%)]\tLoss: 0.630918, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1700/2884 (61%)]\tLoss: 0.636772, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1800/2884 (64%)]\tLoss: 0.611699, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [1900/2884 (68%)]\tLoss: 0.685314, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2000/2884 (71%)]\tLoss: 0.686072, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2100/2884 (75%)]\tLoss: 0.656981, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2200/2884 (79%)]\tLoss: 0.679394, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2300/2884 (82%)]\tLoss: 0.655759, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2400/2884 (86%)]\tLoss: 0.793501, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2500/2884 (89%)]\tLoss: 0.623014, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2600/2884 (93%)]\tLoss: 0.680158, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 1 [2700/2884 (96%)]\tLoss: 0.695780, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6537, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status False\n",
            "Accuracy of the model 0.6146428571428572\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 0.568811, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [1000/23817 (4%)]\tLoss: 0.595210, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [2000/23817 (8%)]\tLoss: 0.594063, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [3000/23817 (13%)]\tLoss: 0.641351, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [4000/23817 (17%)]\tLoss: 0.552956, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [5000/23817 (21%)]\tLoss: 0.561263, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [6000/23817 (25%)]\tLoss: 0.594860, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [7000/23817 (29%)]\tLoss: 0.607369, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [8000/23817 (34%)]\tLoss: 0.631395, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [9000/23817 (38%)]\tLoss: 0.595898, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [10000/23817 (42%)]\tLoss: 0.576248, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [11000/23817 (46%)]\tLoss: 0.608280, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [12000/23817 (50%)]\tLoss: 0.577556, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [13000/23817 (55%)]\tLoss: 0.555316, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [14000/23817 (59%)]\tLoss: 0.584457, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [15000/23817 (63%)]\tLoss: 0.546405, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [16000/23817 (67%)]\tLoss: 0.597060, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [17000/23817 (71%)]\tLoss: 0.538390, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [18000/23817 (76%)]\tLoss: 0.557099, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [19000/23817 (80%)]\tLoss: 0.592096, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [20000/23817 (84%)]\tLoss: 0.622488, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [21000/23817 (88%)]\tLoss: 0.558857, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [22000/23817 (92%)]\tLoss: 0.534434, ISvAL: False, HasStopped: False\n",
            "Train Epoch: 2 [23000/23817 (97%)]\tLoss: 0.686456, ISvAL: False, HasStopped: False\n",
            "Average loss is: tensor(0.5940, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.7049159663865546\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 2 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: False\n",
            "Train Epoch: 2 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: False\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(False, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23817 (0%)]\tLoss: 0.560486, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [1000/23817 (4%)]\tLoss: 0.569608, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [2000/23817 (8%)]\tLoss: 0.561916, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [3000/23817 (13%)]\tLoss: 0.537794, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [4000/23817 (17%)]\tLoss: 0.530663, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [5000/23817 (21%)]\tLoss: 0.599214, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [6000/23817 (25%)]\tLoss: 0.546193, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [7000/23817 (29%)]\tLoss: 0.485762, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [8000/23817 (34%)]\tLoss: 0.588079, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [9000/23817 (38%)]\tLoss: 0.572291, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [10000/23817 (42%)]\tLoss: 0.577478, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [11000/23817 (46%)]\tLoss: 0.570656, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [12000/23817 (50%)]\tLoss: 0.525230, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [13000/23817 (55%)]\tLoss: 0.593344, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [14000/23817 (59%)]\tLoss: 0.541208, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [15000/23817 (63%)]\tLoss: 0.552366, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [16000/23817 (67%)]\tLoss: 0.543422, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [17000/23817 (71%)]\tLoss: 0.549818, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [18000/23817 (76%)]\tLoss: 0.651876, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [19000/23817 (80%)]\tLoss: 0.503421, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [20000/23817 (84%)]\tLoss: 0.532767, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [21000/23817 (88%)]\tLoss: 0.526490, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [22000/23817 (92%)]\tLoss: 0.562276, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 3 [23000/23817 (97%)]\tLoss: 0.525727, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5557, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.7442016806722689\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 3 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 3 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 5\n",
            "Train Epoch: 4 [0/23817 (0%)]\tLoss: 0.558872, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [1000/23817 (4%)]\tLoss: 0.592470, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [2000/23817 (8%)]\tLoss: 0.524216, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [3000/23817 (13%)]\tLoss: 0.556889, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [4000/23817 (17%)]\tLoss: 0.548186, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [5000/23817 (21%)]\tLoss: 0.502883, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [6000/23817 (25%)]\tLoss: 0.565213, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [7000/23817 (29%)]\tLoss: 0.532901, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [8000/23817 (34%)]\tLoss: 0.541148, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [9000/23817 (38%)]\tLoss: 0.501140, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [10000/23817 (42%)]\tLoss: 0.607191, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [11000/23817 (46%)]\tLoss: 0.558458, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [12000/23817 (50%)]\tLoss: 0.580146, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [13000/23817 (55%)]\tLoss: 0.531737, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [14000/23817 (59%)]\tLoss: 0.599337, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [15000/23817 (63%)]\tLoss: 0.539235, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [16000/23817 (67%)]\tLoss: 0.540395, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [17000/23817 (71%)]\tLoss: 0.564271, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [18000/23817 (76%)]\tLoss: 0.580456, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [19000/23817 (80%)]\tLoss: 0.622815, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [20000/23817 (84%)]\tLoss: 0.636919, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [21000/23817 (88%)]\tLoss: 0.523728, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [22000/23817 (92%)]\tLoss: 0.511774, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 4 [23000/23817 (97%)]\tLoss: 0.542527, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5559, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.7440756302521009\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 4 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 4 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 6\n",
            "Train Epoch: 5 [0/23817 (0%)]\tLoss: 0.572187, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [1000/23817 (4%)]\tLoss: 0.571220, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [2000/23817 (8%)]\tLoss: 0.555376, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [3000/23817 (13%)]\tLoss: 0.560639, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [4000/23817 (17%)]\tLoss: 0.552654, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [5000/23817 (21%)]\tLoss: 0.608606, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [6000/23817 (25%)]\tLoss: 0.602212, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [7000/23817 (29%)]\tLoss: 0.578975, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [8000/23817 (34%)]\tLoss: 0.559295, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [9000/23817 (38%)]\tLoss: 0.573991, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [10000/23817 (42%)]\tLoss: 0.552303, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [11000/23817 (46%)]\tLoss: 0.582776, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [12000/23817 (50%)]\tLoss: 0.618566, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [13000/23817 (55%)]\tLoss: 0.522553, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [14000/23817 (59%)]\tLoss: 0.590846, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [15000/23817 (63%)]\tLoss: 0.627435, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [16000/23817 (67%)]\tLoss: 0.534825, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [17000/23817 (71%)]\tLoss: 0.575658, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [18000/23817 (76%)]\tLoss: 0.579984, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [19000/23817 (80%)]\tLoss: 0.594739, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [20000/23817 (84%)]\tLoss: 0.521697, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [21000/23817 (88%)]\tLoss: 0.557978, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [22000/23817 (92%)]\tLoss: 0.520962, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 5 [23000/23817 (97%)]\tLoss: 0.512861, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5559, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.7441176470588236\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 5 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 5 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 7\n",
            "Train Epoch: 6 [0/23817 (0%)]\tLoss: 0.578844, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [1000/23817 (4%)]\tLoss: 0.617785, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [2000/23817 (8%)]\tLoss: 0.568541, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [3000/23817 (13%)]\tLoss: 0.614923, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [4000/23817 (17%)]\tLoss: 0.573125, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [5000/23817 (21%)]\tLoss: 0.556944, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [6000/23817 (25%)]\tLoss: 0.571533, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [7000/23817 (29%)]\tLoss: 0.526830, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [8000/23817 (34%)]\tLoss: 0.567424, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [9000/23817 (38%)]\tLoss: 0.575984, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [10000/23817 (42%)]\tLoss: 0.562849, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [11000/23817 (46%)]\tLoss: 0.615454, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [12000/23817 (50%)]\tLoss: 0.606759, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [13000/23817 (55%)]\tLoss: 0.571344, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [14000/23817 (59%)]\tLoss: 0.615487, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [15000/23817 (63%)]\tLoss: 0.553620, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [16000/23817 (67%)]\tLoss: 0.580652, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [17000/23817 (71%)]\tLoss: 0.507329, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [18000/23817 (76%)]\tLoss: 0.548136, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [19000/23817 (80%)]\tLoss: 0.582149, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [20000/23817 (84%)]\tLoss: 0.512536, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [21000/23817 (88%)]\tLoss: 0.595555, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [22000/23817 (92%)]\tLoss: 0.553817, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 6 [23000/23817 (97%)]\tLoss: 0.558295, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5559, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.7440756302521009\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 6 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 6 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 8\n",
            "Train Epoch: 7 [0/23817 (0%)]\tLoss: 0.544417, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [1000/23817 (4%)]\tLoss: 0.521361, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [2000/23817 (8%)]\tLoss: 0.564753, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [3000/23817 (13%)]\tLoss: 0.550170, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [4000/23817 (17%)]\tLoss: 0.617630, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [5000/23817 (21%)]\tLoss: 0.560063, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [6000/23817 (25%)]\tLoss: 0.603731, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [7000/23817 (29%)]\tLoss: 0.627592, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [8000/23817 (34%)]\tLoss: 0.500403, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [9000/23817 (38%)]\tLoss: 0.543665, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [10000/23817 (42%)]\tLoss: 0.653942, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [11000/23817 (46%)]\tLoss: 0.560786, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [12000/23817 (50%)]\tLoss: 0.540802, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [13000/23817 (55%)]\tLoss: 0.611255, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [14000/23817 (59%)]\tLoss: 0.553891, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [15000/23817 (63%)]\tLoss: 0.574027, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [16000/23817 (67%)]\tLoss: 0.610479, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [17000/23817 (71%)]\tLoss: 0.525890, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [18000/23817 (76%)]\tLoss: 0.584068, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [19000/23817 (80%)]\tLoss: 0.518073, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [20000/23817 (84%)]\tLoss: 0.527474, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [21000/23817 (88%)]\tLoss: 0.591848, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [22000/23817 (92%)]\tLoss: 0.592745, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 7 [23000/23817 (97%)]\tLoss: 0.542924, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5559, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.7440756302521009\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 7 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 7 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 9\n",
            "Train Epoch: 8 [0/23817 (0%)]\tLoss: 0.604688, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [1000/23817 (4%)]\tLoss: 0.541680, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [2000/23817 (8%)]\tLoss: 0.591294, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [3000/23817 (13%)]\tLoss: 0.539388, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [4000/23817 (17%)]\tLoss: 0.571806, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [5000/23817 (21%)]\tLoss: 0.573301, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [6000/23817 (25%)]\tLoss: 0.551363, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [7000/23817 (29%)]\tLoss: 0.599340, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [8000/23817 (34%)]\tLoss: 0.503419, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [9000/23817 (38%)]\tLoss: 0.598251, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [10000/23817 (42%)]\tLoss: 0.548937, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [11000/23817 (46%)]\tLoss: 0.560776, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [12000/23817 (50%)]\tLoss: 0.553067, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [13000/23817 (55%)]\tLoss: 0.587979, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [14000/23817 (59%)]\tLoss: 0.609375, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [15000/23817 (63%)]\tLoss: 0.514701, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [16000/23817 (67%)]\tLoss: 0.552785, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [17000/23817 (71%)]\tLoss: 0.551895, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [18000/23817 (76%)]\tLoss: 0.539798, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [19000/23817 (80%)]\tLoss: 0.538099, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [20000/23817 (84%)]\tLoss: 0.498273, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [21000/23817 (88%)]\tLoss: 0.487935, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [22000/23817 (92%)]\tLoss: 0.532520, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 8 [23000/23817 (97%)]\tLoss: 0.548614, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5558, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.7442016806722689\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5558, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 8 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 8 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5558, device='cuda:0', dtype=torch.float64)]\n",
            "Running EPOCH: 10\n",
            "Train Epoch: 9 [0/23817 (0%)]\tLoss: 0.510145, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [1000/23817 (4%)]\tLoss: 0.540895, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [2000/23817 (8%)]\tLoss: 0.492565, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [3000/23817 (13%)]\tLoss: 0.544912, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [4000/23817 (17%)]\tLoss: 0.587246, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [5000/23817 (21%)]\tLoss: 0.570927, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [6000/23817 (25%)]\tLoss: 0.571482, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [7000/23817 (29%)]\tLoss: 0.586077, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [8000/23817 (34%)]\tLoss: 0.533473, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [9000/23817 (38%)]\tLoss: 0.536745, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [10000/23817 (42%)]\tLoss: 0.538030, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [11000/23817 (46%)]\tLoss: 0.587366, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [12000/23817 (50%)]\tLoss: 0.529889, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [13000/23817 (55%)]\tLoss: 0.522303, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [14000/23817 (59%)]\tLoss: 0.557294, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [15000/23817 (63%)]\tLoss: 0.551037, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [16000/23817 (67%)]\tLoss: 0.643841, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [17000/23817 (71%)]\tLoss: 0.579775, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [18000/23817 (76%)]\tLoss: 0.522828, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [19000/23817 (80%)]\tLoss: 0.523782, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [20000/23817 (84%)]\tLoss: 0.543075, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [21000/23817 (88%)]\tLoss: 0.576778, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [22000/23817 (92%)]\tLoss: 0.578220, ISvAL: False, HasStopped: True\n",
            "Train Epoch: 9 [23000/23817 (97%)]\tLoss: 0.535321, ISvAL: False, HasStopped: True\n",
            "Average loss is: tensor(0.5559, device='cuda:0', dtype=torch.float64) while validation_status: False and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.7441176470588236\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5558, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n",
            "Train Epoch: 9 [0/2884 (0%)]\tLoss: 0.683036, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [100/2884 (4%)]\tLoss: 0.607083, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [200/2884 (7%)]\tLoss: 0.756598, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [300/2884 (11%)]\tLoss: 0.512714, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [400/2884 (14%)]\tLoss: 0.670914, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [500/2884 (18%)]\tLoss: 0.636800, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [600/2884 (21%)]\tLoss: 0.483686, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [700/2884 (25%)]\tLoss: 0.625684, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [800/2884 (29%)]\tLoss: 0.647073, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [900/2884 (32%)]\tLoss: 0.716540, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1000/2884 (36%)]\tLoss: 0.547796, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1100/2884 (39%)]\tLoss: 0.772170, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1200/2884 (43%)]\tLoss: 0.828757, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1300/2884 (46%)]\tLoss: 0.566317, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1400/2884 (50%)]\tLoss: 0.575906, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1500/2884 (54%)]\tLoss: 0.617357, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1600/2884 (57%)]\tLoss: 0.569550, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1700/2884 (61%)]\tLoss: 0.636233, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1800/2884 (64%)]\tLoss: 0.640896, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [1900/2884 (68%)]\tLoss: 0.745723, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2000/2884 (71%)]\tLoss: 0.667854, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2100/2884 (75%)]\tLoss: 0.626667, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2200/2884 (79%)]\tLoss: 0.750066, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2300/2884 (82%)]\tLoss: 0.684373, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2400/2884 (86%)]\tLoss: 0.882902, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2500/2884 (89%)]\tLoss: 0.646220, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2600/2884 (93%)]\tLoss: 0.659359, ISvAL: True, HasStopped: True\n",
            "Train Epoch: 9 [2700/2884 (96%)]\tLoss: 0.694571, ISvAL: True, HasStopped: True\n",
            "Average loss is: tensor(0.6590, device='cuda:0', dtype=torch.float64) while validation_status: True and stopping_status tensor(True, device='cuda:0')\n",
            "Accuracy of the model 0.6164285714285714\n",
            "losses: [tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6537, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64), tensor(0.6590, device='cuda:0', dtype=torch.float64)]\n",
            "train_losses: [tensor(0.6892, device='cuda:0', dtype=torch.float64), tensor(0.6458, device='cuda:0', dtype=torch.float64), tensor(0.5940, device='cuda:0', dtype=torch.float64), tensor(0.5557, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64), tensor(0.5558, device='cuda:0', dtype=torch.float64), tensor(0.5559, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 0., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float64),\n",
              " RealDeclare(\n",
              "   (premise_embeddings): Embedding(100, 50)\n",
              "   (hypothesis_processor): SequenceProcessor(\n",
              "     (embeddings): Embedding(100, 50)\n",
              "     (normaliser): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (cool_lstm): LSTM(50, 64, batch_first=True, bidirectional=True)\n",
              "     (dropout): Dropout(p=0, inplace=False)\n",
              "   )\n",
              "   (premise_linear): Linear(in_features=10000, out_features=128, bias=True)\n",
              "   (linear_penultimate): Linear(in_features=100, out_features=36, bias=True)\n",
              "   (linear_almost_there): Linear(in_features=36, out_features=8, bias=True)\n",
              "   (dropout): Dropout(p=0.5, inplace=False)\n",
              "   (linear_final): Linear(in_features=8, out_features=1, bias=True)\n",
              " ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo0AAAEbCAYAAAC/er6OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSaOIiIiIFEtJo4iIiIgU6/8BOPIBKGL4TP8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAEbCAYAAABUTQWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXY2B8QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JetqyT7Imn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "declare_real_results, declare_predicted_ys =  run_model(models[\"real_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"real declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDZugTi6AEHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_params = {\n",
        "    \"my_model\": Hyperparameters,\n",
        "    \"sheena_model\": SheenaParameters,\n",
        "    \"broke_declare\": DeclareParameters,\n",
        "    \"real_declare\": DeclareParameters\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWBCcIDk674v",
        "colab_type": "text"
      },
      "source": [
        "##VALIDATION LAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    some_results = []\n",
        "  \n",
        "    for per_avg_i in range(per_avg_amount):\n",
        "      predicted_ys, model = run_model(models[model_name], datasets[data_name], all_params[model_name])\n",
        "      full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "    processed_results.append(process_results(list_to_dict(some_results)))\n",
        "    print(processed_results)\n",
        "      #avg_results.append(get_avgs(some_results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for result in full_results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWgiTDHE2tP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}