{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "4b8d15e0-7789-4b17-d0b2-7fb5c2e67e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-375658c45ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip snli_1.0.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "outputId": "7f85b233-8af1-4e0e-d6e3-37b85b448b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "7385ddaf-82db-4d26-819e-458275cd19af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "56180a49-3a6b-4b40-8032-c95437084d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed Spider Man after spider burro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ... Stance\n",
              "0        0  ...      2\n",
              "1        0  ...      2\n",
              "2        0  ...      2\n",
              "3        0  ...      2\n",
              "4        0  ...      2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "outputId": "81494eaa-0882-4818-d614-514345b60052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_id\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  train_unique, test_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_id\"].isin(test_unique[\"claim_id\"])]\n",
        "  train_facts = facts[facts[\"claim_id\"].isin(train_unique[\"claim_id\"])]\n",
        "  return train_facts, test_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n",
        "\n",
        "train_facts.head(50)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>for firms moving overseas in order to create a...</td>\n",
              "      <td>foxnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>get a tax break specifically by outsourcing jo...</td>\n",
              "      <td>newslines.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>confusing clashes over taxes in wednesday s pr...</td>\n",
              "      <td>wsj.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>support on this bill in a time of tight budget...</td>\n",
              "      <td>senate.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>tax a lower rate for american manufacturing an...</td>\n",
              "      <td>archives.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>jobs in america and reduce the federal budget ...</td>\n",
              "      <td>senate.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>obamas debate claims on tax writeoffs for offs...</td>\n",
              "      <td>huffingtonpost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>joseph crowley the bronx one of nine chief dep...</td>\n",
              "      <td>house.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the senate budget committee bernie issued an o...</td>\n",
              "      <td>feelthebern.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>other than becoming a fulltime radical and tha...</td>\n",
              "      <td>wikipedia.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the years i ve repeatedly made clear that we w...</td>\n",
              "      <td>archives.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>bin laden declares war on musharraf in new aud...</td>\n",
              "      <td>foxnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>dear reader please upgrade to the latest versi...</td>\n",
              "      <td>dawn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>posted by joe on sep 20 2007 in at tmv 6 comme...</td>\n",
              "      <td>themoderatevoice.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>accessibility links saturday 16 december 2017 ...</td>\n",
              "      <td>telegraph.co.uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>news middle east tuesday 19 december 2017 bin ...</td>\n",
              "      <td>independent.ie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>that we would take action within pakistan if w...</td>\n",
              "      <td>go.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>pakistan at that very moment the man who had d...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>has won most popular conservatism cant survive...</td>\n",
              "      <td>theatlantic.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>after 911 why didnt the us declare war on paki...</td>\n",
              "      <td>quora.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>behind this week s attacks on the united state...</td>\n",
              "      <td>badgerherald.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the saudi dissident opposed to washington s co...</td>\n",
              "      <td>independent.co.uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>to enjoy the full mail guardian online experie...</td>\n",
              "      <td>mg.co.za</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the saudi arabian peninsula following the gulf...</td>\n",
              "      <td>pbs.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>col john discusses the impact of osama bin lad...</td>\n",
              "      <td>usip.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>osama bin laden declares war on pakistan most ...</td>\n",
              "      <td>theatlantic.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>his supporters questioned whether he had actua...</td>\n",
              "      <td>nytimes.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>years i have repeatedly made clear that we wou...</td>\n",
              "      <td>huffingtonpost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>afraid of the american threats against me he s...</td>\n",
              "      <td>theguardian.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>news middle east tuesday 19 december 2017 bin ...</td>\n",
              "      <td>independent.ie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>this copy is for your personal noncommercial u...</td>\n",
              "      <td>thestar.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>things must come to pass but the end is not ye...</td>\n",
              "      <td>rlhymersjr.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1</td>\n",
              "      <td>2011_may_05_barack-obama_barack-obama-says-bin...</td>\n",
              "      <td>bin laden declared war pakistan</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>we hope youll especially enjoy fba items quali...</td>\n",
              "      <td>amazon.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>requested increase eight senators courageously...</td>\n",
              "      <td>delawareonline.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>trending newsletter trump s plan for increased...</td>\n",
              "      <td>panampost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>state of the union address the president most ...</td>\n",
              "      <td>duke.edu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>us spends more on military than the next eight...</td>\n",
              "      <td>wordpress.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>according to energy information administration...</td>\n",
              "      <td>factcheck.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>factors that frustrate direct budget compariso...</td>\n",
              "      <td>heritage.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>rachel and thomas over at the heritage foundat...</td>\n",
              "      <td>nationalreview.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>fearless muckraking since 1993 profits for the...</td>\n",
              "      <td>counterpunch.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>over the years even as the number of soldiers ...</td>\n",
              "      <td>washingtonpost.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>and the resources they need to confront the th...</td>\n",
              "      <td>cnbc.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>that this commonplace is simplistic inaccurate...</td>\n",
              "      <td>heritage.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>the most powerful nation on earth period he sa...</td>\n",
              "      <td>nationalreview.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>search form why does donald trump insist on mo...</td>\n",
              "      <td>commondreams.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>why does donald trump insist on more military ...</td>\n",
              "      <td>nationofchange.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>2016_jan_13_barack-obama_obama-us-spends-more-...</td>\n",
              "      <td>spend military next eight nations combined</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>doubt that it still matters massively who occu...</td>\n",
              "      <td>theguardian.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>2013_jan_11_ileana-ros-lehtinen_ros-lehtinen-s...</td>\n",
              "      <td>says chuck hagel opposed sanctions iran</td>\n",
              "      <td>ileana ros-lehtinen</td>\n",
              "      <td>the president s war cabinet chuck hagel s misg...</td>\n",
              "      <td>algemeiner.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1</td>\n",
              "      <td>2013_jan_11_ileana-ros-lehtinen_ros-lehtinen-s...</td>\n",
              "      <td>says chuck hagel opposed sanctions iran</td>\n",
              "      <td>ileana ros-lehtinen</td>\n",
              "      <td>of note this week because the president appear...</td>\n",
              "      <td>cfr.org</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    cred_label  ...        article_source\n",
              "0            1  ...           foxnews.com\n",
              "1            1  ...         newslines.org\n",
              "2            1  ...               wsj.com\n",
              "3            1  ...            senate.gov\n",
              "4            1  ...          archives.gov\n",
              "5            1  ...            senate.gov\n",
              "6            1  ...    huffingtonpost.com\n",
              "7            1  ...             house.gov\n",
              "8            1  ...       feelthebern.org\n",
              "9            1  ...         wikipedia.org\n",
              "10           1  ...          archives.gov\n",
              "11           1  ...           foxnews.com\n",
              "12           1  ...              dawn.com\n",
              "13           1  ...  themoderatevoice.com\n",
              "14           1  ...       telegraph.co.uk\n",
              "15           1  ...        independent.ie\n",
              "16           1  ...                go.com\n",
              "17           1  ...               cnn.com\n",
              "18           1  ...       theatlantic.com\n",
              "19           1  ...             quora.com\n",
              "20           1  ...      badgerherald.com\n",
              "21           1  ...     independent.co.uk\n",
              "22           1  ...              mg.co.za\n",
              "23           1  ...               pbs.org\n",
              "24           1  ...              usip.org\n",
              "25           1  ...       theatlantic.com\n",
              "26           1  ...           nytimes.com\n",
              "27           1  ...    huffingtonpost.com\n",
              "28           1  ...       theguardian.com\n",
              "29           1  ...        independent.ie\n",
              "30           1  ...           thestar.com\n",
              "31           1  ...        rlhymersjr.com\n",
              "32           1  ...            amazon.com\n",
              "33           1  ...    delawareonline.com\n",
              "34           1  ...         panampost.com\n",
              "35           1  ...              duke.edu\n",
              "36           1  ...         wordpress.com\n",
              "37           1  ...         factcheck.org\n",
              "38           1  ...          heritage.org\n",
              "39           1  ...    nationalreview.com\n",
              "40           1  ...      counterpunch.org\n",
              "41           1  ...    washingtonpost.com\n",
              "42           1  ...              cnbc.com\n",
              "43           1  ...          heritage.org\n",
              "44           1  ...    nationalreview.com\n",
              "45           1  ...      commondreams.org\n",
              "46           1  ...    nationofchange.org\n",
              "47           1  ...       theguardian.com\n",
              "48           1  ...        algemeiner.com\n",
              "49           1  ...               cfr.org\n",
              "\n",
              "[50 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 256\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_fact_list = convert_to_lists({\"claim_text\": train_facts[\"claim_text\"], \n",
        "                   \"claim_source\": train_facts[\"claim_source\"],\n",
        "                   \"article\": train_facts[\"article\"],\n",
        "                   \"article_source\": train_facts[\"article_source\"]})\n",
        "y_train_fact_list = train_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_fact_list = convert_to_lists({\"claim_text\": test_facts[\"claim_text\"], \n",
        "                   \"claim_source\": test_facts[\"claim_source\"],\n",
        "                   \"article\": test_facts[\"article\"],\n",
        "                   \"article_source\": test_facts[\"article_source\"]})\n",
        "y_test_fact_list = test_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "x_train_snopes_list = convert_to_lists({\"claim_text\": train_snopes[\"claim_text\"],\n",
        "                   \"article\": train_snopes[\"article\"],\n",
        "                   \"article_source\": train_snopes[\"article_source\"]})\n",
        "x_test_snopes_list = convert_to_lists({\"claim_text\": test_snopes[\"claim_text\"],\n",
        "                   \"article\": test_snopes[\"article\"],\n",
        "                   \"article_source\": test_snopes[\"article_source\"]})\n",
        "y_train_snopes_list = train_snopes[\"cred_label\"].tolist()\n",
        "y_test_snopes_list = test_snopes[\"cred_label\"].tolist()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "fact_tokeniser = Tokeniser(x_train_fact_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "snopes_tokeniser = Tokeniser(x_train_snopes_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_fact_train = fact_tokeniser.do_everything(x_train_fact_list)\n",
        "x_fact_test = fact_tokeniser.do_everything(x_test_fact_list)\n",
        "y_fact_train = np.array(y_train_fact_list, dtype=np.float32)\n",
        "y_fact_test = np.array(y_test_fact_list, dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n",
        "x_snopes_train = snopes_tokeniser.do_everything(x_train_snopes_list)\n",
        "x_snopes_test = snopes_tokeniser.do_everything(x_test_snopes_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=we_shufflin )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "train_fact_source_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_source_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=we_shufflin)\n",
        "test_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "\n",
        "train_fact_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_fact_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=we_shufflin)\n",
        "test_fact_loader.name = \"fact_data\"\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=we_shufflin)\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    \n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=20)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "    else:\n",
        "      x = self.linear1(x.reshape(self.hp.batch_size, -1))\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    return self.MLP(factorised_mush), better_mush(premise_attention, hypothesis_attention)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-skRc_EBRhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, unnormalised_predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  auc = roc_auc_score(true_labels, unnormalised_predictions)\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          train_loader=None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  is_binary = hp.num_classes == 1\n",
        "  \n",
        "  if train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    correct = 0\n",
        "    penal = 0\n",
        "    for batch_index, train_data in enumerate(train_loader):\n",
        "      #setting everything up\n",
        "      model.hidden_state = model.init_hidden()\n",
        "      train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "      predicted_y, attention = model(*train_data[:-1])\n",
        "      actual_y = train_data[-1]\n",
        "      squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "      if hp.C > 0:\n",
        "        attentionT = attention.transpose(1,2)\n",
        "        identity = torch.eye(attention.size(1))\n",
        "        identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "        penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "      if is_binary:\n",
        "        loss = loss_function(squeezed_y, actual_y.double())\n",
        "        loss += hp.C * penal/train_loader.batch_size\n",
        "        correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "      else:\n",
        "        loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/train_loader.batch_size)\n",
        "        correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "      total_loss += loss.data\n",
        "\n",
        "      #cleaning up regularisation\n",
        "      if hp.C > 0:\n",
        "        del(penal)\n",
        "        del(identity)\n",
        "        del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "      optimiser.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      if hp.is_debug and batch_index % 10 == 0:\n",
        "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "            epoch, batch_index * len(train_data[0]), len(train_loader.dataset),\n",
        "            100. * batch_index / len(train_loader), loss.item()\n",
        "        ))\n",
        "\n",
        "      if using_gradient_clipping:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      batch_count += 1\n",
        "      optimiser.step()\n",
        "      free_data(train_data)\n",
        "\n",
        "    print(\"Average loss is:\",total_loss/batch_count)\n",
        "    correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "    accuracy = correct_but_numpy / float(batch_count * train_loader.batch_size)\n",
        "    print(\"Accuracy of the model\", accuracy)\n",
        "    losses.append(total_loss/batch_count)\n",
        "    accuracies.append(accuracy)\n",
        "  return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(real_results, 0), torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, accuracies=None, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  if accuracies:\n",
        "    plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Accuracy\")\n",
        "    plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "OK WE MADE THE HELPERS LETS RUN THIS SHIT :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "outputId": "4dfffee0-8d23-4fc6-e6a1-5b3ff825cb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "glove_embeddings = load_glove_embeddings(\"glove.6B.300d.txt\",fact_tokeniser.word_to_id,300)\n",
        "small_gloves = load_glove_embeddings(\"glove.6B.50d.txt\", fact_tokeniser.word_to_id, 50)\n",
        "print(glove_embeddings.shape)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36905\n",
            "36905\n",
            "torch.Size([36905, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 20\n",
        "  dense_dimension = 5\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 30\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 4\n",
        "  C = 0.3\n",
        "  is_debug = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "5d4306d5-d015-4358-eac6-4b4eb6849612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "textual_entailment_model = None\n",
        "if(textual_entailment_model):\n",
        "  del(textual_entailment_model)\n",
        "  torch.cuda.empty_cache()\n",
        "pass\n",
        "textual_entailment_model = TextualEntailmentModel(Hyperparameters, small_gloves).cuda()\n",
        "#textual_entailment_model.to(device)\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(textual_entailment_model.parameters(), \n",
        "                                    lr=0.001)\n",
        "\n",
        "loss, accuracy = train(model=textual_entailment_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)\n"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.643682\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.642583\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.641806\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.641282\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.641311\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.638307\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.633118\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.623140\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.593562\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.561135\n",
            "Average loss is: tensor(1.6331, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5333962912087912\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.569464\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.588814\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.529350\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.549316\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.527047\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.508005\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.535125\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.470257\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.526415\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.508581\n",
            "Average loss is: tensor(1.5457, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6862551510989011\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.489889\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.505296\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.431565\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.501392\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.437870\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.426577\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.314501\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.299823\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.327836\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.281982\n",
            "Average loss is: tensor(1.3895, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7978193681318682\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23454 (0%)]\tLoss: 1.221659\n",
            "Train Epoch: 3 [2560/23454 (11%)]\tLoss: 1.219890\n",
            "Train Epoch: 3 [5120/23454 (22%)]\tLoss: 1.256847\n",
            "Train Epoch: 3 [7680/23454 (33%)]\tLoss: 1.191517\n",
            "Train Epoch: 3 [10240/23454 (44%)]\tLoss: 1.216022\n",
            "Train Epoch: 3 [12800/23454 (55%)]\tLoss: 1.243253\n",
            "Train Epoch: 3 [15360/23454 (66%)]\tLoss: 1.199020\n",
            "Train Epoch: 3 [17920/23454 (77%)]\tLoss: 1.164529\n",
            "Train Epoch: 3 [20480/23454 (88%)]\tLoss: 1.166558\n",
            "Train Epoch: 3 [23040/23454 (99%)]\tLoss: 1.123966\n",
            "Average loss is: tensor(1.1926, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9111006181318682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "outputId": "5236b397-069f-4ee5-a659-7712a5b0734b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, loss, accuracy)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAogAAAEbCAYAAABZU3XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "outputId": "f2ac678f-26cd-4058-84b5-c83600c6ae8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "real_results, predicted_ys = batch_wise_evaluate(textual_entailment_model, \n",
        "         test_fact_loader,\n",
        "         Hyperparameters)"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "outputId": "540d284f-4985-4d23-f7a7-b0ef8aa0a034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu(), real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment model\n",
            "Classifier 'textual entailment model' has Acc=0.505 P=0.505 R=0.505 F1=0.505 AUC=0.509\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.502     0.484     0.493      3029\n",
            "         1.0      0.509     0.527     0.517      3073\n",
            "\n",
            "    accuracy                          0.505      6102\n",
            "   macro avg      0.505     0.505     0.505      6102\n",
            "weighted avg      0.505     0.505     0.505      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1466 1455]\n",
            " [1563 1618]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5052639984863913,\n",
              " 0.5052547147828074,\n",
              " 0.5054080629301868,\n",
              " 0.5051009773912764)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUa9XwZ-4Xny",
        "colab_type": "text"
      },
      "source": [
        "##Baseline Sentence Entailment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9tLOcRYOc9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    combined = premise_embedding * hypothesis_embedding\n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8fbaSIM4jhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineHyperparameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 30\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  num_classes = 1\n",
        "  epochs = 4\n",
        "  C = 0.3\n",
        "  is_debug = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKfpJ-Hk4ePj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "2b0681da-c4c3-48ee-cfb8-0c799fc7f27c"
      },
      "source": [
        "baseline_model = BaselineSentenceEntailment(Hyperparameters, small_gloves).cuda()\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(baseline_model.parameters(), \n",
        "                                    lr=0.001)\n",
        "\n",
        "base_loss, base_accuracy = train(model=baseline_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.634797\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.640161\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.638377\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.639743\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.638881\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.637278\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.630290\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.620125\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.597623\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.586273\n",
            "Average loss is: tensor(1.6303, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5600961538461539\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.597146\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.573365\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.571857\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.571174\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.568097\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.559038\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.530688\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.533940\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.526701\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.499647\n",
            "Average loss is: tensor(1.5506, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6765539148351648\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.548648\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.477184\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.489447\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.457400\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.440621\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.491243\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.469332\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.450463\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.417680\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.452249\n",
            "Average loss is: tensor(1.4720, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7463083791208791\n",
            "Running EPOCH: 4\n",
            "Train Epoch: 3 [0/23454 (0%)]\tLoss: 1.392224\n",
            "Train Epoch: 3 [2560/23454 (11%)]\tLoss: 1.421256\n",
            "Train Epoch: 3 [5120/23454 (22%)]\tLoss: 1.359954\n",
            "Train Epoch: 3 [7680/23454 (33%)]\tLoss: 1.375041\n",
            "Train Epoch: 3 [10240/23454 (44%)]\tLoss: 1.388032\n",
            "Train Epoch: 3 [12800/23454 (55%)]\tLoss: 1.329806\n",
            "Train Epoch: 3 [15360/23454 (66%)]\tLoss: 1.347970\n",
            "Train Epoch: 3 [17920/23454 (77%)]\tLoss: 1.414228\n",
            "Train Epoch: 3 [20480/23454 (88%)]\tLoss: 1.363443\n",
            "Train Epoch: 3 [23040/23454 (99%)]\tLoss: 1.313236\n",
            "Average loss is: tensor(1.3748, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7984203296703297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQ16nQW6Xb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "abc9d3b5-2f6b-4174-a7f4-b7af78ecfd66"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, base_loss, base_accuracy)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwY6ih2k9hGy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "86fecf25-cdea-4756-e613-e8bc18c9e2f8"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "base_real_results, baseline_predicted_ys = batch_wise_evaluate(baseline_model, \n",
        "         test_fact_loader,\n",
        "         BaselineHyperparameters)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlPE-kpx9riu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "670fb64e-968b-4916-f828-2d192869fe01"
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", baseline_predicted_ys.cpu(), base_real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment model\n",
            "Classifier 'textual entailment model' has Acc=0.495 P=0.492 R=0.492 F1=0.490 AUC=0.492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.420     0.469     0.443      2614\n",
            "         1.0      0.564     0.514     0.538      3488\n",
            "\n",
            "    accuracy                          0.495      6102\n",
            "   macro avg      0.492     0.492     0.490      6102\n",
            "weighted avg      0.502     0.495     0.497      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1226 1695]\n",
            " [1388 1793]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.49168925043972034,\n",
              " 0.49153058601180655,\n",
              " 0.4947558177646673,\n",
              " 0.4903554487636644)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3qhGP9Y-Cj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}