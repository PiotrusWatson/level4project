{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "c1785d29-adb4-44bc-8ab1-892c0e686e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-21 15:24:56--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip.1’\n",
            "\n",
            "snli_1.0.zip.1      100%[===================>]  90.17M  21.4MB/s    in 7.1s    \n",
            "\n",
            "2020-01-21 15:25:03 (12.7 MB/s) - ‘snli_1.0.zip.1’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "replace snli_1.0/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "946d7f70-3da2-4d76-eb08-fa86e4b5d4a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-21 15:53:08--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-01-21 15:53:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-01-21 15:53:09--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        0%[                    ] 219.85K   396KB/s               ^C\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8d23e0030bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget http://nlp.stanford.edu/data/glove.6B.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip glove*.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "5e7191cc-7105-43e6-bd8c-ee567772ed86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "abec700a-e491-4072-a42c-7d95356ccab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ... Stance\n",
              "0        0  ...      2\n",
              "1        0  ...      2\n",
              "2        0  ...      2\n",
              "3        0  ...      2\n",
              "4        0  ...      2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "outputId": "679c8f70-4765-4f20-ba60-18d67b6376ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_id\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  train_unique, test_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_id\"].isin(test_unique[\"claim_id\"])]\n",
        "  train_facts = facts[facts[\"claim_id\"].isin(train_unique[\"claim_id\"])]\n",
        "  return train_facts, test_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n",
        "\n",
        "train_facts.head(500)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>for firms moving overseas in order to create a...</td>\n",
              "      <td>foxnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>get a tax break specifically by outsourcing jo...</td>\n",
              "      <td>newslines.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>confusing clashes over taxes in wednesday s pr...</td>\n",
              "      <td>wsj.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>support on this bill in a time of tight budget...</td>\n",
              "      <td>senate.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>tax a lower rate for american manufacturing an...</td>\n",
              "      <td>archives.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jan_08_cory-booker_booker-mcconnell-full-...</td>\n",
              "      <td>2009 mitch mcconnell person thats saying hey g...</td>\n",
              "      <td>cory booker</td>\n",
              "      <td>to have a different standard the senate will h...</td>\n",
              "      <td>thedailybeast.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jan_08_cory-booker_booker-mcconnell-full-...</td>\n",
              "      <td>2009 mitch mcconnell person thats saying hey g...</td>\n",
              "      <td>cory booker</td>\n",
              "      <td>double standard for mcconnell to be less conce...</td>\n",
              "      <td>weaselzippers.us</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jan_08_cory-booker_booker-mcconnell-full-...</td>\n",
              "      <td>2009 mitch mcconnell person thats saying hey g...</td>\n",
              "      <td>cory booker</td>\n",
              "      <td>cory booker on government reform even billiona...</td>\n",
              "      <td>ontheissues.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_feb_15_benjamin-netanyahu_benjamin-netany...</td>\n",
              "      <td>past weeks president donald trump pointed iran...</td>\n",
              "      <td>benjamin netanyahu</td>\n",
              "      <td>think are deeply committed to do and we are ob...</td>\n",
              "      <td>whitehouse.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_feb_15_benjamin-netanyahu_benjamin-netany...</td>\n",
              "      <td>past weeks president donald trump pointed iran...</td>\n",
              "      <td>benjamin netanyahu</td>\n",
              "      <td>are deeply committed to do and we are obviousl...</td>\n",
              "      <td>haaretz.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cred_label  ...     article_source\n",
              "0             1  ...        foxnews.com\n",
              "1             1  ...      newslines.org\n",
              "2             1  ...            wsj.com\n",
              "3             1  ...         senate.gov\n",
              "4             1  ...       archives.gov\n",
              "..          ...  ...                ...\n",
              "554           1  ...  thedailybeast.com\n",
              "555           1  ...   weaselzippers.us\n",
              "556           1  ...    ontheissues.org\n",
              "557           1  ...     whitehouse.gov\n",
              "558           1  ...        haaretz.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "a6041d27-946f-4f10-e872-30d2fc8c5fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>morning new tv advertisement argues that posit...</td>\n",
              "      <td>desmoinesregister.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>iowans could not support household on current ...</td>\n",
              "      <td>americanprogressaction.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>not budged in five years leaving many falling ...</td>\n",
              "      <td>americanprogressaction.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>are working to support themselves or their fam...</td>\n",
              "      <td>iowademocrats.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>prove that i had those good hardworking skills...</td>\n",
              "      <td>iowademocrats.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>2008_oct_02_joe-biden_mccain-refused-to-commit...</td>\n",
              "      <td>john mccain said wouldnt even sit government s...</td>\n",
              "      <td>joe biden</td>\n",
              "      <td>cap is melting palin im not one to attribute e...</td>\n",
              "      <td>mysinchew.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>0</td>\n",
              "      <td>2008_oct_02_joe-biden_mccain-refused-to-commit...</td>\n",
              "      <td>john mccain said wouldnt even sit government s...</td>\n",
              "      <td>joe biden</td>\n",
              "      <td>control the weapons the theocracy does secreta...</td>\n",
              "      <td>blastmagazine.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>2008_oct_02_joe-biden_mccain-refused-to-commit...</td>\n",
              "      <td>john mccain said wouldnt even sit government s...</td>\n",
              "      <td>joe biden</td>\n",
              "      <td>be left with only one conclusion mccain was co...</td>\n",
              "      <td>chrisweigant.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2850</th>\n",
              "      <td>0</td>\n",
              "      <td>2009_may_19_mike-pence_120-million-deprived-he...</td>\n",
              "      <td>democrats propose health care plan deprive rou...</td>\n",
              "      <td>mike pence</td>\n",
              "      <td>speech politifact called claim false that demo...</td>\n",
              "      <td>democrats.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2851</th>\n",
              "      <td>0</td>\n",
              "      <td>2009_may_19_mike-pence_120-million-deprived-he...</td>\n",
              "      <td>democrats propose health care plan deprive rou...</td>\n",
              "      <td>mike pence</td>\n",
              "      <td>covering conduct going back as far as 1994 was...</td>\n",
              "      <td>democrats.org</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...              article_source\n",
              "85             1  ...       desmoinesregister.com\n",
              "86             1  ...  americanprogressaction.org\n",
              "87             1  ...  americanprogressaction.org\n",
              "88             1  ...           iowademocrats.org\n",
              "89             1  ...           iowademocrats.org\n",
              "...          ...  ...                         ...\n",
              "2766           0  ...               mysinchew.com\n",
              "2767           0  ...           blastmagazine.com\n",
              "2768           0  ...            chrisweigant.com\n",
              "2850           0  ...               democrats.org\n",
              "2851           0  ...               democrats.org\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 256\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_fact_list = convert_to_lists({\"claim_text\": train_facts[\"claim_text\"], \n",
        "                   \"claim_source\": train_facts[\"claim_source\"],\n",
        "                   \"article\": train_facts[\"article\"],\n",
        "                   \"article_source\": train_facts[\"article_source\"]})\n",
        "y_train_fact_list = train_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_fact_list = convert_to_lists({\"claim_text\": test_facts[\"claim_text\"], \n",
        "                   \"claim_source\": test_facts[\"claim_source\"],\n",
        "                   \"article\": test_facts[\"article\"],\n",
        "                   \"article_source\": test_facts[\"article_source\"]})\n",
        "y_test_fact_list = test_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "x_train_snopes_list = convert_to_lists({\"claim_text\": train_snopes[\"claim_text\"],\n",
        "                   \"article\": train_snopes[\"article\"],\n",
        "                   \"article_source\": train_snopes[\"article_source\"]})\n",
        "x_test_snopes_list = convert_to_lists({\"claim_text\": test_snopes[\"claim_text\"],\n",
        "                   \"article\": test_snopes[\"article\"],\n",
        "                   \"article_source\": test_snopes[\"article_source\"]})\n",
        "y_train_snopes_list = train_snopes[\"cred_label\"].tolist()\n",
        "y_test_snopes_list = test_snopes[\"cred_label\"].tolist()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "fact_tokeniser = Tokeniser(x_train_fact_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "snopes_tokeniser = Tokeniser(x_train_snopes_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_fact_train = fact_tokeniser.do_everything(x_train_fact_list)\n",
        "x_fact_test = fact_tokeniser.do_everything(x_test_fact_list)\n",
        "y_fact_train = np.array(y_train_fact_list, dtype=np.float32)\n",
        "y_fact_test = np.array(y_test_fact_list, dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n",
        "x_snopes_train = snopes_tokeniser.do_everything(x_train_snopes_list)\n",
        "x_snopes_test = snopes_tokeniser.do_everything(x_test_snopes_list)\n",
        "y_snopes_train = np.array(y_train_snopes_list, dtype=np.float32)\n",
        "y_snopes_test = np.array(y_test_snopes_list, dtype=np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "train_fact_source_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_source_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "\n",
        "train_fact_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_fact_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test)\n",
        "test_fact_loader.name = \"fact_data\"\n",
        "\n",
        "train_snopes_data = data_utils.TensorDataset(torch.from_numpy(x_snopes_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_snopes_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_snopes_train).type(torch.LongTensor))\n",
        "test_snopes_data= data_utils.TensorDataset(torch.from_numpy(x_snopes_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_snopes_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_snopes_test).type(torch.LongTensor))\n",
        "train_snopes_loader = data_utils.DataLoader(train_snopes_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_snopes_loader.name = \"fact_data\"\n",
        "\n",
        "test_snopes_loader = data_utils.DataLoader(test_snopes_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test)\n",
        "test_snopes_loader.name = \"fact_data\"\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(x)\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=20)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "    else:\n",
        "      x = self.linear1(x.reshape(self.hp.batch_size, -1))\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-skRc_EBRhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, unnormalised_predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(unnormalised_predictions.shape) == 1):\n",
        "    auc = roc_auc_score(true_labels, unnormalised_predictions)\n",
        "  else:\n",
        "    auc = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          train_loader=None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  is_binary = hp.num_classes == 1\n",
        "  \n",
        "  if train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    correct = 0\n",
        "    penal = 0\n",
        "    for batch_index, train_data in enumerate(train_loader):\n",
        "      #setting everything up\n",
        "      model.hidden_state = model.init_hidden()\n",
        "      train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "      predicted_y, attention = model(*train_data[:-1])\n",
        "      actual_y = train_data[-1]\n",
        "      squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "      if hp.C > 0:\n",
        "        attentionT = attention.transpose(1,2)\n",
        "        identity = torch.eye(attention.size(1))\n",
        "        identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "        penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "      if is_binary:\n",
        "        loss = loss_function(squeezed_y, actual_y.double())\n",
        "        loss += hp.C * penal/train_loader.batch_size\n",
        "        correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "      else:\n",
        "        loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/train_loader.batch_size)\n",
        "        correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "      total_loss += loss.data\n",
        "\n",
        "      #cleaning up regularisation\n",
        "      if hp.C > 0:\n",
        "        del(penal)\n",
        "        del(identity)\n",
        "        del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "      optimiser.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      if hp.is_debug and batch_index % 10 == 0:\n",
        "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "            epoch, batch_index * len(train_data[0]), len(train_loader.dataset),\n",
        "            100. * batch_index / len(train_loader), loss.item()\n",
        "        ))\n",
        "\n",
        "      if using_gradient_clipping:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      batch_count += 1\n",
        "      optimiser.step()\n",
        "      free_data(train_data)\n",
        "\n",
        "    print(\"Average loss is:\",total_loss/batch_count)\n",
        "    correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "    accuracy = correct_but_numpy / float(batch_count * train_loader.batch_size)\n",
        "    print(\"Accuracy of the model\", accuracy)\n",
        "    losses.append(total_loss/batch_count)\n",
        "    accuracies.append(accuracy)\n",
        "  return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(real_results, 0), torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, accuracies=None, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  if accuracies:\n",
        "    plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Accuracy\")\n",
        "    plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "OK WE MADE THE HELPERS LETS RUN THIS SHIT :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "outputId": "9ac3864c-4dde-4c9e-8ea5-72163ed4b623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "glove_embeddings = load_glove_embeddings(\"glove.6B.300d.txt\",fact_tokeniser.word_to_id,300)\n",
        "small_gloves = load_glove_embeddings(\"glove.6B.50d.txt\", fact_tokeniser.word_to_id, 50)\n",
        "print(glove_embeddings.shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36905\n",
            "36905\n",
            "torch.Size([36905, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 3\n",
        "  dropout=0.3\n",
        "  C = 0.3\n",
        "  is_debug = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "e05462e5-ff65-4df9-a8a5-941f3ad8f90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "textual_entailment_model = None\n",
        "if(textual_entailment_model):\n",
        "  del(textual_entailment_model)\n",
        "  torch.cuda.empty_cache()\n",
        "pass\n",
        "textual_entailment_model = TextualEntailmentModel(Hyperparameters, small_gloves).cuda()\n",
        "#textual_entailment_model.to(device)\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(textual_entailment_model.parameters(), \n",
        "                                    lr=0.01)\n",
        "\n",
        "loss, accuracy = train(model=textual_entailment_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642562\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.635598\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.587098\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.456433\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.362328\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.207547\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.150028\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.071143\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.037369\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.023858\n",
            "Average loss is: tensor(1.3333, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7959306318681318\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.003229\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.937819\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.910886\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.891565\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.950585\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.965697\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.927097\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.885830\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.906182\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.882283\n",
            "Average loss is: tensor(0.9062, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9853622939560439\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.859099\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.858742\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.853879\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.869491\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.879011\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.811034\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.797870\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.759725\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.775326\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.749198\n",
            "Average loss is: tensor(0.8175, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.996823489010989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "outputId": "72ce9202-3ac7-494e-e794-40ba6608622e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, loss, accuracy)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3wWZdb/8c9Jp/feIygoVaKsoNh2\nLagoohQRgV3LFnV9nmd9Vlfd4rrF8ttdV59VURFBioKIve1asCsoIsVClaY06TXJ+f1xTeBOTCBA\nkkn5vl+veeW+Z66ZOfckkJNzzXWNuTsiIiIiIgBJcQcgIiIiIuWHkkMRERER2UvJoYiIiIjspeRQ\nRERERPZScigiIiIieyk5FBEREZG9lByKVDBm1tbM3MxS4o5FREQqHyWHIiIiIrKXkkORckzVQRER\nKWtKDqVKMbNfm9lKM9tiZl+Y2enR+rFmdltCu1PMbEXC+6VmdqOZzTez78zsETPLKOIcI83sbTO7\nK2q7xMzOTthex8weNrPVUSy3mVlywr7vmNnfzWw98HszS46Otc7MFgPnFHK+xdFnWmJmw0r2qomI\nSFWi5FCqDDM7CrgaOM7dawFnAksP4hDDon2OAI4Ebt5P217AF0BD4A7gYTOzaNtYIBtoD/QAzgAu\nL7DvYqAJ8CfgCuDcqG0WcFHCZ6oB/BM4O/pMvYHZB/GZRERE8lFyKFVJDpAOHG1mqe6+1N0XHcT+\n97r7cnffQEjahu6n7TJ3f9Ddc4BHgWZAEzNrAvQDrnP3be6+Bvg7MCRh31Xufo+7Z7v7DmAQ8I+E\nc/+lwLlygc5mVs3dV7v7vIP4TCIiIvkoOZQqw90XAtcBvwfWmNlkM2t+EIdYnvB6GbC/fb9JOO/2\n6GVNoA2QCqw2s41mthF4AGhcxHmIzlPw3HnH3gYMBn4aHfN5M+tYvI8jIiLyfUoOpUpx94nufiIh\nSXPg9mjTNqB6QtOmhezeKuF1a2DVIYSwHNgFNHT3utFS292PSQyzwD6rCzn3vsbuL7v7jwjVyc+B\nBw8hLhEREUDJoVQhZnaUmZ1mZunATmAHoUsWwn16/cysvpk1JVQYC/qFmbU0s/rATcDjBxuDu68G\nXgH+n5nVNrMkMzvCzE7ez25PANdG564H3JDwmZqY2fnRvYe7gK0Jn0lEROSgKTmUqiQd+CuwjtDt\n2xi4Mdo2HviUMEDlFQpP/CZG2xYDi4DbCmlTHJcBacB84DtgKqHqV5QHgZej+D4GpiVsSwL+m1DF\n3ACcDPzsEOMSERHB3Av2YIlIQWa2FLjc3f8ddywiIiKlSZVDEREREdlLyaGIiIiI7KVuZRERERHZ\nS5VDEREREdkrJe4ASkLDhg29bdu2cYchIlKhzJo1a527N4o7DhEpXypFcti2bVtmzpwZdxgiIhWK\nmS07cCsRqWrUrSwiIiIieyk5FBEREZG9lByKiIiIyF6V4p5DEam89uzZw4oVK9i5c2fcoVRYGRkZ\ntGzZktTU1LhDEZEKQMmhiJRrK1asoFatWrRt2xYzizucCsfdWb9+PStWrKBdu3ZxhyMiFYC6lUWk\nXNu5cycNGjRQYniIzIwGDRqo8ioixabkUETKPSWGh0fXT0QORtVODretgxdvgOxdcUciIiIiUi5U\n7eRw6VvwwX0wZSTk7Ik7GhEpx6ZPn46Z8fnnn8cdiohIqarayeExA6DfXfDFC/Dk5ZCTHXdEIlJO\nTZo0iRNPPJFJkyaV2jlycnJK7dgiIsVVtZNDgOOvgDP/DPOnw/SfQa7+cxaR/LZu3crbb7/Nww8/\nzOTJk/euv/322+nSpQvdunXjhhtuAGDhwoX88Ic/pFu3bhx77LEsWrSIN954g3PPPXfvfldffTVj\nx44FwuM/f/3rX3PssccyZcoUHnzwQY477ji6devGwIED2b59OwDffvstAwYMoFu3bnTr1o13332X\n3/72t/zjH//Ye9ybbrqJu+++uwyuiIhUZprKBuCEX0D2TvjPrZCSDuf9E5KUN4uUN394dh7zV20u\n0WMe3bw2vzvvmP22efrppznrrLM48sgjadCgAbNmzWLNmjU8/fTTfPDBB1SvXp0NGzYAMGzYMG64\n4QYGDBjAzp07yc3NZfny5fs9foMGDfj4448BWL9+PVdccQUAN998Mw8//DDXXHMN1157LSeffDJP\nPfUUOTk5bN26lebNm3PhhRdy3XXXkZuby+TJk/nwww9L4KqISFWm5DDPSf8TBqa8eXtIEPvdBRrh\nJyKELuVf/vKXAAwZMoRJkybh7owaNYrq1asDUL9+fbZs2cLKlSsZMGAAECafLo7BgwfvfT137lxu\nvvlmNm7cyNatWznzzDMBeO211xg3bhwAycnJ1KlThzp16tCgQQM++eQTvv32W3r06EGDBg1K7HOL\nSNWk5DDRKTeGCuI7d0NyOpz5JyWIIuXIgSp8pWHDhg289tprfPbZZ5gZOTk5mBkXX3xxsY+RkpJC\nbm7u3vcF5xysUaPG3tcjR45k+vTpdOvWjbFjx/LGG2/s99iXX345Y8eO5ZtvvuHHP/5xsWMSESmK\n+k4TmcEP/wC9fgrv/1/oZnaPOyoRidHUqVMZPnw4y5YtY+nSpSxfvpx27dpRp04dHnnkkb33BG7Y\nsIFatWrRsmVLpk+fDsCuXbvYvn07bdq0Yf78+ezatYuNGzfyn//8p8jzbdmyhWbNmrFnzx4mTJiw\nd/3pp5/OfffdB4SBK5s2bQJgwIABvPTSS3z00Ud7q4wiIodDyWFBZnDWX6HnKHj7bzDjzrgjEpEY\nTZo0aW83cZ6BAweyevVq+vfvT1ZWFt27d+euu+4CYPz48fzzn/+ka9eu9O7dm2+++YZWrVoxaNAg\nOnfuzKBBg+jRo0eR5/vjH/9Ir1696NOnDx07dty7/u677+b111+nS5cu9OzZk/nz5wOQlpbGqaee\nyqBBg0hOTi6FKyAiVY15JaiMZWVl+cyZM0v2oLm58MzVMHtCqCaeeF3JHl9EimXBggV06tQp7jDK\nrdzc3L0jnTt06FBku8Kuo5nNcves0o5RRCqWMq0cmtkYM1tjZnOL2H6+mc0xs9lmNtPMTizL+PJJ\nSoL+90DngfDv38H798cWiohIYebPn0/79u05/fTT95sYiogcjLIekDIWuBcYV8T2/wDPuLubWVfg\nCaBjEW1LX1IyDHgAcnbDS7+GlDTI0g3fIlI+HH300SxevDjuMESkkinTyqG7zwA27Gf7Vt/Xz10D\niL/POzkVBo6BDmfCc/8FsyfGHZGIiIhIqSl3A1LMbICZfQ48DxRZpjOzK6Ou55lr164t3aBS0mDQ\nOMg8FZ7+BXw2tXTPJyIiIhKTcpccuvtT7t4RuAD4437ajXb3LHfPatSoUekHlpoBQyZC694w7UqY\n/0zpn1NERESkjJW75DBP1AWdaWYN445lr7TqcMnj0DILpv4Yvngp7ohERERESlS5Sg7NrL1ZeCSJ\nmR0LpAPr442qgPSaMGwKNO0MTwyHhUVPZisilUPNmjXjDkFEpMyU9VQ2k4D3gKPMbIWZ/cTMfmpm\nP42aDATmmtls4P+AwV4eJ2LMqAOXToOGR8HkYbDkrbgjEhERESkRZT1aeai7N3P3VHdv6e4Pu/v9\n7n5/tP12dz/G3bu7+wnu/nZZxndQqteHy6ZDvTYwcTB8/UHcEYlIGVq6dCmnnXYaXbt25fTTT+fr\nr78GYMqUKXTu3Jlu3brRt29fAObNm8fxxx9P9+7d6dq1K1999VWcoYuI7FdZz3NYudRoCJc9A2P7\nwYSLQrLYomfcUYlUXi/eAN98VrLHbNoFzv7rQe92zTXXMGLECEaMGMGYMWO49tprmT59Orfeeisv\nv/wyLVq0YOPGjQDcf//9/PKXv2TYsGHs3r2bnJyckv0MIiIlqFzdc1gh1WoSEsRq9WD8hbB6TtwR\niUgZeO+997jkkksAGD58OG+/HTo6+vTpw8iRI3nwwQf3JoEnnHACf/7zn7n99ttZtmwZ1apViy1u\nEZEDUeWwJNRpASOehUf6wfgLYOTz0FjPghUpcYdQ4Str999/Px988AHPP/88PXv2ZNasWVxyySX0\n6tWL559/nn79+vHAAw9w2mmnxR2qiEihVDksKfXawIhnICkVHu0P6xbGHZGIlKLevXszefJkACZM\nmMBJJ50EwKJFi+jVqxe33norjRo1Yvny5SxevJjMzEyuvfZazj//fObMUQ+DiJRfSg5LUoMjQgUR\nh0fPgw1L4o5IRErA9u3badmy5d7lb3/7G/fccw+PPPIIXbt2Zfz48dx9990AXH/99XTp0oXOnTvT\nu3dvunXrxhNPPEHnzp3p3r07c+fO5bLLLov5E4mIFM3K40wxBysrK8tnzpwZdxj7fDsPxp4D6bVg\n5AtQt1XcEYlUWAsWLKBTJ92mcbgKu45mNsvds2IKSUTKKVUOS0OTY2D4dNixCcb1h82r445IRERE\npFiUHJaW5t1h+DTYuiYkiFvXxh2RiIiIyAEpOSxNLbPCo/Y2rYBx58P2DXFHJFIhVYbbX+Kk6yci\nB0PJYWlr0xuGToYNi8I0Nzs2xh2RSIWSkZHB+vXrleAcIndn/fr1ZGRkxB2KiFQQmuewLGSeDIMn\nwOSh8NjA8CSV9FpxRyVSIbRs2ZIVK1awdq1uzThUGRkZtGzZMu4wRKSCUHJYVjr8EC4eC09cBhMG\nwaVTIa1G3FGJlHupqam0a9cu7jBERKoMdSuXpY7nwMCHYPn7MGkI7NkRd0QiIiIi+Sg5LGvHDIAL\n7oclb8HjwyF7V9wRiYiIiOyl5DAO3QZD/3/CwldhyijI2RN3RCIiIiKAksP4HHsZ9LsLvngenrwc\ncrLjjkhEREREA1JidfwVoVv5lZsgJR0uuA+SkuOOSkRERKowJYdx6301ZO+E1/4YEsRz74YkFXRF\nREQkHkoOy4O+vwoVxBl3QHI69LsTzOKOSkRERKogJYflxam/CRXEd/8ZKohn3KYEUURERMqcksPy\nwgx+dGuoIL53L6RkwOm3xB2ViIiIVDFKDssTMzj7dsjZBW/dFRLEk6+POyoRERGpQpQcljdmcM7f\nIXs3vH4bpKRBn1/GHZWIiIhUEUoOy6OkJDj/3lBBfPW3oYLY66q4oxIREZEqQMlheZWUDAMeCPcg\nvvi/kJwGWaPijkpEREQqOU2oV54lp8JFj0CHM+G5/4LZE+OOSERERCo5JYflXUoaDBoHmSfD07+A\nz6bGHZGIiIhUYkoOK4LUDBgyCVqfANOuhAXPxh2RiIiIVFJlmhya2RgzW2Nmc4vYPszM5pjZZ2b2\nrpl1K8v4yrW06nDJ49CiJ0wZBV++HHdEIiIiUgmVdeVwLHDWfrYvAU529y7AH4HRZRFUhZFeCy6d\nCk07w+PDYdFrcUckIiIilUyZJofuPgPYsJ/t77r7d9Hb94GWZRJYRZJRBy6dBg07wKRLYOnbcUck\nIiIilUh5vufwJ8CLRW00syvNbKaZzVy7dm0ZhlUOVK8Plz0N9drAhEHw9QdxRyQiIiKVRLlMDs3s\nVEJy+Oui2rj7aHfPcvesRo0alV1w5UWNhiFBrNUUJlwEKz+OOyIRERGpBMpdcmhmXYGHgPPdfX3c\n8ZRrtZrCiGehWj0YPwC++SzuiERERKSCK1fJoZm1BqYBw939y7jjqRDqtAgJYlpNGHc+rPk87ohE\nRESkAivrqWwmAe8BR5nZCjP7iZn91Mx+GjX5LdAA+JeZzTazmWUZX4VVrw2MeAaSUmFcf1i3MO6I\nREREpIIyd487hsOWlZXlM2cqj2TtF/BIv/Ac5lEvQP12cUckIuWYmc1y96y44xCR8qVcdSvLYWp0\nVBikkr0jVBA3Lo87IhEREalglBxWNk07w/CnYMemkCBuXh13RCIiIlKBKDmsjJr3gEufhK1rQoK4\ntYrNAykiIiKHTMlhZdXqOBg2BTatCKOYtxf5YBoRERGRvZQcVmZtesPQSbB+IYy/AHZsjDsiERER\nKeeUHFZ2mafAkAnw7Xx4bCDs2hJ3RCIiIlKOKTmsCjr8CAY9Cqtnh2cx794Wd0QiIiJSTik5rCo6\nngMXPgjL34dJQ2DPjrgjEhERkXJIyWFV0vlCuOB+WPIWPD4csnfFHZGIiIiUM0oOq5pug+G8u2Hh\nqzBlFOTsiTsiERERKUeUHFZFPUdAv7vgi+fhycshJzvuiERERKScSIk7AInJ8VeEbuVXboKUdLjg\nPkhKjjsqERERiZmSw6qs99WQvRNe+2NIEM+9G5JUTBYREanKlBxWdX1/FSqIM+6A5HTodyeYxR2V\niIiIxETJocCpvwkVxHf/GSqIZ9ymBFFERKSKKlYfopm9ZmYdi9h2pJm9VrJhSZkygx/dCsdfBe/d\nC6/dFndEIiIiEpPiVg5PAWoXsa0WcHKJRCPxMYOzb4ecXfDWXZCSASdfH3dUIiIiUsYOplvZi1h/\nBLC1BGKRuJnBOX+H7N3w+m2hi7nPtXFHJSIiImWoyOTQzEYBo6K3Dow2sy0FmlUDOgP/KZ3wpMwl\nJcH594YK4qu3hApiryvjjkpERETKyP4qh7lATvTaCrzPsx64D7i95EOT2CQlw4AHwijmF6+HlDTo\nOTLuqERERKQMFJkcuvujwKMAZvY68DN3/7ysApOYJafCRWPg8Uvh2evCNDfdh8YdlYiIiJSyYo1W\ndvdTlRhWQSnpMGg8ZJ4MT/8c5j4Zd0QiIiJSyoo9IMXMagP9gNZARoHN7u5/LMnApJxIzYAhE2HC\nxfDkFZCcBp3OizsqERERKSXFSg7NrA/wLFC3iCYOKDmsrNJqwCWPw/gBMGVUSBaPPCPuqERERKQU\nFPdBuv8AlgLHARnunlRgSS61CKV8SK8Fw6ZCk2PCfYiLXo87IhERESkFxU0OOwE3u/ssd99dmgFJ\nOVatLgx/Chp2gElDYek7cUckIiIiJay4yeHXQHppBiIVRPX6MHw61G0NEwfB8g/jjkhERERKUHGT\nwz8AN0SDUqSqq9kIRjwDNRvDYwNh5cdxRyQiIiIlpLjJ4blAE2CJmT1nZuMKLI8W5yBmNsbM1pjZ\n3CK2dzSz98xsl5n9qpixSRxqNYURz4au5vED4JvP4o5IRERESkBxk8MTCSOSNwPHACcVshTHWOCs\n/WzfAFwL3FXM40mc6rQMCWJaDRh3AazRVJgiIiIVXXEnwW53gCWzmMeZQUgAi9q+xt0/AvYUL3yJ\nXb22IUFMSoZx/WH9orgjEhERkcNQ3MphuWNmV5rZTDObuXbt2rjDqdoaHAGXPQO5OfDoefDd0rgj\nEhERkUNUrOTQzFofaCntQAty99HunuXuWY0aNSrr00tBjTvCZU/Dnu0hQdy0Iu6IRERE5BAUt3K4\nFFhygEWquqadwzyIOzaGBHHLN3FHJCIiIgepuM9W/jFhQEqiBoRRzO3Qo/MkT/MecOmTYQTzo/1h\n5PNh6hsRERGpEMy9YM53kAcwGw8sc/ebi9F2EnAK0BD4FvgdkArg7vebWVNgJlAbyAW2Ake7++b9\nHTcrK8tnzpx5OB9DStrSd8IciPUzYeRzYfJsESlXzGyWu2fFHYeIlC8lkRyeCTzi7s1LJqSDp+Sw\nnFr0OkwcDI07hfsRq9WNOyIRSaDkUEQKUxKjlRsDGSVwHKlsjjgVBj8G386DCRfBri1xRyQiIiIH\nUKx7Ds2sbyGr04DOwI3AWyUZlFQiR54BF4+FJy6DCYPg0qlh0mwREREpl4o7IOUNvj8gxaKvbwI/\nK6mApBLqdC4MfBCevBwmDYVLHofUanFHJSIiIoUobnJ4aiHrdhIGomi+EjmwzgMhZw889dNQRRz8\nGKSkxx2ViIiIFFCs5NDd3yztQKQK6DYEsnfCs7+EqT8O3c3JqXFHJSIiIgmKWzkEwMw6AycD9QnP\nSH7D3eeVRmBSSfUcCdm74cXrYdoVcOFDkHxQP4YiIiJSioo7ICUFGAsMZd+9hgBuZhOBke6eU/Lh\nSaXU60rI2QWv3AzJ6XDBfZBUYR/zLSIiUqkUt2TzO2AQ8FvgMeAboClwabRtcfRVpHh6XxO6mF+7\nDVLS4Ny7lSCKiIiUA8VNDi8FbnP3PyWsWwb8ycySgVEoOZSD1fd6yN4FM+6ElAw4+w4wO/B+IiIi\nUmqKmxw2B94tYtu7wE0lE45UOafeFCqI794DyWlwxm1KEEVERGJU3ORwFdAH+Hch23pH20UOnhn8\n6I+hgvjevWH+w9MO+JhuERERKSXFTQ4nADeZWW70ejXhnsMhhKrh7aUTnlQJZnDW7fu6mJPT4eTr\n445KRESkSipucvh7IBP4Q/Q6jwGTgFtLNCqpepKS4Nx/hATx9dvCBNl9ro07KhERkSqnuJNgZwOX\nmNmfgL7sm+dwhuY5lBKTlATn/x/k7IZXbwmDVHpdGXdUIiIiVcpBzT4cJYJKBqX0JKfAhaNDgvji\n9WGam54j445KRESkyjjYJ6S0AloBGQW3uftrJRWUVHHJqXDRGHj8Unj2unAPYvehcUclIiJSJRT3\nCSmZhIEox+etir569NqB5BKPTqqulHQYNB4mDYanfx4qiJ0Hxh2ViIhIpVfcyuFDQGvgOuBzYHep\nRSSSJzUDhkyECRfDk1eEeRA7nRd3VCIiIpVacZPD4wjPT36yNIMR+Z60GnDJ4zB+AEwZFZLFI8+I\nOyoREZFKq7gPs12BqoUSl/RaMGwqNDkm3Ie46PW4IxIREam0ipsc/hn4tZnVKM1gRIpUrS4Mfwoa\ndoBJQ2HpO3FHJCIiUikVd57D8WbWEVhqZu8D332/iY8o8ehEElWvD8Onw9hzYOKgkCy2Ov7A+4mI\niEixFatyaGYjgRuBusCxwEmFLCKlr2YjGPEM1GwMjw2ElR/HHZFIufLSSy9x1FFH0b59e/76179+\nb/uyZcs4/fTT6dq1K8BRZtYyb5uZ3W5mc6NlcMJ6M7M/mdmXZrbAzK6N1tcxs2fN7FMzm2dmo6L1\n3c3svWjdnALHejhqP8fMpppZzcT4zGygmbmZZUXvU83sUTP7LDr3jQltfxnFOs/MrktYX9/MXjWz\nr6Kv9aL1HaO4dpnZrwqc9ywz+8LMFprZDQnrTzOzj6PzPGpmKfv77Pu7jiIVhrsfcAGWAU8CdYvT\nvqyXnj17ulQxG5e7/72z+19au6+eE3c0IuVCdna2Z2Zm+qJFi3zXrl3etWtXnzdvXr42F110kY8d\nO9bd3YEvgPHhJecArxJ6lGoAHwG1o22jgHFAUvS+cfT1N8Dt0etGhCdnpQFHAh2i9c2B1Xm/P/KO\nGb3+G3BDwvtawAzgfSArWncJMDl6XR1YCrQFOgNzo3UpwL+B9lG7O/KOC9yQEGNjwgDLPwG/Sjhv\nMrCI8JjYNOBT4GhCAWU5cGTU7lbgJwf47EVeRy1aKspS3HsOGwD/cveNxWwvUrrqtIQRz4bRzOMu\ngDWfxx2RSOw+/PBD2rdvT2ZmJmlpaQwZMoSnn346X5v58+dz2mmn5b3dApwfvT6a8EjUbHffBswB\nzoq2/Qy41d1zAdx9TbTegVpmZkBNQoKU7e5fuvtXUdtVwBpCAoW7b4ZQjQSqRcfI80fgdmBnwjoH\nakQVu2qEwZGbgU7AB+6+3cMjXt8ELoz2OR94NHr9KHBBXtzu/hGwp8ClOx5Y6O6L3X03MDk6RgNg\nt7t/GbV7FcibcLXQz36A6yhSIRQ3OXyb8A9RpPyo1xYuewaSkmFcf1i/KO6IRGK1cuVKWrVqtfd9\ny5YtWblyZb423bp1Y9q0aXlv6xISnAaEatlZZlbdzBoCpxKeiAVwBDDYzGaa2Ytm1iFafy/hd8Mq\n4DPgl3kJZB4zO55QUVuUsO4R4BugI3BPtO5YoJW7P1/gY00FthGqj18Dd7n7BkLV8CQza2Bm1YF+\nCfE2cffV0etvgCb7v3K0IFQI86yI1q0DUvK6uIGLEs5R1Gff33UUqRCKmxz+ErjCzIZF/xCTCi6l\nGaRIkRq2DwlibjY8eh58tzTuiETKtbvuuos333yTHj16QOjGXQnkuPsrwAvAu8Ak4D0gJ9otHdjp\n7lnAg8CYaP2ZwGxC13F34F4zq513LjNrBowHRiUmje4+KtpnASHpTCJ0Mf9PISEfH8XRHGgH/I+Z\nZbr7AkKV8RXgpSiOnII7u7uTvzpZbNG+Q4C/m9mHhEpr3jkK/ewHuI4iFUJxk7oFQBfCPSdrCCX5\ngotIPBp3hMueht3bQoK4aUXcEYnEokWLFixfvq8AtmLFClq0aJGvTfPmzZk2bRqffPIJhMSQvFuG\n3P1P7t7d3X9EeDRqXnfqCiCv3PgU0DV6PQqY5sFCYAmhGkiUJD4P3OTu7xeM1d1zCN23AwlJamfg\nDTNbCvwAeCaq2F0CvOTue6Lu7HeArOgYD7t7T3fvS5hFIy/eb6PENC9BXcP+rSR/da9lwrV5z91P\ncvfjCfdD5p2jyM++n+soUiEUNzm8FfhD9LWo5YDMbIyZrTGzuUVsNzP7ZzRabE7UzSByYE27hKlt\ndmwMCeKWb+KOSKTMHXfccXz11VcsWbKE3bt3M3nyZPr375+vzbp168jN3VvEa0ZUBTSz5Kh7GTPr\nSkgAX4naTSd0jwKczL5k52vg9GifJsBRwGIzSyMkkePcfWreyaL/49vnvQb6A5+7+yZ3b+jubd29\nLWFASn93nxmd47RonxqExPHz6H3j6Gtrwv2GE6NTPQPkTa82Ash/4+X3fQR0MLN2UexDomMkniMd\n+DVw/wE++/6uo0jFcLgjWoBTgDHFbNuXMBXO3CK29wNeJPyl9QPCzcYHPK5GK8teX3/gflsz93uO\nc9+yJu5oRMrc888/7x06dPDMzEy/7bbb3N39lltu8aefftrd3adMmeLt27f3Dh06OLAWSPfw/28G\nMD9a3ge6+77/m+sSqoCfEbpJu0XrmxMSn88I9wBeGq2/lNCjNDth6U4oSLyT0H4ChYzkBd5g32jl\nmsAUYF4U2/UJ7d6K1n0KnJ6wvgHwH+Arwijm+tH6poQq6GZgY/Q6b0R2P0LSu4hQ7cw71p2E3rMv\ngOsS1hf12Yu8jlq0VJTF3A/+VozoL7/LgOFAa2CHu9fc/157920LPOfunQvZ9gDwhrtPit5/AZzi\n+24sLlRWVpbPnDnzoD6DVIf1BwwAACAASURBVGJL34bHLoIGR4QRzdXrxx2RSLlkZrM83EcoIrJX\nsZ6QAmHCT2AwoUT/g2j1p8BfCTfdloSiRox9Lzk0syuBKwFat25dQqeXSqHtiTB0EkwcDOMHhEmz\nM+rEHZVUIbm5zu6cXHZl57I7O5fdOdHXve9z9m3LLqJd4v7RPvnaRm3O6dKMIcfr/0ARKTn7TQ6j\nEWRnERLC8wjl8lXA/wG/IJTYZ5R2kIVx99HAaAiVwzhikHLsiFNh8HiYPCxUEYdPg/RacUclpWRv\nMrYnl11RElVYUrar4PvsXHZn5+RrV7BN3rZdhR4np5DEL5c9OSX3X1JaShLpyUmkpSQsyUmkp4av\ne3L135+IlKwik0Mz+3+EUWKNCROSPkWYTPTfQG3g6lKIp8gRYyIH7cgz4eJH4IkRoYo4bCqkVY87\nqkohJ9cTEqUcdu35ftUrXwKWL4HK+V6l7PttErflFN4m4XV2CSZI6VEClh4lYQWTsrSUJOqkpe5N\n0PIlboUlcSlJpKckH7BN2t7zJu9dn5pshHEbIiJlZ3+Vw/8izA31AjDS3dfnbTCz0vpT9RngajOb\nDPQCNh3ofkOR/ep0Hgx8EJ68HCYPhaGPQ2pG3FEdtOycwhOu7ydlBboeC6uUFayE5UQVsMKSuiKS\nspwSSsbM2JssJSZjBZOp6tVT9iVPyYmJVF6b5HwJV2EJW/4E7Pv7KBkTEQn2lxw+DFxMeE7kF1HC\nNs7dPzzUk5nZJMLo5oZmtgL4HZAK4O73ExLRfsBCYDthHimRw9N5IGTvhuk/gyeGw+AJkJJW6qdd\ntXEHT32ykk079rBrT04h95AVXWkr2N1ZUoUxMxKSsOSE5Crpe8nY95OpxDbJ30vQvt8m//6JFbG8\n9SlJSsZERMqbIpNDd7/CzK4BBhDuObwK+JmZfUnoYj7oX1fuPvQA251wL6NIyeo+FLJ3wnPXwdRR\ncPFYSE4tlVMtWL2Z0TMW8+ynq8jOdTJS8ydj6SnfT6BqZqQUK5kq2KaoSlv+qpqSMRERKb79Dkhx\n952EkciTolnmhxOmsLkhavJXM/sXMDVqK1J+ZY2CnN3w4v/CtCth4EPhucwlwN15d9F6HpixmBlf\nrqV6WjLDT2jDj/u0o1V93ecoIiIVR7Gnsonu/bsDuCN6pNEIwizy4wgPTq9XKhGKlKReV0H2Lnj1\nFkhJh/P/BUmH/mjw7JxcXpj7DaNnLGLuys00rJnO9WcexbBeralbvfS7rkVEREpasZPDRB4eaTTT\nzP4bOJdQTRSpGPpcGxLE12+D5DQ47+5wM95B2L47m8c/Ws7Dby9hxXc7yGxYg79c2IUBPVqQkVoy\n1UgREZE4HFJymMfd9xDuP3yqZMIRKSMnXx/uQXzrLkjJgLNvL1aCuHbLLsa9t5Rx7y1j0449ZLWp\nx2/PPZofdmpCUpLu5RMRkYrvsJJDkQrttJtDgvjevWH08o/+WGSCuHjtVh58awlPfryCPTm5/KhT\nE646OZOebfRoPhERqVyUHErVZQZn3Ba6mN+9B1KqwWk35Wsya9l3jJ6xiFfmf0tqchIDj23J5Se1\n44hGxXqUuIiISIWj5FCqNjM4+w7I2QUz7oCUNHJP/BX/XvAto2csZuay76hTLZVfnNKeEb3b0qhW\netwRi4iIlColhyJJSXDuP8jZs4vk127jgXdWcvumH9GibjV+d97RDMpqRY10/VMREZGqQb/xpMrb\ntH0Pj32wjHHzB3JLzjJ+tusR+hzfgqPP/x9Skg99mhsREZGKSMmhVFkrvtvOmLeXMvmjr9m+O4e+\nRzai/olj8Vn/Tdc5t0GbxtBzRNxhioiIlCklh1LlzFu1idEzFvPcnNUYcF635lxxUiZHN68dGmSO\nhcnD4Nlfhomyuw2JM1wREZEypeRQqgR3562v1jF6xmLeXriOGmnJjOrdlh+f2I7mdavlb5ySDoPH\nw8TBMP1nYaLszhfGE7iIiEgZU3IoldqenFyen7OaB2YsZsHqzTSqlc6vz+rIJb1aU6daatE7plaD\noZPgsYvgyctDgtjp3LILXEREJCZKDqVS2rorm8kffs0j7yxl5cYdtG9ckzsGduX8Hs1JTynm4+3S\nasCwJ2D8AJgyEoZMhCPPKNW4RURE4qbkUCqVNVt2MvadpTz2/jI278zm+Lb1ufX8Yzj1qMaH9ni7\n9FowbCqM6w+PXxqSxcxTSjpsERGRckPJoVQKC9ds5cEZi3nqk5Xsyc3lrGOacmXfTHq0rnf4B69W\nF4ZPh7HnwsQhcOmT0LbP4R9XRESkHFJyKBWWuzNz2Xc88OZi/r3gW9JTkrg4qyWXn5RJu4Y1SvZk\n1evDZU/D2H4wcVBIFlsdV7LnEBERKQeUHEqFk5PrvDr/Wx6YsYhPvt5IveqpXHt6By47oQ0Na5bi\n4+1qNoLLngkJ4mMDYcTT0LxH6Z1PREQkBkoOpcLYuSeHJz9ewUNvLWHJum20ql+NP/Q/houzWlI9\nrYx+lGs3gxHPwiNnh4EqI56Dpp3L5twiIiJlQMmhlHvfbdvN+PeX8ei7S1m/bTddW9bh3kt6cNYx\nTeN5vF2dllGC2A/GnQ+jXoBGR5V9HCIiIqVAyaGUW8s3bOfht5fw+EfL2bEnh1OPasSVfY/gB5n1\nMTuEkcclqV7bfV3Mj/YPCWKDI+KNSUREpAQoOZRy57MVm3hgxiJe+Gw1yUlG/24tuLJvJkc1rRV3\naPk1bJ+QIJ4XEsR6beOOSkRE5LAoOZRywd1588u1jJ6xmHcXradWegpXnJTJqD7taFonI+7wita4\nYzSK+dx9FcQ6LeOOSkRE5JApOZRY7cnJ5dlPVzF6xmI+/2YLTWqnc+PZHRnaqzW1M/bzeLvypGkX\nGP5UuP8wL0Gs1TTuqERERA6JkkOJxZade5j84XLGvLOE1Zt2cmSTmtx1cTf6d2tOWkoMg0wOV4tj\nw+TY4y4ICeLI58PUNyIiIhWMkkMpU99u3smYd5Yw8f2v2bIrmx9k1ufPA7pwylGN4h9kcrhaHR8e\nr/fYRTD+gjCiuXr9uKMSERE5KEoOpUx89e0WRs9YzPTZK8nJdc7u0oyr+mbStWXduEMrWW1PhKGT\nYOLgaB7EZyCjTtxRiYiIFFuZJ4dmdhZwN5AMPOTufy2wvQ0wBmgEbAAudfcVZR2nHD5354MlGxg9\nYzGvfb6GjNQkhh7fmstPzKR1g+pxh1d6jjgVBo+HycNCFfGM26BZV0itFndkIiIiB2TuXnYnM0sG\nvgR+BKwAPgKGuvv8hDZTgOfc/VEzOw0Y5e7D93fcrKwsnzlzZilGLgcjJ9d5ed43PDBjMZ8u30j9\nGmmMOKEtw09oQ/0aaXGHV3YWPAtTRkHuHkhKgSadoUVPaJkFLbKgQXtIqoD3V0qlYWaz3D0r7jhE\npHwp6+TwBOD37n5m9P5GAHf/S0KbecBZ7r7cwk1om9y99v6Oq+SwfNixO4eps5bz0NtLWLZ+O20a\nVOfykzK56NiWVEtLjju8eGxdCys+hBUzYeVMWPkJ7N4StmXUgebH7ksWW2ZBjYbxxitVipJDESlM\nWXcrtwCWJ7xfAfQq0OZT4EJC1/MAoJaZNXD39WUTohysDdt2M+69pYx7bxkbtu2me6u63HBWR844\npinJSRV8kMnhqtkIOp4TFoDcHFj35b5kccUseOv/geeG7XXb5E8Wm3aF1HI8z6OIiFQ65XFAyq+A\ne81sJDADWAnkFGxkZlcCVwK0bt26LOOTyLL123jorSVMmbWcnXty+WGnxlzZ9wiOa1uv4o88Li1J\nydC4U1iOje6W2L0NVn+6L2H8+gOY+2TUPuqOTkwY6x+h7mgRESk15a5buUD7msDn7r7fR06oW7ls\nfbp8I6NnLObFuatJSUrigh7NueKkTDo0KWePt6vItnyTUF2cCas+gd1bw7aMOuHexbxksUVPdUfL\nIVG3sogUpqwrhx8BHcysHaEiOAS4JLGBmTUENrh7LnAjYeSyxCw313njyzU88OZiPliygVoZKVx1\n8hGM6t2WxrXV7VniajWFTueGBUJ39Nov9iWLK2fBW3ft646u1zZ/sqjuaBEROURlmhy6e7aZXQ28\nTJjKZoy7zzOzW4GZ7v4McArwFzNzQrfyL8oyRslvd3YuT89eyYNvLebLb7fSrE4GN5/TiSHHt6Zm\nenm8K6GSSkqGJkeH5djLwrrd22DV7H0J49fvwdypUftUaNo5IWHMggZHgLr7RUTkAMq0W7m0qFu5\n5G3euYeJH3zNI+8s4dvNu+jYtBZXnZzJuV2bk5qs+93Krc2r81cX83VH180/lU6LnlCjQbzxSqzU\nrSwihVHpR/JZvWkHj7yzlIkffM3WXdn0ad+AOy7qRt8ODTXIpCKo3Qxqnwedzgvvc3Ng7ef7ksWV\ns2DGnQnd0e3yJ4zNukJKenzxi4hI7JQcCgCff7OZ0TMW88zsVTjQL3q8XecWevRbhZaUDE2OCUvP\nEWHdrq2weva+AS/L3i3QHd2lwOjoTHVHi4hUIUoOqzB3573F63ngzcW8+eVaqqUmc+kP2vCTE9vR\nqn4lfrxdVZdeMzwDuu2J+9ZtXpV/7sVPJsCHo8O2avW+Pzq6ev14YhcRkVKn5LAKys7J5cW53zB6\nxmI+W7mJhjXT+NUZR3LpD9pQt3oVeryd7FO7ORzdPyxQoDs6ShgX3bGvO7p+Zv6EsWkXdUeLiFQS\nGpBShWzfnc0THy3n4XeWsHzDDjIb1uDykzK58NgWZKRW0cfbSfHt2pJ/dPTKWbBlddiWnBYSxMTq\norqjyz0NSBGRwig5rALWbd3FuHeXMu79ZWzcvoeebepxZd9MftSpCUlV/fF2cng2rUxIFj8Oo6P3\nbAvbqtUvMDr6WHVHlzNKDkWkMOpWrsSWrNvGg28t5slZK9idk8sPOzXhqr6ZZLXVL2gpIXVahOXo\n88P7nOzQHZ1YXXzj30D0R2j9zPxzLzbtAim6lUFEpDxRclgJffz1d4x+czEvz/+G1OQkBh7bgstP\nyuSIRjXjDk0qu+SUMPl2087Qc2RYt2tLqCjmJYtLZsBnT0Tt08LTXPaOju4ZptdRd7SISGzUrVxJ\n5OY6r32+hgdmLOKjpd9ROyOF4Se0YUTvtjSupceoSTniDptXhkQxcbLuPdvD9uoN9g12adFT3dGl\nSN3KIlIYVQ4ruF3ZOUz/ZCWjZyxm0dpttKhbjd+eezSDj2tFDT3eTsojM6jTMiz5uqMX5B8d/dWr\n7OuOPiJ/dbGJuqNFREqLKocV1KYde5jwwTIeeWcpa7fs4uhmtbnq5Ez6dWmmx9tJ5bBzc6go5iWL\nK2fC1m/DtuT08DSXxNHR9dqqO/ogqXIoIoVRcljBrNy4gzFvL2Hyh1+zbXcOJ3VoyFV9j6BP+wZ6\nvJ1Ubnnd0YnVxVWfQPaOsD2xO7plz/C6Wr14Yy7nlByKSGHU71hBzF+1mQffWsyzn4bH253XtRlX\n9M3kmOZ6vJ1UEYnd0cdcENblZMOa+fmri4nd0Q3a568uNums7mgRkQNQ5bAcc3feWbieB2Ys4q2v\n1lEjLZkhx7fmxye2o0XdanGHJ1I+7dwMqz7eN/fi97qju+1LFltmQd02VbY7WpVDESmMksNyKDsn\nl+c/W83oGYuZt2ozjWqlM6pPW4Yd34Y61VPjDk+kYnGHTSvyz724anZCd3TDhMm687qj68YbcxlR\ncigihVG3cjmybVc2j3+0nIffXsLKjTs4olENbh/YhQt6tCA9RY+3EzkkZlC3VViOGRDW5ewJ3dF5\nyeKKmfDVK+zrju6Qv7rYpDMkl/8/zDZu3MjEiRP5+c9/HlsMZtYDuNrdf2LhRui7gX7AdmCku39c\nyD5Dgd8QvgGrgEvdfZ2ZXQz8HugEHO/uM6P2w4DrEw7RFTjW3Web2RtAMyDK/jnD3dccRPxvAL/K\nO9fBMLMXgEvcfePB7luMYy9197YlcJw3OIjPZ2bpwDigJ7AeGOzuS83sFML3c+RBnHss8Jy7TzWz\n64DR7r794D7B4TGz7kBzd3+hhI63FMhy93UHaHcn4d/BC8A2YKu733UI57sA+NLd5yesuwb4BZAD\nPO/u/5uwrTUwH/i9u99lZmnAv4HT3D27qPMoOSwH1mzZyaPvLuWx979m0449HNe2Hn/ofwyndWys\nx9uJlIbk1NC93KwbHPeTsG7npvyTdS/8D3w6KWxLyQhtEwe7lMPu6I0bN/Kvf/0rluTQzFKiXza/\nAW6LVp8NdIiWXsB90dd8+xESyKOjhPAO4GpCUjgXuBB4IHEfd58ATIj27wJMd/fZCU2GHUpyd7jc\nvV9Zn7MM/AT4zt3bm9kQ4HZgcAkc9zrgMcIfDWWpO5BFSNKKJeFn+3BcCdR39xwz+/1hHOcC4DlC\nwoeZnQqcD3Rz911m1rhA+78BL+a9cffdZvYfwvdwQlEn0ZwnMVq0dis3TpvDibe/zr/eWMQJmQ2Y\n9vPeTPlpb354tJ57LFKmMupA5inQ91cwdBL86ku47jO46BE47nLAYObDMPXHcHc3uKsDTBwMb94J\ni16DHSVeLDpoN9xwA4sWLaJ79+5cf30orN15550cd9xxdO3ald/97ncALF26lE6dOgG0MbN5ZvaK\nmVUDMLNrzWy+mc0xs8nRuvpmNj1a976ZdY3W/97MxpvZO8B4M6sFdHX3T6OQzgfGefA+UNfMmhUI\n26KlRlRprE2oHuLuC9z9iwN87KHA5EO9ZmZWzcwmm9kCM3sKqJaw7Qwze8/MPjazKWZW08zOMrMp\nCW1OMbPnotdLzaxh9Pqy6Hp9ambjo3WNzOxJM/soWvocRKhrE875azP7LDr2X6N1b5hZVvS6YVTR\nOtDnu8/MZkY/A38o4rznA49Gr6cCp0ffp93Apv0FbMG9ZvaFmf0baBytvxZoDrxuZq+b2Y/N7B8J\n+11hZn83s7Zm9rmZTYjin2pm1aM2Pc3sTTObZWYvF/JzVVg8acCtwGAzm21mgw/iZzvZzO4ys7lR\n22sSDn1N9DPymZl1LOS8zwA1gVlmNrjAtu7ReeeY2VNmVi/hGnwUfY+fNLPqZtYb6A/cGcV/BPAz\n4K/uvgsgsUpuocq4BJhXIKTpwLD9Xix3r/BLz549vSL5aMl6v/zRj7ztDc95h5te8N9Mm+OL126N\nOywROZDs3e4rP3H/8EH3aT91vyfL/Xe19y33ZIX1Hz4Y2mXvLtPwlixZ4sccc8ze9y+//LJfccUV\nnpub6zk5OX7OOef4m2++6UuWLPHk5GQH5nm47/wJQlcuhMQsPXpdN/p6D/C76PVpwOzo9e+BWUC1\n6P2pwJMe/d9MqHCcmPD+P4QuuHz/hwMXAZuB1cAMILnA9jcK2y/atgjoXKDtZ8Bs4Baie+uLWoD/\nBsZEr7sC2YTKUsMolhrRtl8DvyX0uH2dsP6+hGu3NNrvGOBLoGG0vn70dWLe9QBaAwsSrtvsQpZ3\nC4n3bOBdoHqBY++9RlEMS/f3+Qrsmxzt3zV6fyvQP3o9F2hZ4Ho33N81TWh7IfBqdPzmwEbgosRr\nFb2uGR03NXr/LtAFaEu41aBPtH4M8CsgNWrTKFo/OOEzXl/EtfxntH0kcG9CjMX92f4ZITlOKXDt\nlgLXRK9/DjxUxLXYmvD694SufYA5wMkJ1/0f0esGCe1vSzjH2LxrGL2fDfwB+AB4Ezgu4Zq+F33d\ne76E7/fa/X3v1K1cRnJznVcXfMvoGYuZtew76lZP5ZpT23NZ77Y0rJked3giUhzJqdC8e1iOuzys\n27lp36joFbNg4avw6cSwLSUDmnWP7l2M5mCs27rMuqNfeeUVXnnlFXr06AHA1q1b+eqrr2jdujXt\n2rVj4cKFefflzSL8Iobwy2qCmU0nVBgATgQGArj7a2bWwMxqR9uecfe84zQjocJVHGaWSvjF2wNY\nTPhlfSP7uqb3t28vYLu7z01YPczdV0ZVzCeB4YR75orSF/gngLvPMbM50fofAEcD74RCGWnAe+6e\nbWYvAeeZ2VTgHOB/CxzzNGCKR/ehufuGaP0PgaNt3/e/tpnVdPfXCd2dxfFD4BGP7tVLOPbBfj6A\nQWZ2JSHhbRZ93jnu/ttixnIgfYFJ7p4DrDKz1wpr5O5bo23nmtkCQpL4mZm1BZa7+ztR08eAa4GX\ngM7Aq9G1TCb8YYG73wnceRAxFvdn+4fA/R51Lxe47tOir7MICXGxmFkdwh9gb0arHgXyqtKdzew2\noC4hwXu5iMOkAPUJP6/HAU+YWSYhIfx7dG3z7eCha3u3mdVy9y1FHVRK0c49OUz7eCUPvbWYxeu2\n0bJeNf7Q/xguzmpJ9TRdfpEKL6MOHHFqWCCMjt74df65F2c+DO//X9heo3H+ZLHFseEYpcDdufHG\nG7nqqqvyrV+6dCnp6fn+KM1hX3fjOYRf6ucBN1m4p29/tiW83gEkPsx9JdAq4X3LaF2i7lGsiwDM\n7AnghgOcM88QYFLiCndfGX3dYmYTgePZf3JYFANedfehhWybTLgvcgMws6hfsIVIAn7g7jvznSjc\nN/b3Qtpvd/fexTx2NvtuFcvYX8PonO0IVbjj3P07C4NFCtsv73u4wsL9oXUIA1NK2kOE+1U/Bx5J\nWF9wShUnfG/mufsJBQ9iZtdTeJfpDHe/9iBj2nbgJgDsir7mUHJ51VjgAnf/1MxGAqcU0W4FMM1D\nSfBDM8slVI57ARdZuIe3LpBrZjvd/d5ov3RgZ6FHRMlhqdm4fTePvb+Mse8uY93WXXRpUYd7hvbg\n7M5NSdHj7UQqLzOo1yYsnQeGdTl74Nu5+ede/DLvHnGDhkfmHx3d+OhDGh1dq1YttmzZl6eceeaZ\n3HLLLQwbNoyaNWuycuVKUlOLPq6ZJQGt3P11M3ubkHzVBN4i/ML9o4VRquvcfXPBigSwAPifhPfP\nAFdbuHexF7DJ3VcX2GcloZrWyN3XAj+KjrNfUayDgJMS1qUQKjHroorkuYSRmZjZAMKI5xsLHGoG\ncAnwmpl1JnS9ArwP/J+ZtXf3hWZWA2jh7l8Suu/GAFdQ+P2OrwFPmdnf3H29mdWPKk2vANcQVbbM\nrLu7zz7IyuGrwG/NbIK7b0849lLCiOIPCd30B/p8tQnJzyYza0Lorn6jkPM9A4wgdFFeBLwWJSJ7\nmdnxhBHqlxXYdwZwlZk9Srjf8FRC1zrAFqAWkFdd/cDMWgHHJsQI0NrMTnD396LP8TbwBdAob330\nvT7S3ecVo3KYd948xf3ZfjX6LK9H1eO8637I3H2TmX1nZie5+1uEKndeFbEWsDr6bMPY90dVwfin\nE67r62Z2JKHCvc7dE/9d/J7QrX1v9L5B1GZPUbEpOSxhyzds5+G3l/DEzOVs353DKUc14sq+mZyQ\nqcfbiVRZyanQvEdY8uzYGE3WHVUXv3wZZkeDB1OqfX+y7jqtDtgd3aBBA/r06UPnzp05++yzufPO\nO1mwYAEnnBAKLDVr1uSxxx4jObnIqbGSgcei7i4j3Ke1MfrlMibqktxOSBa+x90/N7M6Cd1VLxCm\n71gY7Tcqr62ZzXb37u6+ysJgiBlmtgdYRrgvLC+huwdoBDwf7XNmdIi+hC7HxQkhpAMvR79QkwmJ\n4YPRtiMI9zUWdB/wSNSduYDQNYi7r40qNpMsTOcCcDNhGpEcC4NQRhZ2Ldx9npn9CXjTzHKAT6K2\n1xISzjmE378zgJ8Wdi2L4u4vWZiOZaaZ7SZc498AdxG6FK8Eni/G5/vUzD4hVOqWA3ldt5jZrYSK\n6DPAw4QBGQsJldIhhYTVmn1TByV6itDFPp9wn+Z7CdtGAy+Z2Sp3j8ruPAF0d/fvEtp9AfzCzMZE\nx7nPw4jbi4B/Rj+rKcA/+P7Ai8K8DtxgZrOBvxC6Xw/4s02obB4JzIl+Th8E7i2iLRYGB/3U3S8/\nQDwjgPstDLRZzL5/I7cQ7iNcG33NSwgnAw9aGNRzEeGPlDFmNpcwSGhEweS9EKeS/2fk+/Ef+Bjl\nX3mYBHvuyk2MnrGY5z9bjQH9uzfnyr6ZdGxa+4D7ioiE7uhl+edeXP0p5EQ9VjUa508Wmx8LGYf3\n/4uVwiTYZvZfwBZ3f6gkj3u4zOwx4L+i6qSUIAtz+I139zkHbLz/4zxHuE/uP9H7toR5ETsfdpCy\nl5lNA26IquCFUuXwMLg7b321jgdmLOKdheupmZ7CT05sx6g+bWlWR4+3E5GDYAb12oalS9QrmL0b\n1szLnzB+kTc9m0Gjo6DXVZD145iCLtR9wMVxB1GQu18adwyVlbtff+BWRTOzuoTu8E/zEkMpHRam\n85m+v8QQlBwekj05uTw3ZxWjZyxhwerNNKmdzg1nd+SSXq2pnVH+n6IgIhVESlpCd/QVYd2O76L7\nFqNk0crX05OiwRbj445DKg4PT5Q5spD1SwmjkqWEuPtuijFAS8nhQdi6K5vJH37NmLeXsGrTTv5/\ne/ceI2dVxnH8+3NpLbRIb9A2pS2Llig0gFgqCFZIBSpRGxLUIrcaBS9gJFGMokGpRI1EoyZcFeUq\nWJFLg6BFQTASoIgFSm3LWqu0FFfKnQq49PGPc3Z8nc6yszD7vsvO75NM9r2cmffp2bOZp+95zzl7\nTBrDOUfvzYJ9pzJyOw8yMbMSbD8O3jIvvczMBoGTwyZ0P/MCP71zPVfc9XeefaGHd3aO5+yjZnHI\nHl7ezszMzIaX0pNDSfNJ62h2kGYS/3bd+emkiSDH5jJfihYtkD1QXd3PctEd67j+z4/Ss3Ur75s1\nhZPn7s4+08ZWEY6ZmZnZoCs1OZTUAZxLmsdqA7Bc0tKIWFUo9lVgSUScL2lP0jD93cqKMSJYvv5J\nLrz9r/xudTejRryBj+w/jU+8u5MZE0aXFYaZmZlZJcq+czgH6OqdlypPjLqANHdRryBNzglpJvZH\nywjs5a3Bsoce48I71rHikacYP3okp713JiccuBvjR48sIwQzMzOzypWdHE4lTbbZawNp1vyirwPL\nJH0WGE1az3AbeaLP4oKRtQAACbdJREFUkwGmT5/+qgN64T8vc82fNvDjP6xj/eYtzJiwA99YsBdH\nv2Ma248cWqMAzczMzAbbUByQcgxwSUR8V9KBpJnZZ0XE1mKhiLiINMM6s2fPflUzed+6+p+c/osH\n2Pz8S+wzbSznzX8rR+w1mQ4PMjEzM7M2VXZy2Mwi7B8H5gPkNRNHkRaR7m51MDMmjGbfaWM5ee7u\nzOkc7+XtzMzMrO2VPTnfcmCmpM48S/dC0qLeRf8A5gFIehswirS2YMu9eecxXLxof97pdY/NzMzM\ngJKTw4joAU4FfkNaAHxJXqB8saQP5mKfB06SdD9wFbCoiUWkzczMzKwFSn/mMM9ZeFPdsTML26uA\ng8qOy8zMzMzK71Y2MzMzsyHMyaGZmZmZ1Tg5NDMzM7MaJ4dmZmZmVuPk0MzMzMxqnByamZmZWY2G\nwxSCkv4F/P1Vvn0i8HgLw2mVoRoXDN3YHNfAOK6BGY5xzYiInVsZjJm9/g2L5PC1kHRvRMyuOo56\nQzUuGLqxOa6BcVwD47jMrF24W9nMzMzMapwcmpmZmVmNk0O4qOoA+jBU44KhG5vjGhjHNTCOy8za\nQts/c2hmZmZm/+M7h2ZmZmZW4+TQzMzMzGqGbXIo6SeSuiWt7OO8JP1QUpekByTtVzh3oqSH8+vE\nkuM6NsfzoKQ7Je1TOLc+H18h6d5WxtVkbIdIejpff4WkMwvn5ktak+vzSyXGdHohnpWSXpY0Pp8b\ntPqSNE3SbZJWSXpI0ucalCm9jTUZV+ltrMm4qmhfzcRVVRsbJekeSffn2M5qUOaNkn6e6+VuSbsV\nzn05H18j6YhWxmZmw1xEDMsXMBfYD1jZx/kjgZsBAQcAd+fj44F1+ee4vD2uxLje1Xs94H29ceX9\n9cDECuvsEODGBsc7gL8CuwMjgfuBPcuIqa7sB4Bby6gvYAqwX97eEVhb/2+uoo01GVfpbazJuKpo\nX/3GVWEbEzAmb48A7gYOqCvzGeCCvL0Q+Hne3jPX0xuBzlx/HYMRp19++TX8XsP2zmFE3AE88QpF\nFgCXRXIXMFbSFOAI4JaIeCIingRuAeaXFVdE3JmvC3AXsGurrt2fJuqsL3OArohYFxEvAVeT6rfs\nmI4BrmrFdfsTEZsi4r68/SzwF2BqXbHS21gzcVXRxpqsr74MZvsaaFxltrGIiOfy7oj8qh9BuAC4\nNG9fA8yTpHz86oh4MSL+BnSR6tHMrF/DNjlswlTgkcL+hnysr+NV+DjpzlOvAJZJ+pOkkyuK6cDc\nzXWzpL3yscrrTNIOpATrl4XDpdRX7sp7O+nOTlGlbewV4ioqvY31E1dl7au/+qqijUnqkLQC6Cb9\nh6LPNhYRPcDTwASGwN+kmb1+bVd1ANaYpENJX9wHFw4fHBEbJe0C3CJpdb6zVpb7SGuxPifpSOB6\nYGaJ138lHwD+GBHFu4yDXl+SxpCShdMi4plWfvZr0UxcVbSxfuKqrH01+XssvY1FxMvAvpLGAtdJ\nmhURDZ+/NTNrlXa+c7gRmFbY3zUf6+t4aSTtDfwYWBARm3uPR8TG/LMbuI6Su4ki4pnebq6IuAkY\nIWkiQ6DOSM9b/V9332DXl6QRpITiyoi4tkGRStpYE3FV0sb6i6uq9tVMfWWlt7HCdZ4CbmPbxw9q\ndSNpO2AnYDND42/SzF6n2jk5XAqckEeUHgA8HRGbgN8Ah0saJ2kccHg+VgpJ04FrgeMjYm3h+GhJ\nO/Zu57hKvYMgaXJ+nglJc0jtZzOwHJgpqVPSSNKX6NIS49oJeA9wQ+HYoNZXroeLgb9ExPf6KFZ6\nG2smriraWJNxld6+mvw9VtXGds53DJG0PXAYsLqu2FKgd7T70aTBMpGPL8yjmTtJd2DvaVVsZja8\nDdtuZUlXkUY/TpS0Afga6YFuIuIC4CbSaNIuYAvwsXzuCUnfIH0hASyu60Ya7LjOJD0zdF7+nuyJ\niNnAJFK3EqTf288i4tetiqvJ2I4GPi2pB/g3sDB/EfVIOpWU4HQAP4mIh0qKCeAoYFlEPF9462DX\n10HA8cCD+ZkwgDOA6YXYqmhjzcRVRRtrJq7S21eTcUE1bWwKcKmkDlKivCQibpS0GLg3IpaSEtvL\nJXWRBm4tzHE/JGkJsAroAU7JXdRmZv3y8nlmZmZmVtPO3cpmZmZmVsfJoZmZmZnVODk0MzMzsxon\nh2ZmZmZW4+TQzMzMzGqcHFpbkrRIUvTxeqrCuC7JU/aYmZlVYtjOc2jWpA+R1p0t6qkiEDMzs6HA\nyaG1uxUR0VV1EGZmZkOFu5XN+lDoep4r6XpJz0naLOncvJxZsewUSZdJelzSi5IekHRcg8/slHS5\npMdyuXWSftCg3Nsl/UHSFkkPS/pU3fnJki6V9Gj+nE2SbpS0S+trwszM2onvHFq765BU/3ewNSK2\nFvavAJYA5wFzSMvPjQYWQW1d3duBcaSl1x4BjiMta7ZDRFyUy3WS1rfdkj/jYdIybYfXXf9NwM+A\n7wOLScvunS9pTUTclstcDswATs/XmwTMA3Z4tRVhZmYGTg7NVjc49ivg/YX9myLiC3l7maQAFkv6\nZkSsJSVvM4FDI+L3udzNkiYBZ0u6OK9rexawPbBPRDxa+PxL666/I/CZ3kRQ0h3AEcAxQG9yeCBw\nRkRcWXjfL5r+V5uZmfXByaG1u6PYdkBK/WjlJXX7VwNnk+4irgXmAhsLiWGvK4CfAnsCD5LuEN5Y\nlxg2sqVwh5CIeFHSWtJdxl7LgdMlCbgVWBleKN3MzFrAyaG1u5VNDEj5Zx/7U/PP8cCmBu97rHAe\nYALbJqKNPNng2IvAqML+R4CvAV8kdT9vknQBcHZdl7iZmdmAeECKWf8m9bG/Mf98Apjc4H2TC+cB\nHud/CeVrEhHdEXFKREwF3gpcQuq2/mQrPt/MzNqXk0Oz/n24bn8hsBW4O+/fDuwq6aC6ch8FuoFV\neX8Z8H5JU1oZXESsiYgzSHccZ7Xys83MrP24W9na3b6SJjY4fm9h+0hJ55CSuzmk7tzLIuLhfP4S\n4HPAtZK+Quo6PhY4DPhkHoxCft+RwJ2Svgl0ke4kzo+Ibaa96YuknYDfAleSBtT8B1hAGi29rNnP\nMTMza8TJobW7vkb47lzYPg74PPBp4CXgR0Dv6GUi4nlJ7wG+A3ybNNp4DXB8RFxRKLde0gGkwSzf\nAsaQuqZvGGDMLwD3ASeRprPZmq93bEQM9LPMzMz+jzzA0awxSYtIo41nehUVMzNrF37m0MzMzMxq\nnByamZmZWY27lc3MzMysxncOzczMzKzGyaGZmZmZ1Tg5NDMzM7MaJ4dmZmZmVuPk0MzMzMxq/gsi\nZpQKS2r0YQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "outputId": "e039a5f6-cfa3-4b81-aa06-55597a23cf61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "real_results, predicted_ys = batch_wise_evaluate(textual_entailment_model, \n",
        "         test_fact_loader,\n",
        "         Hyperparameters)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "outputId": "11cf68ec-cdbd-4b49-db34-257fde51873e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "print(predicted_ys.cpu().shape)\n",
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu(), real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6102])\n",
            "Evaluation for: textual entailment model\n",
            "Classifier 'textual entailment model' has Acc=0.609 P=0.609 R=0.609 F1=0.608 AUC=0.659\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.612     0.588     0.599      3041\n",
            "         1.0      0.606     0.630     0.617      3061\n",
            "\n",
            "    accuracy                          0.609      6102\n",
            "   macro avg      0.609     0.609     0.608      6102\n",
            "weighted avg      0.609     0.609     0.608      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1787 1134]\n",
            " [1254 1927]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6087805666583546,\n",
              " 0.6085842392883666,\n",
              " 0.6086529006882989,\n",
              " 0.6084467890788837)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUa9XwZ-4Xny",
        "colab_type": "text"
      },
      "source": [
        "##Baseline Sentence Entailment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9tLOcRYOc9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    combined = premise_embedding * hypothesis_embedding\n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8fbaSIM4jhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineHyperparameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 30\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  num_classes = 1\n",
        "  epochs = 4\n",
        "  C = 0.3\n",
        "  is_debug = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKfpJ-Hk4ePj",
        "colab_type": "code",
        "outputId": "c7ef4c7f-0f64-4145-a667-f1c52b7b3cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "baseline_model = BaselineSentenceEntailment(Hyperparameters, small_gloves).cuda()\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(baseline_model.parameters(), \n",
        "                                    lr=0.01)\n",
        "\n",
        "base_loss, base_accuracy = train(model=baseline_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.638712\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.625161\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.636640\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.526138\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.480699\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.346422\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.229483\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.228674\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.120491\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.027766\n",
            "Average loss is: tensor(1.3835, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7605597527472527\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.981206\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.954562\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.910219\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.785806\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.719956\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.633391\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.618916\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.475919\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.401287\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.344776\n",
            "Average loss is: tensor(0.6707, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9752747252747253\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.334622\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.304926\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.264800\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.227195\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.221624\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.202312\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.205138\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.207914\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.189135\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.162585\n",
            "Average loss is: tensor(0.2282, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9921445741758241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQ16nQW6Xb7",
        "colab_type": "code",
        "outputId": "b031da13-14e7-48d0-c446-0c1cf96e572a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, base_loss, base_accuracy)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwY6ih2k9hGy",
        "colab_type": "code",
        "outputId": "d6b621b4-14b9-4eaa-810e-a3661d1ec606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "base_real_results, baseline_predicted_ys = batch_wise_evaluate(baseline_model, \n",
        "         test_fact_loader,\n",
        "         Hyperparameters)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlPE-kpx9riu",
        "colab_type": "code",
        "outputId": "f360f230-b499-4749-92dd-b061ca12edfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "evaluation_summary(\"baseline model\", baseline_predicted_ys.cpu(), base_real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: baseline model\n",
            "Classifier 'baseline model' has Acc=0.640 P=0.641 R=0.641 F1=0.640 AUC=0.666\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.668     0.614     0.639      3178\n",
            "         1.0      0.614     0.668     0.640      2924\n",
            "\n",
            "    accuracy                          0.640      6102\n",
            "   macro avg      0.641     0.641     0.640      6102\n",
            "weighted avg      0.642     0.640     0.640      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1950  971]\n",
            " [1228 1953]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6407687354554349,\n",
              " 0.6407570558189468,\n",
              " 0.6396263520157326,\n",
              " 0.6396262649090981)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gmzMChW2xxT",
        "colab_type": "text"
      },
      "source": [
        "##DECLARE BASELINE :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3qhGP9Y-Cj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(101, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    mean_embeddings = torch.unsqueeze(torch.sum(embeddings, 1) / self.hp.max_length, 1) #change to accurate size of lenfgth\n",
        "  \n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100\n",
        "    main_embeddings = torch.cat((mean_embeddings, added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    attention_weights = softmax(self.premise_linear(processed_premise))#TODO: turn into row vector\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis,attention_weights.transpose(1,2))\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    avg = torch.sum(combined, 1)/self.hp.max_length #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    output = torch.sigmoid(self.linear_final(smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2AvWEFOBYMQ",
        "colab_type": "code",
        "outputId": "3c07227c-88f7-41c4-f0a9-7794614b1cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "declare_model = BaselineDeclare(Hyperparameters, small_gloves).cuda()\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(declare_model.parameters(), \n",
        "                                    lr=0.01)\n",
        "\n",
        "declare_loss, declare_accuracy = train(model=declare_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.640852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-6393b01d8cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrms_optimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                        using_gradient_clipping=True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-36771d8ad14d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;31m#woah we gotta do this to do backprop!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_debug\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZweMG4WBnoY",
        "colab_type": "code",
        "outputId": "02bf66a8-6d51-418c-a3d8-4acd95b1ac25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, declare_loss, declare_accuracy)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWZOTQzMzMzNr+i+PCta/ne0m2gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sN7kk2TqdE",
        "colab_type": "code",
        "outputId": "53eba204-2703-4344-bc06-498b458f3c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "declare_real_results, declare_predicted_ys = batch_wise_evaluate(declare_model, \n",
        "         test_fact_loader,\n",
        "         Hyperparameters)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeJHn9s8T1bk",
        "colab_type": "code",
        "outputId": "00c1eadb-14ed-4eae-9d5f-d2415518e978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "evaluation_summary(\"declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: declare model\n",
            "Classifier 'declare model' has Acc=0.621 P=0.620 R=0.620 F1=0.620 AUC=0.672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.598     0.605     0.601      2886\n",
            "         1.0      0.642     0.635     0.638      3216\n",
            "\n",
            "    accuracy                          0.621      6102\n",
            "   macro avg      0.620     0.620     0.620      6102\n",
            "weighted avg      0.621     0.621     0.621      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1746 1175]\n",
            " [1140 2041]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6196813156170221,\n",
              " 0.6198144542360959,\n",
              " 0.6206161914126516,\n",
              " 0.6197274106471953)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0JKeQxFT5kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE_jCwcNtVnh",
        "colab_type": "text"
      },
      "source": [
        "##New DeCLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msT39pfytW8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = 2*self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 1)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(101, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    mean_embeddings = torch.unsqueeze(torch.sum(embeddings, 1) / self.hp.max_length, 1) #change to accurate size of lenfgth\n",
        "  \n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((mean_embeddings.repeat(1, 100, 1), added_embeddings), 2)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    attention_weights = softmax(self.premise_linear(processed_premise))#TODO: turn into row vector\n",
        "    print(attention_weights.shape)\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    print(processed_hypothesis.shape)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, attention_weights.transpose(1,2)).transpose(1,2)\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    avg = torch.sum(combined, 1)/self.hp.max_length #todo: fix padding\n",
        "    print(avg.shape)\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    output = torch.sigmoid(self.linear_final(smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKIZlt47tn1s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "43febb8b-ad10-4ec0-8aa8-9b492c3d535c"
      },
      "source": [
        "declare_real_model = RealDeclare(Hyperparameters, small_gloves).cuda()\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(declare_real_model.parameters(), \n",
        "                                    lr=0.01)\n",
        "\n",
        "declare_loss, declare_accuracy = train(model=declare_real_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "torch.Size([256, 100, 1])\n",
            "torch.Size([256, 500, 100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-2c6fe34cfaf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrms_optimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                        using_gradient_clipping=True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-36771d8ad14d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m#get y values - do forward pass and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mpredicted_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mactual_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0msqueezed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-50dfc53402b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise, hypothesis)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#matrix multiply of the two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_hypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m#final processing - another average, and then a relu + sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;31m#todo: fix padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 6: wrong matrix size at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:534"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z27nLF9IuB6Z",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, declare_loss, declare_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m4y58jizv8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}