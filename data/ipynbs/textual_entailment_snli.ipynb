{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "67f687c0-4265-4663-ab53-dc91dfb2bcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-26 17:12:06--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  19.4MB/s    in 7.8s    \n",
            "\n",
            "2020-01-26 17:12:15 (11.6 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "034f594b-fd4d-4ecd-96fb-f4276f9b6545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-26 17:12:25--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-01-26 17:12:25--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-01-26 17:12:26--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.01MB/s    in 6m 30s  \n",
            "\n",
            "2020-01-26 17:18:56 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b81e83f7-60e5-45d1-d3a0-88f03ae3d4ce"
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-26 17:19:20--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  2.21MB/s    in 2.1s    \n",
            "\n",
            "2020-01-26 17:19:23 (2.21 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "61c71dd6-6d68-4ec8-a491-ff9c79b69595"
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "60ceb6dc-370b-47af-9f00-6a5727f5081b"
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-26 17:19:36--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  2.43MB/s    in 2.2s    \n",
            "\n",
            "2020-01-26 17:19:39 (2.43 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ff90b0d-57f5-4b0b-c154-2de8b236b12f"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "5098e5e0-c70e-443f-e3c3-f968064c4b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "bd98fcb6-c1d5-4c0c-b4f5-8b4fe9e11ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ... Stance\n",
              "0        0  ...      2\n",
              "1        0  ...      2\n",
              "2        0  ...      2\n",
              "3        0  ...      2\n",
              "4        0  ...      2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "outputId": "b0768cbb-8e17-42c0-e01a-7ee86e1732aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_id\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  train_unique, test_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_id\"].isin(test_unique[\"claim_id\"])]\n",
        "  train_facts = facts[facts[\"claim_id\"].isin(train_unique[\"claim_id\"])]\n",
        "  return train_facts, test_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n",
        "\n",
        "train_facts.head(500)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>for firms moving overseas in order to create a...</td>\n",
              "      <td>foxnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>get a tax break specifically by outsourcing jo...</td>\n",
              "      <td>newslines.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>confusing clashes over taxes in wednesday s pr...</td>\n",
              "      <td>wsj.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>support on this bill in a time of tight budget...</td>\n",
              "      <td>senate.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2012_oct_08_barack-obama_obama-says-tax-code-r...</td>\n",
              "      <td>federal tax code loopholes giving incentives c...</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>tax a lower rate for american manufacturing an...</td>\n",
              "      <td>archives.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jan_08_cory-booker_booker-mcconnell-full-...</td>\n",
              "      <td>2009 mitch mcconnell person thats saying hey g...</td>\n",
              "      <td>cory booker</td>\n",
              "      <td>to have a different standard the senate will h...</td>\n",
              "      <td>thedailybeast.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jan_08_cory-booker_booker-mcconnell-full-...</td>\n",
              "      <td>2009 mitch mcconnell person thats saying hey g...</td>\n",
              "      <td>cory booker</td>\n",
              "      <td>double standard for mcconnell to be less conce...</td>\n",
              "      <td>weaselzippers.us</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jan_08_cory-booker_booker-mcconnell-full-...</td>\n",
              "      <td>2009 mitch mcconnell person thats saying hey g...</td>\n",
              "      <td>cory booker</td>\n",
              "      <td>cory booker on government reform even billiona...</td>\n",
              "      <td>ontheissues.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_feb_15_benjamin-netanyahu_benjamin-netany...</td>\n",
              "      <td>past weeks president donald trump pointed iran...</td>\n",
              "      <td>benjamin netanyahu</td>\n",
              "      <td>think are deeply committed to do and we are ob...</td>\n",
              "      <td>whitehouse.gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_feb_15_benjamin-netanyahu_benjamin-netany...</td>\n",
              "      <td>past weeks president donald trump pointed iran...</td>\n",
              "      <td>benjamin netanyahu</td>\n",
              "      <td>are deeply committed to do and we are obviousl...</td>\n",
              "      <td>haaretz.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cred_label  ...     article_source\n",
              "0             1  ...        foxnews.com\n",
              "1             1  ...      newslines.org\n",
              "2             1  ...            wsj.com\n",
              "3             1  ...         senate.gov\n",
              "4             1  ...       archives.gov\n",
              "..          ...  ...                ...\n",
              "554           1  ...  thedailybeast.com\n",
              "555           1  ...   weaselzippers.us\n",
              "556           1  ...    ontheissues.org\n",
              "557           1  ...     whitehouse.gov\n",
              "558           1  ...        haaretz.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "adf756b9-bc31-4589-d3b5-5cb3521b4da1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>morning new tv advertisement argues that posit...</td>\n",
              "      <td>desmoinesregister.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>iowans could not support household on current ...</td>\n",
              "      <td>americanprogressaction.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>not budged in five years leaving many falling ...</td>\n",
              "      <td>americanprogressaction.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>are working to support themselves or their fam...</td>\n",
              "      <td>iowademocrats.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_aug_01_bruce-braley_iowa-senate-candidate...</td>\n",
              "      <td>says us senate candidate joni ernst not suppor...</td>\n",
              "      <td>bruce braley</td>\n",
              "      <td>prove that i had those good hardworking skills...</td>\n",
              "      <td>iowademocrats.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>0</td>\n",
              "      <td>2008_oct_02_joe-biden_mccain-refused-to-commit...</td>\n",
              "      <td>john mccain said wouldnt even sit government s...</td>\n",
              "      <td>joe biden</td>\n",
              "      <td>cap is melting palin im not one to attribute e...</td>\n",
              "      <td>mysinchew.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>0</td>\n",
              "      <td>2008_oct_02_joe-biden_mccain-refused-to-commit...</td>\n",
              "      <td>john mccain said wouldnt even sit government s...</td>\n",
              "      <td>joe biden</td>\n",
              "      <td>control the weapons the theocracy does secreta...</td>\n",
              "      <td>blastmagazine.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>0</td>\n",
              "      <td>2008_oct_02_joe-biden_mccain-refused-to-commit...</td>\n",
              "      <td>john mccain said wouldnt even sit government s...</td>\n",
              "      <td>joe biden</td>\n",
              "      <td>be left with only one conclusion mccain was co...</td>\n",
              "      <td>chrisweigant.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2850</th>\n",
              "      <td>0</td>\n",
              "      <td>2009_may_19_mike-pence_120-million-deprived-he...</td>\n",
              "      <td>democrats propose health care plan deprive rou...</td>\n",
              "      <td>mike pence</td>\n",
              "      <td>speech politifact called claim false that demo...</td>\n",
              "      <td>democrats.org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2851</th>\n",
              "      <td>0</td>\n",
              "      <td>2009_may_19_mike-pence_120-million-deprived-he...</td>\n",
              "      <td>democrats propose health care plan deprive rou...</td>\n",
              "      <td>mike pence</td>\n",
              "      <td>covering conduct going back as far as 1994 was...</td>\n",
              "      <td>democrats.org</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...              article_source\n",
              "85             1  ...       desmoinesregister.com\n",
              "86             1  ...  americanprogressaction.org\n",
              "87             1  ...  americanprogressaction.org\n",
              "88             1  ...           iowademocrats.org\n",
              "89             1  ...           iowademocrats.org\n",
              "...          ...  ...                         ...\n",
              "2766           0  ...               mysinchew.com\n",
              "2767           0  ...           blastmagazine.com\n",
              "2768           0  ...            chrisweigant.com\n",
              "2850           0  ...               democrats.org\n",
              "2851           0  ...               democrats.org\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 256\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_fact_list = convert_to_lists({\"claim_text\": train_facts[\"claim_text\"], \n",
        "                   \"claim_source\": train_facts[\"claim_source\"],\n",
        "                   \"article\": train_facts[\"article\"],\n",
        "                   \"article_source\": train_facts[\"article_source\"]})\n",
        "y_train_fact_list = train_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_fact_list = convert_to_lists({\"claim_text\": test_facts[\"claim_text\"], \n",
        "                   \"claim_source\": test_facts[\"claim_source\"],\n",
        "                   \"article\": test_facts[\"article\"],\n",
        "                   \"article_source\": test_facts[\"article_source\"]})\n",
        "y_test_fact_list = test_facts[\"cred_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "x_train_snopes_list = convert_to_lists({\"claim_text\": train_snopes[\"claim_text\"],\n",
        "                   \"article\": train_snopes[\"article\"],\n",
        "                   \"article_source\": train_snopes[\"article_source\"]})\n",
        "x_test_snopes_list = convert_to_lists({\"claim_text\": test_snopes[\"claim_text\"],\n",
        "                   \"article\": test_snopes[\"article\"],\n",
        "                   \"article_source\": test_snopes[\"article_source\"]})\n",
        "y_train_snopes_list = train_snopes[\"cred_label\"].tolist()\n",
        "y_test_snopes_list = test_snopes[\"cred_label\"].tolist()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "fact_tokeniser = Tokeniser(x_train_fact_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "snopes_tokeniser = Tokeniser(x_train_snopes_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_fact_train = fact_tokeniser.do_everything(x_train_fact_list)\n",
        "x_fact_test = fact_tokeniser.do_everything(x_test_fact_list)\n",
        "y_fact_train = np.array(y_train_fact_list, dtype=np.float32)\n",
        "y_fact_test = np.array(y_test_fact_list, dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n",
        "x_snopes_train = snopes_tokeniser.do_everything(x_train_snopes_list)\n",
        "x_snopes_test = snopes_tokeniser.do_everything(x_test_snopes_list)\n",
        "y_snopes_train = np.array(y_train_snopes_list, dtype=np.float32)\n",
        "y_snopes_test = np.array(y_test_snopes_list, dtype=np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"claim_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article_source\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "train_fact_source_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_source_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_fact_source_loader.name = \"fact_data\"\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "train_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_train).type(torch.LongTensor))\n",
        "test_fact_data = data_utils.TensorDataset(torch.from_numpy(x_fact_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_fact_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_fact_test).type(torch.LongTensor))\n",
        "\n",
        "train_fact_loader = data_utils.DataLoader(train_fact_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_fact_loader.name = \"fact_data\"\n",
        "\n",
        "\n",
        "test_fact_loader = data_utils.DataLoader(test_fact_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test)\n",
        "test_fact_loader.name = \"fact_data\"\n",
        "\n",
        "train_snopes_data = data_utils.TensorDataset(torch.from_numpy(x_snopes_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_snopes_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_snopes_train).type(torch.LongTensor))\n",
        "test_snopes_data= data_utils.TensorDataset(torch.from_numpy(x_snopes_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_snopes_test[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_snopes_test).type(torch.LongTensor))\n",
        "train_snopes_loader = data_utils.DataLoader(train_snopes_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_snopes_loader.name = \"fact_data\"\n",
        "\n",
        "test_snopes_loader = data_utils.DataLoader(test_snopes_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test)\n",
        "test_snopes_loader.name = \"fact_data\"\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, test_data, tokeniser):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.test_data = test_data\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(x)\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=20)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "    else:\n",
        "      x = self.linear1(x.reshape(self.hp.batch_size, -1))\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-skRc_EBRhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, unnormalised_predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(unnormalised_predictions.shape) == 1):\n",
        "    auc = roc_auc_score(true_labels, unnormalised_predictions)\n",
        "  else:\n",
        "    auc = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    combined = premise_embedding * hypothesis_embedding\n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db8ikkk64Kx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def load_embeddings(self, word_embeddings):\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, self.hp)\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(101, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    mean_embeddings = torch.unsqueeze(torch.sum(embeddings, 1) / self.hp.max_length, 1) #change to accurate size of lenfgth\n",
        "  \n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100\n",
        "    main_embeddings = torch.cat((mean_embeddings, added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    attention_weights = softmax(self.premise_linear(processed_premise))#TODO: turn into row vector\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis,attention_weights.transpose(1,2))\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    avg = torch.sum(combined, 1)/self.hp.max_length #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    output = torch.sigmoid(self.linear_final(smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def load_embeddings(self, word_embeddings):\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, self.hp)\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = 2*self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 1)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 50)\n",
        "    self.linear_almost_there = torch.nn.Linear(50, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    mean_embeddings = torch.unsqueeze(torch.sum(embeddings, 1) / self.hp.max_length, 1) #change to accurate size of lenfgth\n",
        "  \n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((mean_embeddings.repeat(1, 100, 1), added_embeddings), 2)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    attention_weights = softmax(self.premise_linear(processed_premise.transpose(1,2)))#TODO: turn into row vector\n",
        "    repeated_weights = attention_weights.repeat(1, 1, 100)\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, repeated_weights)\n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    avg = torch.sum(combined, 1)/self.hp.max_length #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    output = torch.sigmoid(self.linear_final(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          train_loader=None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  is_binary = hp.num_classes == 1\n",
        "  \n",
        "  if train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    correct = 0\n",
        "    penal = 0\n",
        "    for batch_index, train_data in enumerate(train_loader):\n",
        "      #setting everything up\n",
        "      model.hidden_state = model.init_hidden()\n",
        "      train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "      predicted_y, attention = model(*train_data[:-1])\n",
        "      actual_y = train_data[-1]\n",
        "      squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "      if hp.C > 0:\n",
        "        attentionT = attention.transpose(1,2)\n",
        "        identity = torch.eye(attention.size(1))\n",
        "        identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "        penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "      if is_binary:\n",
        "        loss = loss_function(squeezed_y, actual_y.double())\n",
        "        loss += hp.C * penal/train_loader.batch_size\n",
        "        correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "      else:\n",
        "        loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/train_loader.batch_size)\n",
        "        correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "      total_loss += loss.data\n",
        "\n",
        "      #cleaning up regularisation\n",
        "      if hp.C > 0:\n",
        "        del(penal)\n",
        "        del(identity)\n",
        "        del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "      optimiser.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      if hp.is_debug and batch_index % 10 == 0:\n",
        "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "            epoch, batch_index * len(train_data[0]), len(train_loader.dataset),\n",
        "            100. * batch_index / len(train_loader), loss.item()\n",
        "        ))\n",
        "\n",
        "      if using_gradient_clipping:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      batch_count += 1\n",
        "      optimiser.step()\n",
        "      free_data(train_data)\n",
        "\n",
        "    print(\"Average loss is:\",total_loss/batch_count)\n",
        "    correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "    accuracy = correct_but_numpy / float(batch_count * train_loader.batch_size)\n",
        "    print(\"Accuracy of the model\", accuracy)\n",
        "    losses.append(total_loss/batch_count)\n",
        "    accuracies.append(accuracy)\n",
        "  return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(real_results, 0), torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, accuracies=None, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  if accuracies:\n",
        "    plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Accuracy\")\n",
        "    plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp):\n",
        "  runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "  bce_loss = torch.nn.BCELoss()\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(runnable_model.parameters(), lr=0.01)\n",
        "  losses, accuracies = train(model=runnable_model,\n",
        "                       train_loader=dataset.train_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=True)\n",
        "  plot_stuff(hp.epochs, losses, accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "  results, predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         dataset.test_loader,\n",
        "         hp)\n",
        "  return results, predicted_ys\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, unnormalised_predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(unnormalised_predictions.shape) == 1):\n",
        "    auc = roc_auc_score(true_labels, unnormalised_predictions)\n",
        "  else:\n",
        "    auc = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "#RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "outputId": "0a192808-64d3-4ea7-a690-267b1512b7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": Dataset(train_fact_loader, test_fact_loader, y_fact_test, fact_tokeniser),\n",
        "    \"snopes\": Dataset(train_snopes_loader, test_snopes_loader, y_snopes_test, snopes_tokeniser)\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    \"broke_declare\": BaselineDeclare,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36905\n",
            "44784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 3\n",
        "  dropout=0.3\n",
        "  C = 0.3\n",
        "  is_debug = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "c5218df0-81c5-4d17-c3b8-7f5bf118be8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "real_results, predicted_ys = run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters)\n",
        "\n",
        "#textual_entailment_model.to(device)\n",
        "\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642364\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.639016\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.640057\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.633217\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.516710\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.356792\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.231255\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.109308\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.095685\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.094225\n",
            "Average loss is: tensor(1.4044, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7306833791208791\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.020336\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.047250\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.005036\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.945435\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.916769\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.889260\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.842966\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.826155\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.811110\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.832166\n",
            "Average loss is: tensor(0.9236, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9817135989010989\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.814999\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.842293\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.826168\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.814992\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.804128\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.794293\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.753690\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.746051\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.746545\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.742944\n",
            "Average loss is: tensor(0.7877, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9962654532967034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndW2y/C5YGKIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "outputId": "43bf0307-2311-4be1-e228-938cc837bc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu(), real_results.cpu(), datasets[\"politifact\"].test_data)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment model\n",
            "Classifier 'textual entailment model' has Acc=0.606 P=0.607 R=0.607 F1=0.606 AUC=0.652\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.628     0.582     0.604      3152\n",
            "         1.0      0.586     0.632     0.608      2950\n",
            "\n",
            "    accuracy                          0.606      6102\n",
            "   macro avg      0.607     0.607     0.606      6102\n",
            "weighted avg      0.607     0.606     0.606      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1834 1087]\n",
            " [1318 1863]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6067660270170123, 0.606689107803493, 0.6058669288757784, 0.6058580265337487)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c1b6efc-ea2f-42c7-89b9-79633e4e6a00"
      },
      "source": [
        "sheena_real_results, sheena_predicted_ys = run_model(models[\"sheena_model\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.634782\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.577712\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.526435\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.464170\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.395243\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.358070\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.283384\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.255302\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.220125\n",
            "Average loss is: tensor(1.4339, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7340745192307693\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.225889\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.142469\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.066053\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.008116\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.937962\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.924249\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.944152\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.800182\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.799103\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.821914\n",
            "Average loss is: tensor(0.9531, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9429086538461539\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.787225\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.774756\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.764513\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.739024\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.703506\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.682828\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.708794\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.675782\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.684684\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.680631\n",
            "Average loss is: tensor(0.7276, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9904275412087912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "outputId": "0ad22ff2-7e3e-4af2-89f5-a0e120f75b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "evaluation_summary(\"sheena model\", sheena_predicted_ys.cpu(), sheena_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: sheena model\n",
            "Classifier 'sheena model' has Acc=0.602 P=0.606 R=0.611 F1=0.599 AUC=0.670\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.716     0.566     0.632      3690\n",
            "         1.0      0.497     0.655     0.565      2412\n",
            "\n",
            "    accuracy                          0.602      6102\n",
            "   macro avg      0.606     0.611     0.599      6102\n",
            "weighted avg      0.629     0.602     0.606      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[2090  831]\n",
            " [1600 1581]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6062609526501122, 0.61093415038628, 0.6016060308095706, 0.5988145390913826)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "outputId": "264bc64d-f33d-4dfc-940b-36ca37d5fde0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "broke_real_results, broke_predicted_ys = run_model(models[\"broke_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.673615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.644456\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.639757\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.632728\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.618737\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.554347\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.457232\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.485178\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.365099\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.328158\n",
            "Average loss is: tensor(1.5524, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6514852335164835\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.320772\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.197011\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.180315\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.237347\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.208557\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.195512\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.222981\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.185680\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.156586\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.193329\n",
            "Average loss is: tensor(1.2131, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8936298076923077\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.048643\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.082463\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.081549\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.072126\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.065783\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.031796\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.015339\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.091562\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.080656\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.049078\n",
            "Average loss is: tensor(1.0539, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9615384615384616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "74f7cd04-85be-420a-b038-fb4e38eacb63"
      },
      "source": [
        "evaluation_summary(\"broke declare model\", broke_predicted_ys.cpu(), broke_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: broke declare model\n",
            "Classifier 'broke declare model' has Acc=0.637 P=0.635 R=0.636 F1=0.635 AUC=0.686\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.599     0.626     0.612      2797\n",
            "         1.0      0.671     0.646     0.658      3305\n",
            "\n",
            "    accuracy                          0.637      6102\n",
            "   macro avg      0.635     0.636     0.635      6102\n",
            "weighted avg      0.638     0.637     0.637      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1751 1170]\n",
            " [1046 2135]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6353124148097318,\n",
              " 0.6360094049329923,\n",
              " 0.6368403802032121,\n",
              " 0.6353964742528827)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JetqyT7Imn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e86afb6-22be-40b9-ca8b-612ae837ce04"
      },
      "source": [
        "declare_real_results, declare_predicted_ys =  run_model(models[\"real_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.662348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.640913\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.629185\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.625781\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.519659\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.479595\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.390187\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.437586\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.302833\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.321990\n",
            "Average loss is: tensor(1.4992, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6912774725274725\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.195601\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.193872\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.220343\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.157109\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.160525\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.164178\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.154400\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.193675\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.174979\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.143737\n",
            "Average loss is: tensor(1.1787, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9096840659340659\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.087388\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.994660\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.026641\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.015412\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.095105\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.079442\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.990060\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.043791\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.028615\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.073409\n",
            "Average loss is: tensor(1.0468, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9653588598901099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAEbCAYAAACoWC5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "9e04b5af-d23a-4ca5-be8c-469464339248"
      },
      "source": [
        "evaluation_summary(\"real declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: real declare model\n",
            "Classifier 'real declare model' has Acc=0.641 P=0.639 R=0.640 F1=0.639 AUC=0.687\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.594     0.633     0.613      2742\n",
            "         1.0      0.683     0.647     0.665      3360\n",
            "\n",
            "    accuracy                          0.641      6102\n",
            "   macro avg      0.639     0.640     0.639      6102\n",
            "weighted avg      0.643     0.641     0.641      6102\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1735 1186]\n",
            " [1007 2174]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6387037744757391,\n",
              " 0.6398868135875795,\n",
              " 0.6406096361848574,\n",
              " 0.6387397948413953)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUa9XwZ-4Xny",
        "colab_type": "text"
      },
      "source": [
        "##Baseline Sentence Entailment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gmzMChW2xxT",
        "colab_type": "text"
      },
      "source": [
        "##DECLARE BASELINE :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3qhGP9Y-Cj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2AvWEFOBYMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "declare_model = BaselineDeclare(Hyperparameters, small_gloves).cuda()\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "rms_optimiser = torch.optim.Adam(declare_model.parameters(), \n",
        "                                    lr=0.01)\n",
        "\n",
        "declare_loss, declare_accuracy = train(model=declare_model,\n",
        "                       train_loader=train_fact_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = rms_optimiser,\n",
        "                       hp = Hyperparameters,\n",
        "                       using_gradient_clipping=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZweMG4WBnoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, declare_loss, declare_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sN7kk2TqdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "declare_real_results, declare_predicted_ys = batch_wise_evaluate(declare_model, \n",
        "         test_fact_loader,\n",
        "         Hyperparameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeJHn9s8T1bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0JKeQxFT5kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE_jCwcNtVnh",
        "colab_type": "text"
      },
      "source": [
        "##New DeCLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msT39pfytW8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKIZlt47tn1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "declare_real_model = RealDeclare(Hyperparameters, small_gloves).cuda()\n",
        "run_model(declare_real_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z27nLF9IuB6Z",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_stuff(Hyperparameters.epochs, real_declare_loss, real_declare_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m4y58jizv8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "real_declare_real_results, real_declare_predicted_ys = batch_wise_evaluate(declare_real_model, \n",
        "         test_fact_loader,\n",
        "         Hyperparameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJH7fF2NBAwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_summary(\"real declare model\", real_declare_predicted_ys.cpu(), real_declare_real_results.cpu(), y_fact_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1HWg0fKjLyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO3eB5WlSG9r",
        "colab_type": "text"
      },
      "source": [
        "##GET ALL RESULTS A BUNCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b60e8230-b7b1-4882-c241-bcc65a7e1b8b"
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    for avg_i in range(avg_amount):\n",
        "      some_results = []\n",
        "      for per_avg_i in range(per_avg_amount):\n",
        "        results, predicted_ys = run_model(models[model_name], datasets[data_name], Hyperparameters)\n",
        "        full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), results.cpu(), datasets[data_name].test_data))\n",
        "        some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), results.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "      processed_results.append(process_results(list_to_dict(some_results)))\n",
        "      print(processed_results)\n",
        "      #avg_results.append(get_avgs(some_results))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640008\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.639256\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.638881\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.633644\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.590716\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.571471\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.509672\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.419613\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.413082\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.319499\n",
            "Average loss is: tensor(1.5423, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6494247939560439\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.237126\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.176343\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.071484\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.037076\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.978480\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.990531\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.998183\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.985040\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.948741\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.914665\n",
            "Average loss is: tensor(1.0383, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9481456043956044\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.901720\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.885312\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.865430\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.871156\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.853450\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.838050\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.834300\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.845588\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.803542\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.779829\n",
            "Average loss is: tensor(0.8539, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9944196428571429\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641692\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.629551\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.587500\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.586831\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.477129\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.252690\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.217466\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.130888\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.106286\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.042880\n",
            "Average loss is: tensor(1.3684, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7583705357142857\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.991398\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.961707\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.905155\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.924699\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.887116\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.918206\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.855397\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.834721\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.796181\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.781149\n",
            "Average loss is: tensor(0.8880, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9833447802197802\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.775015\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.760755\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.740992\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.732472\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.735754\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.712624\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.694279\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.683980\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.669347\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.667476\n",
            "Average loss is: tensor(0.7210, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9969522664835165\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642971\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.638637\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.636305\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.569513\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.465651\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.294029\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.210987\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.130211\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.110510\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.026742\n",
            "Average loss is: tensor(1.3832, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7519745879120879\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.980667\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.958879\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.923197\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.886564\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.849066\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.838288\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.825012\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.799027\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.804755\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.845207\n",
            "Average loss is: tensor(0.8717, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9812843406593407\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.721087\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.702431\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.677441\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.674769\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.632187\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.602460\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.594018\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.561912\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.564774\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.604446\n",
            "Average loss is: tensor(0.6352, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9956644917582418\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.636834\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.637359\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.635852\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.595746\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.474002\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.389592\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.217972\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.170202\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.066242\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.022257\n",
            "Average loss is: tensor(1.3916, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7291380494505495\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.007953\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.926875\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.926713\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.935638\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.924001\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.935018\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.903092\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.871377\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.872131\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.899841\n",
            "Average loss is: tensor(0.9278, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9806833791208791\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.892158\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.905070\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.862383\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.877389\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.860893\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.859138\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.857117\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.865740\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.855969\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.859261\n",
            "Average loss is: tensor(0.8690, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.995836195054945\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.644926\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.644979\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.641486\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.621895\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.593166\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.574826\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.575441\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.426394\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.398249\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.231054\n",
            "Average loss is: tensor(1.5303, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6465916895604396\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.243314\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.207038\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.105692\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.106965\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.107434\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.023073\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.980387\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.965434\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.940365\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.956235\n",
            "Average loss is: tensor(1.0590, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9312757554945055\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.906012\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.885783\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.854325\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.839879\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.783178\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.739181\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.711914\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.731437\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.684012\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.629434\n",
            "Average loss is: tensor(0.7697, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.990470467032967\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.643637\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.639104\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.613046\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.562992\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.439176\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.327020\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.146943\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.175648\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.108453\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.057172\n",
            "Average loss is: tensor(1.3703, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7476820054945055\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.969016\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.004632\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.937199\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.934270\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.926376\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.903836\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.908076\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.891831\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.946742\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.879266\n",
            "Average loss is: tensor(0.9254, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9827008928571429\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.866695\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.855332\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.852383\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.857487\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.869264\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.865198\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.812120\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.797450\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.808950\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.750187\n",
            "Average loss is: tensor(0.8352, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9973385989010989\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.646384\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.643772\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.640206\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.635454\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.636390\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.607748\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.503437\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.384232\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.211813\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.139928\n",
            "Average loss is: tensor(1.5079, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6498111263736264\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.039519\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.054334\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.008794\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.925789\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.957499\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.892685\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.864306\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.868350\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.832986\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.861014\n",
            "Average loss is: tensor(0.9315, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9722699175824175\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.816069\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.799975\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.775806\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.758762\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.751107\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.766550\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.744137\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.693585\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.695421\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.658494\n",
            "Average loss is: tensor(0.7465, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9955786401098901\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642786\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.642419\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.580928\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.554081\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.320944\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.287228\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.112429\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.112112\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.040706\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.022801\n",
            "Average loss is: tensor(1.3419, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7701751373626373\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.024481\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.938921\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.928165\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.889925\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.843708\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.821972\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.879971\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.862272\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.791262\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.777038\n",
            "Average loss is: tensor(0.8754, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9832160027472527\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.776870\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.749486\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.736799\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.737197\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.725860\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.673721\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.680498\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.660693\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.634165\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.604277\n",
            "Average loss is: tensor(0.6947, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9956644917582418\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.637190\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.636052\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.566527\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.577638\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.378493\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.260593\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.275223\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.120023\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.083317\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.059627\n",
            "Average loss is: tensor(1.3485, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7885903159340659\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.041178\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.012854\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.971514\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.948149\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.936411\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.871423\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.847793\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.800478\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.787926\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.755460\n",
            "Average loss is: tensor(0.8973, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9855339972527473\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.758256\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.731781\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.715872\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.702157\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.689455\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.721336\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.679149\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.682579\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.658766\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.671809\n",
            "Average loss is: tensor(0.7000, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9972956730769231\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640386\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.635670\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.621053\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.448866\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.279000\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.239348\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.154850\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.087019\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 0.978148\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.019931\n",
            "Average loss is: tensor(1.3125, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7900927197802198\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.921358\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.892123\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.873307\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.918401\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.910297\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.900149\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.888560\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.854257\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.857579\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.828355\n",
            "Average loss is: tensor(0.8914, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9841603708791209\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.812704\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.852949\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.803059\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.820785\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.802192\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.812276\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.800374\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.799539\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.820617\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.802871\n",
            "Average loss is: tensor(0.8130, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.996351304945055\n",
            "[(0    my_model\n",
            "1    my_model\n",
            "2    my_model\n",
            "3    my_model\n",
            "4    my_model\n",
            "5    my_model\n",
            "6    my_model\n",
            "7    my_model\n",
            "8    my_model\n",
            "9    my_model\n",
            "Name: model_name, dtype: object, 0    politifact\n",
            "1    politifact\n",
            "2    politifact\n",
            "3    politifact\n",
            "4    politifact\n",
            "5    politifact\n",
            "6    politifact\n",
            "7    politifact\n",
            "8    politifact\n",
            "9    politifact\n",
            "Name: dataset_name, dtype: object, precision    0.626346\n",
            "recall       0.626996\n",
            "accuracy     0.625287\n",
            "f1           0.624753\n",
            "auc          0.664768\n",
            "dtype: float64, precision    0.013277\n",
            "recall       0.012918\n",
            "accuracy     0.014339\n",
            "f1           0.014282\n",
            "auc          0.016045\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.653025\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.639845\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.619824\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.486395\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.352474\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.206577\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.234904\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.001665\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.019833\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 0.961880\n",
            "Average loss is: tensor(1.3321, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.770260989010989\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.939253\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.965555\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.888023\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.872853\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.880081\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.777085\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.744470\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.706366\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.678844\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.637222\n",
            "Average loss is: tensor(0.8089, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9831301510989011\n",
            "Running EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-8b795561aa13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0msome_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mper_avg_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_avg_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mfull_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msome_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-148-405a75d74305>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, dataset, hp)\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        using_gradient_clipping=True)\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mplot_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-16474b806858>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss_function, optimiser, hp, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0;31m#woah we gotta do this to do backprop!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_debug\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}