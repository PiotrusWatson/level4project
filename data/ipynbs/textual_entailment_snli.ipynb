{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textual entailment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/textual_entailment_snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4NbGwIC9-z",
        "colab_type": "text"
      },
      "source": [
        "##HAHA ITS TIME TO SPEND 5 HOURS DOWNLOADING THINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NYKgSzNCZO",
        "colab_type": "text"
      },
      "source": [
        "lets get the snli dataset baybee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oddcFXt8M-gL",
        "colab_type": "code",
        "outputId": "29ac85c8-7a6c-450a-851a-6601b34d1106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-10 17:54:23--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  69.0MB/s    in 1.3s    \n",
            "\n",
            "2020-02-10 17:54:25 (69.0 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4-cOWalFcJ",
        "colab_type": "code",
        "outputId": "ab83d602-bdd5-4b59-ce42-6732e4f40343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-10 17:54:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-10 17:54:33--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-10 17:54:33--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.07MB/s    in 6m 26s  \n",
            "\n",
            "2020-02-10 18:00:59 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "ad4f0249-f6aa-4e5d-97f6-afd425dd0f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-10 18:01:23--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  3.75MB/s    in 1.3s    \n",
            "\n",
            "2020-02-10 18:01:25 (3.75 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjuOCbTuMC9",
        "colab_type": "code",
        "outputId": "34f9e594-4c9b-469f-c237-574c532dd20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/FakeNewsChallenge/fnc-1.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fnc-1'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 49\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcNEOBx94jF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NsutKljq6J",
        "colab_type": "code",
        "outputId": "392dc9e3-ce40-4706-b118-44cd2e2f3321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the Snopes Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
        "!unzip Snopes.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-10 18:01:35--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/Snopes.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559754 (5.3M) [application/zip]\n",
            "Saving to: ‘Snopes.zip’\n",
            "\n",
            "Snopes.zip          100%[===================>]   5.30M  4.24MB/s    in 1.2s    \n",
            "\n",
            "2020-02-10 18:01:37 (4.24 MB/s) - ‘Snopes.zip’ saved [5559754/5559754]\n",
            "\n",
            "Archive:  Snopes.zip\n",
            "   creating: Snopes/\n",
            "  inflating: Snopes/README           \n",
            "  inflating: Snopes/snopes.tsv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7BNxcOlee",
        "colab_type": "text"
      },
      "source": [
        "Some imports lol :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9P3r8j78KeG",
        "colab_type": "code",
        "outputId": "10fe3958-f73d-4a82-ec1c-eeab691e68fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlJXKPPODFqp",
        "colab_type": "text"
      },
      "source": [
        "##LOOK AT ALL THIS CODE TO IMPORT DATA GOD THERE MUST BE SOMETHING WRONG WITH ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPUlCQyAOUxX",
        "colab_type": "code",
        "outputId": "495125f8-9080-4d35-f400-a886460c1024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch,keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import math\n",
        "\n",
        "np.random.seed(128)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVr0NUjsQ7Vt",
        "colab_type": "text"
      },
      "source": [
        "lets load this shit :^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaA9gj9kPLcu",
        "colab_type": "code",
        "outputId": "04182dc5-f5f0-45d3-819f-91a8ca9380b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_dataframe.head(50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3416050480.jpg#4r1n</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3416050480.jpg#4r1c</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3416050480.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3416050480.jpg#4r1e</td>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2267923837.jpg#2r1n</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
              "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2267923837.jpg#2r1e</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>( There ( ( are children ) present ) )</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2267923837.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2267923837.jpg#2r1c</td>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
              "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
              "      <td>The kids are frowning</td>\n",
              "      <td>( ( The kids ) ( are frowning ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3691670743.jpg#0r1c</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy skates down the sidewalk.</td>\n",
              "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3691670743.jpg#0r1e</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy does a skateboarding trick.</td>\n",
              "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3691670743.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3691670743.jpg#0r1n</td>\n",
              "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
              "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
              "      <td>The boy is wearing safety equipment.</td>\n",
              "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1n</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An older man drinks his juice as he waits for ...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( drinks ( his juic...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#0r1c</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A boy flips a burger.</td>\n",
              "      <td>( ( A boy ) ( ( flips ( a burger ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[entailment, neutral, entailment, neutral, neu...</td>\n",
              "      <td>4804607632.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#0r1e</td>\n",
              "      <td>An older man sits with his orange juice at a s...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( ( sits ( with ( ( h...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>An elderly man sits in a small shop.</td>\n",
              "      <td>( ( An ( elderly man ) ) ( ( sits ( in ( a ( s...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#4r1n</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>Some women are hugging on vacation.</td>\n",
              "      <td>( ( Some women ) ( ( are ( hugging ( on vacati...</td>\n",
              "      <td>(ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#4r1c</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>The women are sleeping.</td>\n",
              "      <td>( ( The women ) ( ( are sleeping ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#4r1e</td>\n",
              "      <td>Two blond women are hugging one another.</td>\n",
              "      <td>( ( Two ( blond women ) ) ( ( are ( hugging ( ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (JJ blond) (NNS women)) ...</td>\n",
              "      <td>There are women showing affection.</td>\n",
              "      <td>( There ( ( are ( women ( showing affection ) ...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#2r1n</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are eating omelettes.</td>\n",
              "      <td>( ( The people ) ( ( are ( eating omelettes ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#2r1c</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The people are sitting at desks in school.</td>\n",
              "      <td>( ( The people ) ( ( are ( sitting ( at ( desk...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#2r1e</td>\n",
              "      <td>A few people in a restaurant setting, one of t...</td>\n",
              "      <td>( ( ( A ( few people ) ) ( in ( ( ( a ( restau...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ few) (NNS people))...</td>\n",
              "      <td>The diners are at a restaurant.</td>\n",
              "      <td>( ( The diners ) ( ( are ( at ( a restaurant )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4804607632.jpg#3r1e</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man is drinking juice.</td>\n",
              "      <td>( ( A man ) ( ( is ( drinking juice ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4804607632.jpg#3r1c</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>Two women are at a restaurant drinking wine.</td>\n",
              "      <td>( ( Two women ) ( ( are ( at ( a ( restaurant ...</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4804607632.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4804607632.jpg#3r1n</td>\n",
              "      <td>An older man is drinking orange juice at a res...</td>\n",
              "      <td>( ( An ( older man ) ) ( ( is ( ( drinking ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...</td>\n",
              "      <td>A man in a restaurant is waiting for his meal ...</td>\n",
              "      <td>( ( ( A man ) ( in ( a restaurant ) ) ) ( ( is...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4850814517.jpg#1r1n</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man getting a drink of water from a fo...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( getting ( ( a drin...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4850814517.jpg#1r1c</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man wearing a brown shirt is reading a...</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( wearing ( a ( brown ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4850814517.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4850814517.jpg#1r1e</td>\n",
              "      <td>A man with blond-hair, and a brown shirt drink...</td>\n",
              "      <td>( ( ( ( ( ( A man ) ( with blond-hair ) ) , ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man)) (PP (IN with) (...</td>\n",
              "      <td>A blond man drinking water from a fountain.</td>\n",
              "      <td>( ( ( A ( blond man ) ) ( ( drinking water ) (...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#0r1c</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends scowl at each other over a full di...</td>\n",
              "      <td>( ( The friends ) ( ( scowl ( at ( ( each othe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#0r1e</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>There are two woman in this picture.</td>\n",
              "      <td>( There ( ( are ( ( two woman ) ( in ( this pi...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>4705552913.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#0r1n</td>\n",
              "      <td>Two women who just had lunch hugging and sayin...</td>\n",
              "      <td>( ( ( Two women ) ( who ( just ( had ( lunch (...</td>\n",
              "      <td>(ROOT (NP (NP (CD Two) (NNS women)) (SBAR (WHN...</td>\n",
              "      <td>The friends have just met for the first time i...</td>\n",
              "      <td>( ( The friends ) ( ( ( ( ( ( have just ) ( me...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4705552913.jpg#3r1n</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>The two sisters saw each other across the crow...</td>\n",
              "      <td>( ( The ( two sisters ) ) ( ( ( ( ( saw ( each...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4705552913.jpg#3r1c</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two groups of rival gang members flipped each ...</td>\n",
              "      <td>( ( ( Two groups ) ( of ( rival ( gang members...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
              "      <td>4705552913.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4705552913.jpg#3r1e</td>\n",
              "      <td>Two women, holding food carryout containers, hug.</td>\n",
              "      <td>( ( ( ( ( Two women ) , ) ( holding ( food ( c...</td>\n",
              "      <td>(ROOT (S (NP (NP (CD Two) (NNS women)) (, ,) (...</td>\n",
              "      <td>Two women hug each other.</td>\n",
              "      <td>( ( Two women ) ( ( hug ( each other ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3637966641.jpg#1r1n</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to score the games winning out.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( ( trying ( to score ) ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3637966641.jpg#1r1e</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is trying to tag a runner out.</td>\n",
              "      <td>( ( A team ) ( ( is ( trying ( to ( ( tag ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3637966641.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3637966641.jpg#1r1c</td>\n",
              "      <td>A Little League team tries to catch a runner s...</td>\n",
              "      <td>( ( A ( Little ( League team ) ) ) ( ( tries (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NNP Little) (NNP League) ...</td>\n",
              "      <td>A team is playing baseball on Saturn.</td>\n",
              "      <td>( ( A team ) ( ( is ( ( playing baseball ) ( o...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>3636329461.jpg#0r1c</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school hosts a basketball game.</td>\n",
              "      <td>( ( A school ) ( ( hosts ( a ( basketball game...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>3636329461.jpg#0r1n</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A high school is hosting an event.</td>\n",
              "      <td>( ( A ( high school ) ) ( ( is ( hosting ( an ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>3636329461.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>3636329461.jpg#0r1e</td>\n",
              "      <td>The school is having a special event in order ...</td>\n",
              "      <td>( ( The school ) ( ( is ( ( having ( ( a ( spe...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN school)) (VP (VBZ is...</td>\n",
              "      <td>A school is hosting an event.</td>\n",
              "      <td>( ( A school ) ( ( is ( hosting ( an event ) )...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>4934873039.jpg#0r1c</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women do not care what clothes they wear.</td>\n",
              "      <td>( ( The women ) ( ( ( do not ) ( care ( ( what...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#0r1e</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>Women are waiting by a tram.</td>\n",
              "      <td>( Women ( ( are ( waiting ( by ( a tram ) ) ) ...</td>\n",
              "      <td>(ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>[neutral, contradiction, neutral, neutral, ent...</td>\n",
              "      <td>4934873039.jpg#0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#0r1n</td>\n",
              "      <td>High fashion ladies wait outside a tram beside...</td>\n",
              "      <td>( ( High ( fashion ladies ) ) ( ( ( wait ( out...</td>\n",
              "      <td>(ROOT (S (NP (JJ High) (NN fashion) (NNS ladie...</td>\n",
              "      <td>The women enjoy having a good fashion sense.</td>\n",
              "      <td>( ( The women ) ( ( enjoy ( having ( a ( good ...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#1r1n</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A child with mom and dad, on summer vacation a...</td>\n",
              "      <td>( ( ( A child ) ( with ( ( mom and ) dad ) ) )...</td>\n",
              "      <td>(ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#1r1e</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the beach.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#1r1c</td>\n",
              "      <td>A man, woman, and child enjoying themselves on...</td>\n",
              "      <td>( ( ( A ( man ( , ( woman ( , ( and child ) ) ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN man) (, ,) (NN woman)...</td>\n",
              "      <td>A family of three is at the mall shopping.</td>\n",
              "      <td>( ( ( A family ) ( of three ) ) ( ( is ( at ( ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>4934873039.jpg#2r1n</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>The people waiting on the train are sitting.</td>\n",
              "      <td>( ( ( The people ) ( waiting ( on ( the train ...</td>\n",
              "      <td>(ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[contradiction, entailment, contradiction, ent...</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1c</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people just getting on a train</td>\n",
              "      <td>( There ( are ( people ( just ( getting ( on (...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>4934873039.jpg#2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>4934873039.jpg#2r1e</td>\n",
              "      <td>People waiting to get on a train or just getti...</td>\n",
              "      <td>( ( People ( ( ( waiting ( to ( get ( on ( a t...</td>\n",
              "      <td>(ROOT (NP (NP (NNS People)) (VP (VP (VBG waiti...</td>\n",
              "      <td>There are people waiting on a train.</td>\n",
              "      <td>( There ( ( are ( people ( waiting ( on ( a tr...</td>\n",
              "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#3r1e</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing with a young child outside.</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing ( with ( a ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>[neutral]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2946464027.jpg#3r1n</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple are playing frisbee with a young chil...</td>\n",
              "      <td>( ( A couple ) ( ( are ( ( playing frisbee ) (...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#3r1c</td>\n",
              "      <td>A couple playing with a little boy on the beach.</td>\n",
              "      <td>( ( ( A couple ) ( playing ( with ( ( a ( litt...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN couple)) (VP (VBG pla...</td>\n",
              "      <td>A couple watch a little girl play by herself o...</td>\n",
              "      <td>( ( A couple ) ( ( ( ( watch ( a ( little ( gi...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>[contradiction]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2946464027.jpg#4r1c</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is sitting down for dinner.</td>\n",
              "      <td>( ( The family ) ( ( is ( ( sitting down ) ( f...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>[entailment]</td>\n",
              "      <td>2946464027.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2946464027.jpg#4r1e</td>\n",
              "      <td>A couple play in the tide with their young son.</td>\n",
              "      <td>( ( A couple ) ( ( play ( in ( ( the tide ) ( ...</td>\n",
              "      <td>(ROOT (S (NP (DT A) (NN couple)) (VP (VBP play...</td>\n",
              "      <td>The family is outside.</td>\n",
              "      <td>( ( The family ) ( ( is outside ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     annotator_labels  ...                                    sentence2_parse\n",
              "0                                           [neutral]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "1                                     [contradiction]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "2                                        [entailment]  ...  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...\n",
              "3                                           [neutral]  ...  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...\n",
              "4                                        [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...\n",
              "5                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NNS kids)) (VP (VBP are...\n",
              "6                                     [contradiction]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...\n",
              "7                                        [entailment]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...\n",
              "8                                           [neutral]  ...  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...\n",
              "9                                           [neutral]  ...  (ROOT (S (NP (DT An) (JJR older) (NN man)) (VP...\n",
              "10                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ flips) ...\n",
              "11  [entailment, neutral, entailment, neutral, neu...  ...  (ROOT (S (NP (DT An) (JJ elderly) (NN man)) (V...\n",
              "12                                          [neutral]  ...  (ROOT (S (NP (DT Some) (NNS women)) (VP (VBP a...\n",
              "13                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP ar...\n",
              "14                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "15                                          [neutral]  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "16  [contradiction, contradiction, contradiction, ...  ...  (ROOT (S (NP (DT The) (NNS people)) (VP (VBP a...\n",
              "17                                       [entailment]  ...  (ROOT (S (NP (DT The) (NNS diners)) (VP (VBP a...\n",
              "18                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN man)) (VP (VBZ is) (VP...\n",
              "19                                    [contradiction]  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\n",
              "20                                          [neutral]  ...  (ROOT (S (NP (NP (DT A) (NN man)) (PP (IN in) ...\n",
              "21                                          [neutral]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "22                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (JJ blond) (NN man)) (...\n",
              "23                                       [entailment]  ...  (ROOT (NP (NP (DT A) (JJ blond) (NN man)) (VP ...\n",
              "24                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VBP ...\n",
              "25                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "26      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NNS friends)) (VP (VP (...\n",
              "27                                          [neutral]  ...  (ROOT (S (NP (DT The) (CD two) (NNS sisters)) ...\n",
              "28                                    [contradiction]  ...  (ROOT (S (NP (NP (CD Two) (NNS groups)) (PP (I...\n",
              "29  [entailment, entailment, entailment, entailmen...  ...  (ROOT (S (NP (CD Two) (NNS women)) (VP (VB hug...\n",
              "30                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "31                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "32                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN team)) (VP (VBZ is) (V...\n",
              "33                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ host...\n",
              "34   [neutral, neutral, neutral, neutral, entailment]  ...  (ROOT (S (NP (DT A) (JJ high) (NN school)) (VP...\n",
              "35                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN school)) (VP (VBZ is) ...\n",
              "36                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP do...\n",
              "37                                       [entailment]  ...  (ROOT (S (NP (NNP Women)) (VP (VBP are) (VP (V...\n",
              "38  [neutral, contradiction, neutral, neutral, ent...  ...  (ROOT (S (NP (DT The) (NNS women)) (VP (VBP en...\n",
              "39                                          [neutral]  ...  (ROOT (FRAG (NP (NP (DT A) (NN child)) (PP (IN...\n",
              "40                                       [entailment]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "41                                    [contradiction]  ...  (ROOT (S (NP (NP (DT A) (NN family)) (PP (IN o...\n",
              "42                                          [neutral]  ...  (ROOT (S (NP (NP (DT The) (NNS people)) (VP (V...\n",
              "43  [contradiction, entailment, contradiction, ent...  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "44                                       [entailment]  ...  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...\n",
              "45                                       [entailment]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "46                                          [neutral]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP are)...\n",
              "47                                    [contradiction]  ...  (ROOT (S (NP (DT A) (NN couple)) (VP (VBP watc...\n",
              "48                                    [contradiction]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "49                                       [entailment]  ...  (ROOT (S (NP (DT The) (NN family)) (VP (VBZ is...\n",
              "\n",
              "[50 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIt0hMDmPQN",
        "colab_type": "text"
      },
      "source": [
        "Helper functions: something that bulk converts things into lists, and a tokeniser that also pads and numpies things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106ajZAIuYuc",
        "colab_type": "code",
        "outputId": "8dd69bc9-3289-42b3-da80-aae816a9d383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def merge_bodies(articles, claims):\n",
        "  merged = pd.merge(articles, claims, on=\"Body ID\")\n",
        "  mapping = {\"disagree\": 0, \"discuss\": 1, \"unrelated\": 2, \"agree\": 3}\n",
        "  return merged.replace({\"Stance\": mapping})\n",
        "  \n",
        "  \n",
        "train_articles = pd.read_csv(\"./fnc-1/train_bodies.csv\")\n",
        "train_claims = pd.read_csv(\"./fnc-1/train_stances.csv\")\n",
        "test_articles = pd.read_csv(\"./fnc-1/test_bodies.csv\")\n",
        "test_claims = pd.read_csv(\"./fnc-1/test_stances_unlabeled.csv\")\n",
        "\n",
        "\n",
        "train_challenge = merge_bodies(train_articles, train_claims)\n",
        "\n",
        "test_challenge = merge_bodies(test_articles, test_claims)\n",
        "train_challenge.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ... Stance\n",
              "0        0  ...      2\n",
              "1        0  ...      2\n",
              "2        0  ...      2\n",
              "3        0  ...      2\n",
              "4        0  ...      2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTfl70eyBuk",
        "colab_type": "text"
      },
      "source": [
        "also: lets load politifact :^^)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3T51bxyBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "facts = pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "facts.head(50)\n",
        "snopes = pd.read_csv(\"./Snopes/snopes.tsv\", delimiter= \"\\t\", names=['cred_label','claim_id','claim_text','article','article_source'])\n",
        "politi_mapping = {\"True\": 1, \"Half-True\": 1, \"Mostly True\": 1, \"Mostly False\": 0, \"False\": 0, \"Pants on Fire!\": 0}\n",
        "snopes_mapping = {\"true\": 1, \"half-true\": 1, \"mostly true\": 1, \"mostly false\": 0, \"false\": 0, \"pants on fire!\": 0}\n",
        "\n",
        "def slice_snopes(unique):\n",
        "  true_claims = unique[unique[\"cred_label\"] == 1]\n",
        "  false_claims = unique[unique[\"cred_label\"] == 0]\n",
        "  false_claims = false_claims.head(int(len(false_claims)/3))\n",
        "  return pd.concat([true_claims, false_claims]).sample(frac=1)\n",
        "\n",
        "\n",
        "def preprocess_fact_data(facts, mapping, slice_function=None, is_folding=False):\n",
        "  \n",
        "  facts = facts.replace({\"cred_label\": mapping})\n",
        "  unique = facts.drop_duplicates(\"claim_text\")\n",
        "  if (slice_function):\n",
        "    unique = slice_function(unique)\n",
        "  \n",
        "#splitting the claims\n",
        "  \n",
        "  if is_folding:\n",
        "    results = []\n",
        "    folded = KFold(n_splits=10, shuffle=True)\n",
        "    splitted_object = folded.split(unique)\n",
        "    for train_result, test_result in splitted_object:\n",
        "      train_ilocs = unique.iloc[train_result][\"claim_text\"]\n",
        "      test_ilocs = unique.iloc[test_result][\"claim_text\"]\n",
        "      results.append((facts[facts[\"claim_text\"].isin(train_ilocs)], facts[facts[\"claim_text\"].isin(test_ilocs)]))\n",
        "\n",
        "    return results\n",
        "\n",
        "  train_unique, big_unique = train_test_split(unique, test_size=0.2, random_state=8)\n",
        "  val_unique, test_unique = train_test_split(big_unique, test_size=0.5, random_state=8)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "#recreating dataset\n",
        "  test_facts = facts[facts[\"claim_text\"].isin(test_unique[\"claim_text\"])]\n",
        "  val_facts = facts[facts[\"claim_text\"].isin(val_unique[\"claim_text\"])]\n",
        "  train_facts = facts[facts[\"claim_text\"].isin(train_unique[\"claim_text\"])]\n",
        "  return train_facts, test_facts, val_facts\n",
        "#get unique claims to divide dataset cleanly\n",
        "train_facts, test_facts, val_facts = preprocess_fact_data(facts, politi_mapping)\n",
        "train_snopes, test_snopes, val_snopes = preprocess_fact_data(snopes, snopes_mapping, slice_snopes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-sQJe0Y-Bj",
        "colab_type": "code",
        "outputId": "c7108760-c79c-445f-b2b7-87e0b017d339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "test_facts.head(500)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cred_label</th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_source</th>\n",
              "      <th>article</th>\n",
              "      <th>article_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>vice news we dont have a timeline on the decis...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>a schedule i narcotic along with heroin and ec...</td>\n",
              "      <td>reason.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>now do you think you were maybe talking just a...</td>\n",
              "      <td>cnn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1</td>\n",
              "      <td>2014_feb_04_barack-obama_barack-obama-says-its...</td>\n",
              "      <td>isnt schedule narcotic job congress</td>\n",
              "      <td>barack obama</td>\n",
              "      <td>made to the new yorker that marijuana is no mo...</td>\n",
              "      <td>time.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>1</td>\n",
              "      <td>2017_jun_27_donald-trump_white-house-criticism...</td>\n",
              "      <td>obamacare signed law cbo estimated 23 million ...</td>\n",
              "      <td>donald trump</td>\n",
              "      <td>about the affordable health care act in its da...</td>\n",
              "      <td>eugeneweekly.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4849</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama to ban guns from 42 million social secur...</td>\n",
              "      <td>bearingarms.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4850</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>obama is looking to ban social security recipi...</td>\n",
              "      <td>rightwingnews.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4851</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>main navigation recent posts obama to ban 42 m...</td>\n",
              "      <td>downtrend.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4852</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>get news like this in your facebook news feed ...</td>\n",
              "      <td>thegatewaypundit.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4853</th>\n",
              "      <td>0</td>\n",
              "      <td>2015_jul_30_blog-posting_websites-say-obama-po...</td>\n",
              "      <td>obama makes huge move ban social security reci...</td>\n",
              "      <td>bloggers</td>\n",
              "      <td>to prove everyone wrong yet again this time it...</td>\n",
              "      <td>zerohedge.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      cred_label  ...        article_source\n",
              "187            1  ...            reason.com\n",
              "188            1  ...            reason.com\n",
              "189            1  ...               cnn.com\n",
              "190            1  ...              time.com\n",
              "526            1  ...      eugeneweekly.com\n",
              "...          ...  ...                   ...\n",
              "4849           0  ...       bearingarms.com\n",
              "4850           0  ...     rightwingnews.com\n",
              "4851           0  ...         downtrend.com\n",
              "4852           0  ...  thegatewaypundit.com\n",
              "4853           0  ...         zerohedge.com\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    #todo: modify to make it spit out a summarised version ABOUT HERE\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Beq65oTgcBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, train_loader, test_loader, val_loader, test_data, val_data, tokeniser):\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.val_loader = val_loader\n",
        "    self.test_data = test_data\n",
        "    self.val_data = val_data\n",
        "    self.word_embeddings_small = load_glove_embeddings(\"glove.6B.50d.txt\", tokeniser.word_to_id, 50) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4lYd83j-c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\"claim_text\", \"article\"]\n",
        "big_labels = [\"claim_text\", \"article\", \"article_source\"]\n",
        "\n",
        "def get_list(panda, labels):\n",
        "  label_to_data = {}\n",
        "  for label in labels:\n",
        "    label_to_data[label] = panda[label]\n",
        "  \n",
        "  x_list = convert_to_lists(label_to_data)\n",
        "  y_list = panda[\"cred_label\"].tolist()\n",
        "  return x_list, y_list\n",
        "\n",
        "def get_loader(x, y, vocab_size, max_length, batch_size, name, training=True):\n",
        "  stuff = []\n",
        "  for key in x:\n",
        "    stuff.append(torch.from_numpy(x[key]).type(torch.LongTensor))\n",
        "  stuff.append(torch.from_numpy(y).type(torch.DoubleTensor))\n",
        "\n",
        "  tensorset = data_utils.TensorDataset(*stuff)\n",
        "  loader = data_utils.DataLoader(tensorset, batch_size=batch_size, drop_last=training, shuffle=training)\n",
        "  loader.name = name\n",
        "  return loader\n",
        "\n",
        "  \n",
        "def get_dataset(train, test, val, vocab_size, max_length, batch_size, labels, name):\n",
        "  train_list_x, train_list_y = get_list(train, labels)\n",
        "  test_list_x, test_list_y = get_list(test, labels)\n",
        "  val_list_x, val_list_y = get_list(val, labels)\n",
        "\n",
        "\n",
        "\n",
        "  #tokenising various stuff, setting up numpy dictionaries :)\n",
        "  tokeniser = Tokeniser(train_list_x, vocab_size, max_length)\n",
        "  x_train = tokeniser.do_everything(train_list_x)\n",
        "  x_test = tokeniser.do_everything(test_list_x)\n",
        "  x_val = tokeniser.do_everything(val_list_x)\n",
        "  y_train = np.array(train_list_y, dtype=np.float32)\n",
        "  y_test = np.array(test_list_y, dtype=np.float32)\n",
        "  y_val = np.array(val_list_y, dtype=np.float32)\n",
        "  \n",
        "  #datasets/loaders\n",
        "  train_loader = get_loader(x_train, y_train, vocab_size, max_length, batch_size, name, True)\n",
        "  test_loader = get_loader(x_test, y_test, vocab_size, max_length, batch_size, name, False)\n",
        "  val_loader = get_loader(x_val, y_val, vocab_size, max_length, batch_size, name, False)\n",
        "  return Dataset(train_loader, test_loader, val_loader,  y_test, y_val, tokeniser)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MMrKlu6_jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JRbJ3txE02",
        "colab_type": "text"
      },
      "source": [
        "here i set up the tokeniser, and turn everything into a list its a fun cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "outputId": "8efeac61-da08-4951-fd78-3eb2fccbdc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 256\n",
        "SAMPLE_SAMPLE_SIZE = 1\n",
        "\n",
        "\n",
        "snopes_dataset = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "fact_dataset = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, labels, \"fact_data\")\n",
        "big_snopes = get_dataset(train_snopes, test_snopes, val_snopes, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "big_fact = get_dataset(train_facts, test_facts, val_facts, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE, big_labels, \"fact_data\")\n",
        "\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_train_challenge_list = convert_to_lists({\"claim_text\": train_challenge[\"Headline\"], \"article\": train_challenge[\"articleBody\"]})\n",
        "y_train_challenge_list = train_challenge[\"Stance\"].tolist()\n",
        "\n",
        "x_test_challenge_list = convert_to_lists({\"claim_text\": test_challenge[\"Headline\"], \"article\": test_challenge[\"articleBody\"]})\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39421\n",
            "33766\n",
            "45062\n",
            "37174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TbAK2zxN09",
        "colab_type": "text"
      },
      "source": [
        "this cell uses the setup tokeniser to SLAP THAT SHIT INTO NUMPY ARRAYS WITH PADDING YEAH BABY\n",
        "(also tokenises it thats p important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "challenge_tokeniser = Tokeniser(x_train_challenge_list, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n",
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n",
        "\n",
        "x_challenge_train = challenge_tokeniser.do_everything(x_train_challenge_list)\n",
        "x_challenge_test = challenge_tokeniser.do_everything(x_test_challenge_list)\n",
        "y_challenge_train = np.array(y_train_challenge_list, dtype=np.float32)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRR_2Nr-mLmn",
        "colab_type": "text"
      },
      "source": [
        "and here we slap the loaded stuff into a neat tensordataset. this is good because ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53RKo-fjxQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_shufflin = True\n",
        "shufflin_test = False\n",
        "#alright lets tensordataset textual entailment stuff\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_loader.name = \"entailment_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"entailment_data\"\n",
        "\n",
        "\n",
        "#POLITIFACT/SNOPES W/ SOURCES\n",
        "\n",
        "\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_train[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_train[\"article\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_challenge_train).type(torch.DoubleTensor))\n",
        "train_challenge_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=we_shufflin)\n",
        "train_challenge_loader.name = \"challenge_data\"\n",
        "\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_challenge_test[\"claim_text\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_challenge_test[\"article\"]).type(torch.LongTensor))\n",
        "test_challenge_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=False, shuffle=shufflin_test )\n",
        "test_loader.name = \"challenge_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smeSRlk30Ccq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifZntROetvo",
        "colab_type": "text"
      },
      "source": [
        "Helper function. I don't know why we have such a helper function but it's here.\n",
        "Does a softmax after transposing and reshaping things ??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWEGDqGSHem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(input, axis=1):\n",
        "    \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "    \"\"\"\n",
        "    input_size = input.size()\n",
        "    trans_input = input.transpose(axis, len(input_size)-1)\n",
        "    trans_size = trans_input.size()\n",
        "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "    soft_max_2d = F.softmax(input_2d)\n",
        "    soft_max_nd = soft_max_2d.view(*trans_size)  \n",
        "    return soft_max_nd.transpose(axis, len(input_size)-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNn8GSuge4zO",
        "colab_type": "text"
      },
      "source": [
        "First part of the model (split out so to test alone)\n",
        "Basically, a wrapper for an lstm\n",
        "Takes in a sequence, spits out a sequence of matrices demonstrating ~an understanding~ of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdDpyN44DQa",
        "colab_type": "text"
      },
      "source": [
        "##TEXTUAL ENTAILMENT MODEL CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0p9OyYubDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequenceProcessor(torch.nn.Module):  \n",
        "  def __init__(self, word_embeddings, hp):\n",
        "    super(SequenceProcessor, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.embedding_size = word_embeddings.size(1)\n",
        "    self.cool_lstm = torch.nn.LSTM(\n",
        "        input_size = self.embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "\n",
        "    \n",
        "  def forward(self, x, hidden_layer):\n",
        "    embedding = self.embeddings(x)\n",
        "    return self.cool_lstm(embedding,\n",
        "                          hidden_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns8HjHO-fLmw",
        "colab_type": "text"
      },
      "source": [
        "Next bit of model. Given a processed set of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwa-C0g5RapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionModel(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(AttentionModel, self).__init__()\n",
        "    self.first_linear = torch.nn.Linear(\n",
        "        in_features= 2*hp.lstm_hidden_size,\n",
        "        out_features = hp.dense_dimension,\n",
        "        bias = False\n",
        "    )\n",
        "    self.second_linear = torch.nn.Linear(\n",
        "        in_features = hp.dense_dimension,\n",
        "        out_features = hp.attention_hops,\n",
        "        bias = False\n",
        "    )\n",
        "    self.dropout = torch.nn.Dropout(p=hp.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(x)\n",
        "    tanh_W_H = torch.tanh(self.first_linear(x))\n",
        "    #[512 rows, 150 numerical words, of size 100] (512, 150, 100) <bmm> (1, 100, 100) = (512, 150, 100)\n",
        "    #another batch matrix multiply, wow!\n",
        "    weight_by_attention_hops = self.second_linear(tanh_W_H) # (100, 10) by (512, 10, 100)\n",
        "    #[512 rows, 10 attention hops of size 100] (512, 150, 100) <bmm> (1, 10, 100) = (512, 10, 150)\n",
        "    \n",
        "    attention = softmax(weight_by_attention_hops).transpose(2,1)\n",
        "    sentence_embeddings = torch.bmm(attention,x)\n",
        "    return sentence_embeddings, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oc5NYaftFW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcy-vvnSts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def better_mush(premise, hypothesis):\n",
        "    pooled_premise1 = premise[:,:,::2]\n",
        "    pooled_premise2 = premise[:,:,1::2]\n",
        "    pooled_hypothesis1 = hypothesis[:,:,::2]\n",
        "    pooled_hypothesis2 = hypothesis[:,:,1::2]\n",
        "\n",
        "    better_mush = torch.cat((pooled_premise1 * pooled_hypothesis1 + pooled_premise2 * pooled_hypothesis2,\n",
        "                               pooled_premise1 * pooled_hypothesis2 - pooled_premise2 * pooled_hypothesis1),2)\n",
        "    return better_mush\n",
        "\n",
        "class Factoriser(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(Factoriser, self).__init__()\n",
        "    self.premise_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    self.hypothesis_weight = Parameter(torch.Tensor(\n",
        "        hp.attention_hops, \n",
        "        hp.lstm_hidden_size*2,\n",
        "        hp.gravity\n",
        "        ))\n",
        "    init.kaiming_uniform_(self.premise_weight, a=math.sqrt(5))\n",
        "    init.kaiming_uniform_(self.hypothesis_weight, a=math.sqrt(5))\n",
        "\n",
        "  def batcheddot(self, a, b):\n",
        "    better_a = a.transpose(0,1)\n",
        "    bmmd = torch.bmm(better_a, b)\n",
        "    return bmmd.transpose(0,1)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "\n",
        "    premise_factor = self.batcheddot(premise, self.premise_weight)\n",
        "    hypothesis_factor = self.batcheddot(hypothesis, self.hypothesis_weight)\n",
        "    return better_mush(premise_factor,hypothesis_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkD8l2eTlNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, hp):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(\n",
        "        in_features=hp.attention_hops*hp.gravity, \n",
        "        out_features=20)\n",
        "    if hp.avg:\n",
        "      self.final_linear = torch.nn.Linear(hp.gravity, hp.num_classes)\n",
        "    else:\n",
        "      self.final_linear = torch.nn.Linear(20, hp.num_classes)\n",
        "    self.hp = hp\n",
        "  def forward(self, x):\n",
        "    if self.hp.avg:\n",
        "      x = torch.sum(x, 1)/self.hp.attention_hops\n",
        "    else:\n",
        "      x = self.linear1(x.reshape(self.hp.batch_size, -1))\n",
        "    if (self.hp.num_classes > 1):\n",
        "      x = softmax(self.final_linear(x))\n",
        "    else:\n",
        "      x = torch.sigmoid(self.final_linear(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9bayMWZAG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualEntailmentModel(torch.nn.Module):\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden_state = torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size).cuda()\n",
        "    cell_state = torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(TextualEntailmentModel, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.factoriser = Factoriser(hp)\n",
        "    self.MLP = MLP(hp)\n",
        "    self.hidden_state = self.init_hidden()\n",
        "  \n",
        "  def forward(self, premise, hypothesis):\n",
        "    processed_premise, self.hidden_state = self.premise_processor(premise, self.hidden_state)\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, self.hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    factorised_mush = self.factoriser(premise_embedding, hypothesis_embedding)\n",
        "    return self.MLP(factorised_mush), hypothesis_attention\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWRHhOxCjFl",
        "colab_type": "text"
      },
      "source": [
        "##EVAL SUMMARY :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-skRc_EBRhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, unnormalised_predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(unnormalised_predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f AUC=%0.3f\" % (description,accuracy,precision,recall,f1, auc_full))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysUhazL34GWZ",
        "colab_type": "text"
      },
      "source": [
        "##SHEENABASELINE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_UvQQWx4IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineSentenceEntailment(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineSentenceEntailment, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_embedder = AttentionModel(hp)\n",
        "    self.hypothesis_embedder = AttentionModel(hp)\n",
        "    self.linear_final = torch.nn.Linear(hp.lstm_hidden_size*2, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #premise/hypothesis embeddinbgs\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    main_embeddings = torch.cat((embeddings, added_embeddings), 1)\n",
        "    reshaped_embeddings = main_embeddings.view(self.hp.batch_size, self.hp.max_length, -1)\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "\n",
        "    premise_embedding, premise_attention = self.premise_embedder(processed_premise)\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    hypothesis_embedding, hypothesis_attention = self.hypothesis_embedder(processed_hypothesis)\n",
        "    combined = premise_embedding * hypothesis_embedding\n",
        "    avg = torch.sum(combined, 1)/self.hp.attention_hops\n",
        "    output = torch.sigmoid(self.linear_final(avg))\n",
        "    return output, hypothesis_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRCDtWR4Lcq",
        "colab_type": "text"
      },
      "source": [
        "##BAD DECLARE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db8ikkk64Kx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def load_embeddings(self, word_embeddings):\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, self.hp)\n",
        "  \n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(BaselineDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.premise_lstm = torch.nn.LSTM(\n",
        "        input_size = self.premise_embedding_size,\n",
        "        hidden_size = hp.lstm_hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "      )\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 2*hp.lstm_hidden_size)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(101, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    mean_embeddings = torch.unsqueeze(torch.sum(embeddings, 1) / self.hp.max_length, 1) #change to accurate size of lenfgth\n",
        "  \n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100\n",
        "    main_embeddings = torch.cat((mean_embeddings, added_embeddings), 1)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    processed_premise, hidden_state = self.premise_lstm(main_embeddings, self.hidden_state)\n",
        "    attention_weights = softmax(self.premise_linear(processed_premise))#TODO: turn into row vector\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis,attention_weights.transpose(1,2))\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    avg = torch.sum(combined, 1)/self.hp.max_length #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    output = torch.sigmoid(self.linear_final(smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmJP6kC4Th3",
        "colab_type": "text"
      },
      "source": [
        "## GOOD DECLARE CODE???\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrRhCjK84VeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealDeclare(torch.nn.Module):\n",
        "  def init_hidden(self):\n",
        "    hidden_state = Variable(torch.zeros(2,self.hp.batch_size,self.hp.lstm_hidden_size)).cuda()\n",
        "    cell_state = Variable(torch.zeros(2,self.hp.batch_size, self.hp.lstm_hidden_size)).cuda()\n",
        "    return (hidden_state, cell_state)\n",
        "  def reset_for_testing(self, new_batch):\n",
        "    self.hp.batch_size = new_batch\n",
        "    self.hidden_state = self.init_hidden()\n",
        "\n",
        "  def __init__(self, hp, word_embeddings):\n",
        "    super(RealDeclare, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.embeddings_size = word_embeddings.size(1)\n",
        "    self.premise_embeddings = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n",
        "    self.premise_embeddings.weight = torch.nn.Parameter(word_embeddings)\n",
        "    self.premise_embedding_size = word_embeddings.size(1)\n",
        "    self.hypothesis_processor = SequenceProcessor(word_embeddings, hp)\n",
        "    self.premise_linear = torch.nn.Linear(2*hp.lstm_hidden_size, 1)\n",
        "\n",
        "    self.linear_penultimate = torch.nn.Linear(100, 50)\n",
        "    self.linear_almost_there = torch.nn.Linear(50, 8)\n",
        "    #TODO: add third dense layer with relu\n",
        "    self.linear_final = torch.nn.Linear(8, hp.num_classes)\n",
        "\n",
        "  def forward(self, premise, hypothesis):\n",
        "    #get word embeddings for claim, take a mean over the length\n",
        "    embeddings = self.premise_embeddings(premise)\n",
        "    lengths = self.hp.max_length - (premise == 0).sum(dim=1)\n",
        "    lengths = lengths.repeat(50, 1).transpose(0,1)\n",
        "    summed_embeddings = torch.sum(embeddings, 1)\n",
        "    mean_embeddings = torch.unsqueeze(summed_embeddings / lengths, 1) #change to accurate size of lenfgth\n",
        "\n",
        "\n",
        "    #get word embeddings for article, slap that onto the much smaller claim\n",
        "    added_embeddings = self.premise_embeddings(hypothesis[:, :100])\n",
        "    #TODO: use repeat function to get 100*100 #DONE!\n",
        "    main_embeddings = torch.cat((mean_embeddings.repeat(1, 100, 1), added_embeddings), 2)\n",
        "    #shape is 101 * 50\n",
        "\n",
        "    #attention processing on claim+article combination\n",
        "    attention_weights = softmax(self.premise_linear(main_embeddings.transpose(1,2)))#TODO: turn into row vector\n",
        "    repeated_weights = attention_weights.repeat(1, 1, 100)\n",
        "    #simple embedding of article alone\n",
        "    processed_hypothesis, hidden_state = self.hypothesis_processor(hypothesis, self.hidden_state)\n",
        "    #matrix multiply of the two\n",
        "    combined = torch.bmm(processed_hypothesis, repeated_weights)\n",
        "\n",
        "    #final processing - another average, and then a relu + sigmoid\n",
        "    new_lengths = lengths.repeat(1, 2)\n",
        "    avg = torch.sum(combined, 1)/new_lengths #todo: fix padding\n",
        "\n",
        "    smaller = F.relu(self.linear_penultimate(avg))\n",
        "    even_smaller = F.relu(self.linear_almost_there(smaller))\n",
        "    output = torch.sigmoid(self.linear_final(even_smaller))\n",
        "    return output, torch.zeros(self.hp.batch_size, self.hp.attention_hops, self.hp.lstm_hidden_size*2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNhoKGK_wdLb",
        "colab_type": "text"
      },
      "source": [
        "##TRAIN/TEST/HELPERS\n",
        "HELPER FUNCTIONS FOR DOIN SOME TRAININ AND TESTIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-UHhzD-H_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from inspect import signature\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def l2_matrix_norm(m):\n",
        "  return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "def load_data(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = Variable(data[i]).cuda()\n",
        "  return data\n",
        "\n",
        "def free_data(data):\n",
        "  for point in data:\n",
        "    del(point)\n",
        "def check_data(loader, model):\n",
        "  sample_data = loader.dataset[0]\n",
        "  print(torch.max(loader.dataset[:][-1]))\n",
        "  model_params = len(signature(model).parameters)\n",
        "  return len(sample_data) - 1 != model_params       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuFLnRT7wgBf",
        "colab_type": "text"
      },
      "source": [
        "TRAIN FUNCT, ITS BIG CAUSE IT DOES PRETTY MUCH EVERYTHING\n",
        "\n",
        "INCLUDING NORMALISATION IN THE WEIRD WAY THE SELF ATTENTIVE MODEL REQUIRES\n",
        "\n",
        "ALSO A SWITCH TO ENSURE IT DOES THE BEST AT GETTING BOTH BINARY AND NON BINARY LOSS :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3p3VOkwXCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          train_loader=None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          hp=None, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  model.reset_for_testing(train_loader.batch_size)\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  is_binary = hp.num_classes == 1\n",
        "  \n",
        "  if train_loader.name == \"entailment_data\" and hp.num_classes != 3:\n",
        "      raise ValueError(\"Three classes are needed for entailment to safely happen\")\n",
        "  elif train_loader.name == \"fact_data\" and hp.num_classes !=1:\n",
        "      raise ValueError(\"Two classes are needed for fact checking to safely happen\")\n",
        "  torch.enable_grad()\n",
        "  \n",
        "  for epoch in range(hp.epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    correct = 0\n",
        "    penal = 0\n",
        "    for batch_index, train_data in enumerate(train_loader):\n",
        "      #setting everything up\n",
        "      model.hidden_state = model.init_hidden()\n",
        "      train_data = load_data(train_data)\n",
        "      \n",
        "      #get y values - do forward pass and process\n",
        "      predicted_y, attention = model(*train_data[:-1])\n",
        "      actual_y = train_data[-1]\n",
        "      squeezed_y = predicted_y.double().squeeze(1)\n",
        "\n",
        "      #handling regularisation\n",
        "      if hp.C > 0:\n",
        "        attentionT = attention.transpose(1,2)\n",
        "        identity = torch.eye(attention.size(1))\n",
        "        identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,\n",
        "                                                         attention.size(1),\n",
        "                                                         attention.size(1))).cuda()\n",
        "        penal = l2_matrix_norm(attention@attentionT - identity).cuda()\n",
        "\n",
        "      #get loss, accuracy\n",
        "      if is_binary:\n",
        "        loss = loss_function(squeezed_y, actual_y.double())\n",
        "        loss += hp.C * penal/train_loader.batch_size\n",
        "        correct += torch.eq(torch.round(squeezed_y), actual_y).data.sum()\n",
        "      else:\n",
        "        loss = loss_function(squeezed_y,actual_y.long()) + hp.C * (penal/train_loader.batch_size)\n",
        "        correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "      total_loss += loss.data\n",
        "\n",
        "      #cleaning up regularisation\n",
        "      if hp.C > 0:\n",
        "        del(penal)\n",
        "        del(identity)\n",
        "        del(attentionT)\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "      optimiser.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      if hp.is_debug and batch_index % 10 == 0:\n",
        "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "            epoch, batch_index * len(train_data[0]), len(train_loader.dataset),\n",
        "            100. * batch_index / len(train_loader), loss.item()\n",
        "        ))\n",
        "\n",
        "      if using_gradient_clipping:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      batch_count += 1\n",
        "      optimiser.step()\n",
        "      free_data(train_data)\n",
        "\n",
        "    print(\"Average loss is:\",total_loss/batch_count)\n",
        "    correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "    accuracy = correct_but_numpy / float(batch_count * train_loader.batch_size)\n",
        "    print(\"Accuracy of the model\", accuracy)\n",
        "    losses.append(total_loss/batch_count)\n",
        "    accuracies.append(accuracy)\n",
        "  return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh8DUbUcwxP4",
        "colab_type": "text"
      },
      "source": [
        "TEST FUNCTION\n",
        "\n",
        "THIS STRONG BOY GOES THROUGHH AND ADDS RESULTS ALL OVER THE SHOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79SQs1C2wG7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_wise_evaluate(model, test_loader, hp):\n",
        "  batch_count = 0\n",
        "  total_accuracy = 0\n",
        "  all_results = []\n",
        "  model.eval()\n",
        "  is_binary = hp.num_classes == 1\n",
        "  real_results = []\n",
        "  with torch.no_grad():\n",
        "    for batch_index, test_data in enumerate(test_loader):\n",
        "      #reset everything\n",
        "      model.reset_for_testing(test_data[0].shape[0])\n",
        "      test_data = load_data(test_data)\n",
        "    \n",
        "      #get ys from model and data\n",
        "      y_predicted, _ = model(*test_data[:-1])\n",
        "      y_actual = test_data[-1]\n",
        "      y_squeezed = y_predicted.double().squeeze(1)\n",
        "\n",
        "      #get accuracy\n",
        "      if is_binary:\n",
        "        total_accuracy += torch.eq(torch.round(y_squeezed), y_actual).data.sum()\n",
        "        all_results.append(torch.round(y_squeezed))\n",
        "\n",
        "      else: \n",
        "        total_accuracy += torch.eq(torch.argmax(y_squeezed,1), y_actual).data.sum()\n",
        "        all_results.append(torch.argmax(y_squeezed, 1))\n",
        "\n",
        "      batch_count += 1\n",
        "      real_results.append(y_squeezed)\n",
        "  return torch.cat(real_results, 0), torch.cat(all_results, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzwqgMswGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_stuff(epochs, losses, accuracies=None, title=\"sup nerds\"):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  if accuracies:\n",
        "    plt.plot(range(1, epochs+1), accuracies, scalex=True, scaley=True, label=\"Accuracy\")\n",
        "    plt.annotate(str(accuracies[-1]), xy=(epochs,accuracies[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "\n",
        "  plt.plot(range(1, epochs+1), losses,scalex=True, scaley=True, label=\"Loss\")\n",
        "  plt.annotate(str(losses[-1]), xy=(epochs,losses[-1]), xytext=(3, 3),textcoords=\"offset points\")\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Epochs\", fontsize=16)\n",
        "  plt.ylabel(\"Amount\", fontsize=16)\n",
        "  plt.title(title)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwADakVSwJat",
        "colab_type": "text"
      },
      "source": [
        "NEW FUNCTIONS TO AUTOMATE THE RUNNING OF LOTS OF DATASETS/MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmSdvMewHrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, dataset, hp, is_val=False):\n",
        "  runnable_model = model(hp, dataset.word_embeddings_small).cuda()\n",
        "  bce_loss = torch.nn.BCELoss()\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(runnable_model.parameters(), lr=0.01)\n",
        "  losses, accuracies = train(model=runnable_model,\n",
        "                       train_loader=dataset.train_loader,\n",
        "                       loss_function=bce_loss,\n",
        "                       optimiser = optimiser,\n",
        "                       hp = hp,\n",
        "                       using_gradient_clipping=True)\n",
        "  plot_stuff(hp.epochs, losses, accuracies)\n",
        "  torch.cuda.empty_cache()\n",
        "  if is_val:\n",
        "    check_loader = dataset.val_loader\n",
        "  else:\n",
        "    check_loader = dataset.test_loader\n",
        "  results, predicted_ys = batch_wise_evaluate(runnable_model, \n",
        "         check_loader,\n",
        "         hp)\n",
        "  return results, predicted_ys, runnable_model\n",
        "\n",
        "\n",
        "\n",
        "def get_results(model_name, dataset_name, predictions, unnormalised_predictions, true_labels):\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  if (len(unnormalised_predictions.shape) == 1):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
        "    auc_full = auc(fpr, tpr)\n",
        "  else:\n",
        "    auc_full = 0\n",
        "  return {\"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc_full}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ai-rCghJy-2",
        "colab_type": "text"
      },
      "source": [
        "#RUNNING THE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToBH1XvNkpdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {\n",
        "    \"politifact\": fact_dataset,\n",
        "    \"snopes\": snopes_dataset\n",
        "}\n",
        "models = {\n",
        "    \"my_model\": TextualEntailmentModel,\n",
        "    \"sheena_model\": BaselineSentenceEntailment,\n",
        "    \"broke_declare\": BaselineDeclare,\n",
        "    \"real_declare\": RealDeclare\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUmtTpe81Mc",
        "colab_type": "text"
      },
      "source": [
        "##TextualEntailment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU2rJspTahqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Hyperparameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 20\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 3\n",
        "  dropout=0.1\n",
        "  C = 0.3\n",
        "  is_debug = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wRSAvtABCT",
        "colab_type": "text"
      },
      "source": [
        "##CROSS VALIDATION ZONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Kd6J8o4j6k",
        "colab_type": "text"
      },
      "source": [
        "runnin my textual entailent model :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2R9PS64QS5",
        "colab_type": "code",
        "outputId": "c584be0f-ba50-4bbf-833f-1b2cd699bdfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "\n",
        "results, predicted_ys, text_model= run_model(models[\"my_model\"], datasets[\"politifact\"], Hyperparameters, is_val=True)\n",
        "\n",
        "#textual_entailment_model.to(device)\n",
        "\n"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.640736\n",
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.637605\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.602460\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.513581\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.319040\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.263376\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.141345\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.062872\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 0.963694\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 0.976642\n",
            "Average loss is: tensor(1.3093, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.790028561827957\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 0.922943\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 0.827569\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 0.851358\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 0.808819\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 0.871670\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 0.864064\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 0.818928\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 0.853996\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 0.829955\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 0.829711\n",
            "Average loss is: tensor(0.8528, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9859711021505376\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 0.818298\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 0.793365\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 0.796907\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 0.818945\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 0.817295\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 0.801087\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 0.820077\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 0.815342\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 0.795847\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 0.809611\n",
            "Average loss is: tensor(0.8100, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9961777553763441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JkXr8fcoee",
        "colab_type": "text"
      },
      "source": [
        "WOAH WERE DOIN SOME VALIDATION\n",
        "PLEASE VALIDATE ME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvG0N3VwyiwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "5b3507b3-9c63-46ce-87b6-7826f8337787"
      },
      "source": [
        "evaluation_summary(\"textual entailment model\", predicted_ys.cpu(), results.cpu(),datasets[\"politifact\"].val_data)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: textual entailment model\n",
            "Classifier 'textual entailment model' has Acc=0.597 P=0.596 R=0.596 F1=0.596 AUC=0.596\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.580     0.578     0.579      1384\n",
            "         1.0      0.612     0.614     0.613      1500\n",
            "\n",
            "    accuracy                          0.597      2884\n",
            "   macro avg      0.596     0.596     0.596      2884\n",
            "weighted avg      0.597     0.597     0.597      2884\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[800 579]\n",
            " [584 921]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5960453311297367,\n",
              " 0.5960173410404624,\n",
              " 0.5967406380027739,\n",
              " 0.5960295393791142)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zfw_4Acupe",
        "colab_type": "text"
      },
      "source": [
        "JUST TESTIN HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsngYK8wcvc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_results, other, _ = batch_wise_evaluate(text_model, datasets[\"politifact\"].test_loader, Hyperparameters)\n",
        "evaluation_summary(\"textual entailment test model\", test_results.cpu(), other.cpu(), datasets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViWwxJG5Oys",
        "colab_type": "text"
      },
      "source": [
        "running sheena's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ngDR0NMc26u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SheenaParameters:\n",
        "  lstm_hidden_size = 50\n",
        "  dense_dimension = 20\n",
        "  attention_hops = 10\n",
        "  batch_size = BATCH_SIZE\n",
        "  max_length = MAX_LENGTH\n",
        "  gravity = 70\n",
        "  num_classes = 1\n",
        "  avg=True\n",
        "  epochs = 3\n",
        "  dropout=0.1\n",
        "  C = 0.3\n",
        "  is_debug = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOunnQsb5B7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "215ca980-8985-4c87-f7be-75bef556f7dd"
      },
      "source": [
        "sheena_real_results, sheena_predicted_ys, model = run_model(models[\"sheena_model\"], datasets[\"politifact\"], SheenaParameters)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.641447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.637810\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.567193\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.597132\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.501178\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.389656\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.222440\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.148777\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.079143\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 0.985160\n",
            "Average loss is: tensor(1.3809, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7280325940860215\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 0.972689\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 0.907033\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 0.822837\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 0.821631\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 0.694568\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 0.662198\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 0.577146\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 0.464226\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 0.420081\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 0.396296\n",
            "Average loss is: tensor(0.6629, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9721942204301075\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 0.367029\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 0.367850\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 0.323097\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 0.320947\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 0.359233\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 0.310406\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 0.318634\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 0.309979\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 0.335690\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 0.321258\n",
            "Average loss is: tensor(0.3361, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9928175403225806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VcOuT8wWW7X",
        "colab_type": "code",
        "outputId": "63dd0f0e-e461-4b42-8da7-d6849a08b85f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "evaluation_summary(\"sheena model\", sheena_predicted_ys.cpu(), sheena_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: sheena model\n",
            "Classifier 'sheena model' has Acc=0.617 P=0.623 R=0.624 F1=0.617 AUC=0.623\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.692     0.565     0.622      1593\n",
            "         1.0      0.554     0.682     0.612      1262\n",
            "\n",
            "    accuracy                          0.617      2855\n",
            "   macro avg      0.623     0.624     0.617      2855\n",
            "weighted avg      0.631     0.617     0.617      2855\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[900 401]\n",
            " [693 861]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6229148056588487,\n",
              " 0.6236110738044714,\n",
              " 0.6168126094570928,\n",
              " 0.6167410924640322)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX1DvOud6d0r",
        "colab_type": "text"
      },
      "source": [
        "OK TESTING ON BROKE DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNM29gWaAhWz",
        "colab_type": "code",
        "outputId": "abd7530e-b982-484b-9aa0-f2660e3f7f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "broke_real_results, broke_predicted_ys = run_model(models[\"broke_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.657132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.642244\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.628582\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.611908\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.620506\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.470306\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.430395\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.368045\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.347813\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.290147\n",
            "Average loss is: tensor(1.5115, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6782174059139785\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.218079\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.171573\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.127812\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.140962\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.178767\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.162313\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.192943\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.166833\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.206327\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.140749\n",
            "Average loss is: tensor(1.1748, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9080981182795699\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.072441\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.029454\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.039700\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.041395\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.022918\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.006805\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.049782\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.032539\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.039074\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.017482\n",
            "Average loss is: tensor(1.0243, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9727822580645161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-BrIP06dkJ",
        "colab_type": "code",
        "outputId": "5ff519f0-0efa-4da2-e02d-d88063945c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "evaluation_summary(\"broke declare model\", broke_predicted_ys.cpu(), broke_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: broke declare model\n",
            "Classifier 'broke declare model' has Acc=0.628 P=0.627 R=0.626 F1=0.626 AUC=0.627\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.611     0.599     0.605      2733\n",
            "         1.0      0.642     0.653     0.648      3006\n",
            "\n",
            "    accuracy                          0.628      5739\n",
            "   macro avg      0.627     0.626     0.626      5739\n",
            "weighted avg      0.627     0.628     0.627      5739\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1638 1042]\n",
            " [1095 1964]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.626616956082614, 0.62635066493431, 0.6276354765638613, 0.6264300669101543)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZsRrUK77HAU",
        "colab_type": "text"
      },
      "source": [
        "TESTING ON REAL DECLARE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JetqyT7Imn",
        "colab_type": "code",
        "outputId": "5dee4cdc-5d18-45b3-c641-21d1253ff50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "declare_real_results, declare_predicted_ys =  run_model(models[\"real_declare\"], datasets[\"politifact\"], Hyperparameters)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23817 (0%)]\tLoss: 1.642213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23817 (11%)]\tLoss: 1.636837\n",
            "Train Epoch: 0 [5120/23817 (22%)]\tLoss: 1.651617\n",
            "Train Epoch: 0 [7680/23817 (32%)]\tLoss: 1.626291\n",
            "Train Epoch: 0 [10240/23817 (43%)]\tLoss: 1.635171\n",
            "Train Epoch: 0 [12800/23817 (54%)]\tLoss: 1.619355\n",
            "Train Epoch: 0 [15360/23817 (65%)]\tLoss: 1.488023\n",
            "Train Epoch: 0 [17920/23817 (75%)]\tLoss: 1.512543\n",
            "Train Epoch: 0 [20480/23817 (86%)]\tLoss: 1.441906\n",
            "Train Epoch: 0 [23040/23817 (97%)]\tLoss: 1.406377\n",
            "Average loss is: tensor(1.5766, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6432711693548387\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23817 (0%)]\tLoss: 1.331540\n",
            "Train Epoch: 1 [2560/23817 (11%)]\tLoss: 1.316996\n",
            "Train Epoch: 1 [5120/23817 (22%)]\tLoss: 1.226547\n",
            "Train Epoch: 1 [7680/23817 (32%)]\tLoss: 1.276828\n",
            "Train Epoch: 1 [10240/23817 (43%)]\tLoss: 1.298889\n",
            "Train Epoch: 1 [12800/23817 (54%)]\tLoss: 1.312405\n",
            "Train Epoch: 1 [15360/23817 (65%)]\tLoss: 1.391494\n",
            "Train Epoch: 1 [17920/23817 (75%)]\tLoss: 1.340632\n",
            "Train Epoch: 1 [20480/23817 (86%)]\tLoss: 1.310644\n",
            "Train Epoch: 1 [23040/23817 (97%)]\tLoss: 1.205072\n",
            "Average loss is: tensor(1.3028, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8607610887096774\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23817 (0%)]\tLoss: 1.143060\n",
            "Train Epoch: 2 [2560/23817 (11%)]\tLoss: 1.159585\n",
            "Train Epoch: 2 [5120/23817 (22%)]\tLoss: 1.116163\n",
            "Train Epoch: 2 [7680/23817 (32%)]\tLoss: 1.071864\n",
            "Train Epoch: 2 [10240/23817 (43%)]\tLoss: 1.058171\n",
            "Train Epoch: 2 [12800/23817 (54%)]\tLoss: 1.124656\n",
            "Train Epoch: 2 [15360/23817 (65%)]\tLoss: 1.103765\n",
            "Train Epoch: 2 [17920/23817 (75%)]\tLoss: 1.153490\n",
            "Train Epoch: 2 [20480/23817 (86%)]\tLoss: 1.075670\n",
            "Train Epoch: 2 [23040/23817 (97%)]\tLoss: 1.102014\n",
            "Average loss is: tensor(1.2183, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9269993279569892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjS7Dfz7I8v",
        "colab_type": "code",
        "outputId": "32d36d5e-8160-449b-9a7f-ae1af055c64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "evaluation_summary(\"real declare model\", declare_predicted_ys.cpu(), declare_real_results.cpu(), datasets[\"politifact\"].test_data)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: real declare model\n",
            "Classifier 'real declare model' has Acc=0.614 P=0.617 R=0.618 F1=0.613 AUC=0.617\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.675     0.573     0.620      3158\n",
            "         1.0      0.559     0.663     0.607      2581\n",
            "\n",
            "    accuracy                          0.614      5739\n",
            "   macro avg      0.617     0.618     0.613      5739\n",
            "weighted avg      0.623     0.614     0.614      5739\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[1810  870]\n",
            " [1348 1711]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6173531248627735,\n",
              " 0.6180344550312742,\n",
              " 0.6135215194284719,\n",
              " 0.6134064784646447)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECl8V-P8noH",
        "colab_type": "text"
      },
      "source": [
        "                  \"model_name\":model_name,\n",
        "                  \"dataset_name\": dataset_name,\n",
        "                  \"precision\":precision,\n",
        "                  \"recall\": recall,\n",
        "                  \"accuracy\": accuracy,\n",
        "                  \"f1\": f1,\n",
        "                  \"auc\": auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWBCcIDk674v",
        "colab_type": "text"
      },
      "source": [
        "##VALIDATION LAND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGG5mT_uBEGt",
        "colab_type": "code",
        "outputId": "5542e011-54ba-4e14-c69f-096ca9566703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "avg_amount = 5\n",
        "per_avg_amount = 10\n",
        "import csv\n",
        "\n",
        "full_results = []\n",
        "avg_results = []\n",
        "processed_results = []\n",
        "\n",
        "def list_to_dict(results):\n",
        "  big_results = {\"model_name\": [],\n",
        "                 \"dataset_name\": [],\n",
        "                \"precision\": [],\n",
        "                 \"recall\": [],\n",
        "                 \"accuracy\": [],\n",
        "                 \"f1\": [],\n",
        "                 \"auc\": []}\n",
        "  for result in results:\n",
        "    for key in result:\n",
        "      big_results[key].append(result[key])\n",
        "\n",
        "  return pd.DataFrame.from_dict(big_results)\n",
        "\n",
        "def process_results(big_results):\n",
        "  return (big_results[\"model_name\"][0], big_results[\"dataset_name\"][0], big_results.mean(), big_results.std())\n",
        "  \n",
        "  \n",
        "def get_avgs(some_results):\n",
        "  avg_results = {\"model_name\": some_results[0][\"model_name\"],\n",
        "                 \"dataset_name\": some_results[0][\"dataset_name\"],\n",
        "                \"precision\": 0.0,\n",
        "                 \"recall\": 0.0,\n",
        "                 \"accuracy\": 0.0,\n",
        "                 \"f1\": 0.0,\n",
        "                 \"auc\": 0.0}\n",
        "  for result in some_results:\n",
        "    for key in result:\n",
        "      if type(result[key]) is float:\n",
        "        avg_results[key] += result[key]\n",
        "  \n",
        "  for key in avg_results:\n",
        "    if(type(avg_results[key] is float)):\n",
        "      avg_results[key] /= len(some_results)\n",
        "  \n",
        "  return avg_results\n",
        "\n",
        "\n",
        "\n",
        "for data_name in datasets:\n",
        "  for model_name in models:\n",
        "    some_results = []\n",
        "  \n",
        "    for per_avg_i in range(per_avg_amount):\n",
        "      results, predicted_ys = run_model(models[model_name], datasets[data_name], Hyperparameters)\n",
        "      full_results.append(get_results(model_name, data_name, predicted_ys.cpu(), results.cpu(), datasets[data_name].test_data))\n",
        "      some_results.append(get_results(model_name, data_name, predicted_ys.cpu(), results.cpu(), datasets[data_name].test_data))\n",
        "      \n",
        "    processed_results.append(process_results(list_to_dict(some_results)))\n",
        "    print(processed_results)\n",
        "      #avg_results.append(get_avgs(some_results))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.632861\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.584366\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.558179\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.415853\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.231022\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.178902\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.105380\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.067210\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.103633\n",
            "Average loss is: tensor(1.3588, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7633499313186813\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.963361\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.929948\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.943169\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.978601\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.925199\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.912593\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.868571\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.894872\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.815740\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.831411\n",
            "Average loss is: tensor(0.9038, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9795673076923077\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.769679\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.754233\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.709304\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.704714\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.638389\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.645151\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.593351\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.559690\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.527799\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.550086\n",
            "Average loss is: tensor(0.6368, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9945913461538461\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.645433\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.638646\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.597457\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.527050\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.468394\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.257880\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.170449\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.013798\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 0.956981\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 0.914388\n",
            "Average loss is: tensor(1.3119, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7554945054945055\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.888115\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.818161\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.801195\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.801552\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.811239\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.761841\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.838016\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.801110\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.770721\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.750411\n",
            "Average loss is: tensor(0.8040, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9817135989010989\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.747670\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.740024\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.737994\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.769468\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.787188\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.739748\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.766962\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.738103\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.734725\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.740684\n",
            "Average loss is: tensor(0.7473, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9964800824175825\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.645822\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.640833\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.642258\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.637673\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.606166\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.521772\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.363934\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.241154\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.120816\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.115758\n",
            "Average loss is: tensor(1.4743, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6838942307692307\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.062423\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.972617\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.990736\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.965783\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.026244\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.936675\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.937452\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.918026\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.887858\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.885996\n",
            "Average loss is: tensor(0.9643, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.978021978021978\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.871741\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.866699\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.861440\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.857438\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.857890\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.864440\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.857254\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.852697\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.847177\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.845287\n",
            "Average loss is: tensor(0.8655, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9970810439560439\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642856\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.637513\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.615910\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.553308\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.401358\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.293424\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.202807\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.152584\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.069865\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.073365\n",
            "Average loss is: tensor(1.3749, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7639508928571429\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.030531\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.980463\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.027122\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.014012\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.992401\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.926981\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.936180\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.936084\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.916764\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.909752\n",
            "Average loss is: tensor(0.9645, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9855339972527473\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.927820\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.849757\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.812785\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.789916\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.744820\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.696026\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.626339\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.562219\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.521052\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.496655\n",
            "Average loss is: tensor(0.7043, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9963942307692307\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.635207\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.638596\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.630598\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.467347\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.351681\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.321688\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.116428\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.117195\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.055999\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 0.980207\n",
            "Average loss is: tensor(1.3360, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7738238324175825\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.912998\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.849546\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.842036\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.773125\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.746704\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.688032\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.741097\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.705265\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.666389\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.645583\n",
            "Average loss is: tensor(0.7658, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.984632554945055\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.640063\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.636135\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.613760\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.630792\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.610042\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.604177\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.600890\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.608018\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.617387\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.663629\n",
            "Average loss is: tensor(0.6130, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9967805631868132\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640689\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.637187\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.587576\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.526103\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.398221\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.292622\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.216775\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.219411\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.165481\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.083235\n",
            "Average loss is: tensor(1.3760, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.767084478021978\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.038601\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.040085\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.056598\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.042789\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.924759\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.940826\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.947165\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.929507\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.916141\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.890358\n",
            "Average loss is: tensor(0.9584, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9842032967032966\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.835650\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.804376\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.787149\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.781148\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.772004\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.767813\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.768655\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.766389\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.752861\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.749511\n",
            "Average loss is: tensor(0.7838, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9965659340659341\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642639\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.643811\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.635921\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.620344\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.547139\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.377217\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.298974\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.198833\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.201041\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.140732\n",
            "Average loss is: tensor(1.4279, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7170758928571429\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.054764\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.990114\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.991884\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.971981\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.988487\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.945153\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.913845\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.927848\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.834770\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.867710\n",
            "Average loss is: tensor(0.9564, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9801253434065934\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.802840\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.781387\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.774439\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.789711\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.751079\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.731341\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.685823\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.606280\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.548047\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.531515\n",
            "Average loss is: tensor(0.7072, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9945484203296703\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.639697\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.638492\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.614847\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.605678\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.540878\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.423195\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.247885\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.097954\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.051754\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 0.953009\n",
            "Average loss is: tensor(1.3853, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7189217032967034\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.012911\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.963811\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.970055\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.907275\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.896270\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.879397\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.846552\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.840085\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.831537\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.868382\n",
            "Average loss is: tensor(0.8938, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9793956043956044\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.855553\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.816285\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.812201\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.830928\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.816270\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.811288\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.820411\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.824415\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.821073\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.827170\n",
            "Average loss is: tensor(0.8246, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9965230082417582\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.645270\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.642245\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.641444\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.637299\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.636657\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.510384\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.358400\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.240618\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.125868\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.100964\n",
            "Average loss is: tensor(1.4671, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6796445741758241\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.031891\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.962301\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.005960\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.036396\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.952138\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.930605\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.873492\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.929693\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.924378\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.897289\n",
            "Average loss is: tensor(0.9407, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9787087912087912\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.848941\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.821446\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.839936\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.811971\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.882133\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.826817\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.826023\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.821082\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.765707\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.761142\n",
            "Average loss is: tensor(0.8189, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.995836195054945\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640841\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.634760\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.592177\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.469207\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.394789\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.266393\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.117126\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.129149\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.047144\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.003675\n",
            "Average loss is: tensor(1.3464, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7791037087912088\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.012727\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.953232\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.895922\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.899067\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.921277\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.817077\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.751177\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.783615\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.744925\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.683984\n",
            "Average loss is: tensor(0.8452, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9799536401098901\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.673795\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.666422\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.634409\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.628965\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.646953\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.621463\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.625274\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.611042\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.602548\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.620944\n",
            "Average loss is: tensor(0.6371, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9953210851648352\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640534\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.616433\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.577711\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.505389\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.506914\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.348493\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.319447\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.204340\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.185957\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.081577\n",
            "Average loss is: tensor(1.4118, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7391826923076923\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.069777\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.014748\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.000270\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.972643\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.989990\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.874096\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.770794\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.786458\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.672281\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.589337\n",
            "Average loss is: tensor(0.8619, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9731284340659341\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.554189\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.505964\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.458588\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.440204\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.413806\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.396263\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.390473\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.386517\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.372599\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.396907\n",
            "Average loss is: tensor(0.4334, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9918870192307693\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.639950\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.635444\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.599835\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.533817\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.512158\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.333285\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.276270\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.256331\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.092194\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.098018\n",
            "Average loss is: tensor(1.4078, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7238152472527473\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.039644\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.046825\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.034314\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.939128\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.856740\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.827949\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.779863\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.720148\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.699003\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.672916\n",
            "Average loss is: tensor(0.8407, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9696514423076923\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.611282\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.681828\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.622471\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.648295\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.614062\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.553526\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.562991\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.592977\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.541906\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.578771\n",
            "Average loss is: tensor(0.6164, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9830442994505495\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.637371\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.637496\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.626263\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.564589\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.480936\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.456194\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.402411\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.329244\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.257159\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.265239\n",
            "Average loss is: tensor(1.4578, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7126116071428571\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.225060\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.291707\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.237362\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.227734\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.190592\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.147980\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.133224\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.192630\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.049148\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.057786\n",
            "Average loss is: tensor(1.2009, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8474416208791209\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.916585\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.874486\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.803687\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.776985\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.754705\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.739796\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.659471\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.669165\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.632843\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.610498\n",
            "Average loss is: tensor(0.7535, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9405906593406593\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.639966\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.622465\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.615838\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.572825\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.542840\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.473168\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.383043\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.365130\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.193863\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.233669\n",
            "Average loss is: tensor(1.4608, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7041122939560439\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.114833\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.010257\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.037084\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.070226\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.033595\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.973795\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.968273\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.891368\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.834913\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.816460\n",
            "Average loss is: tensor(0.9830, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9128176510989011\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.689777\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.666324\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.614251\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.796388\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.639703\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.576629\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.580459\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.492462\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.492931\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.453542\n",
            "Average loss is: tensor(0.5999, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9526098901098901\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641652\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.635663\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.572479\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.552565\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.463652\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.287809\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.141296\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.231385\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.085445\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.032985\n",
            "Average loss is: tensor(1.3711, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7550223214285714\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.048827\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.928921\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.813997\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.724199\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.698507\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.607781\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.551555\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.588191\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.516207\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.484265\n",
            "Average loss is: tensor(0.6890, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9751459478021978\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.479626\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.498179\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.481030\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.450921\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.422974\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.383480\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.346568\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.304200\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.300789\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.327919\n",
            "Average loss is: tensor(0.3908, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9942050137362637\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641896\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.609835\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.571286\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.528965\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.410586\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.223103\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.135576\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.094723\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.022158\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.002212\n",
            "Average loss is: tensor(1.3361, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.765882554945055\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.937625\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.855656\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.823771\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.833879\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.786385\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.691529\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.722328\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.619472\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.632106\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.664022\n",
            "Average loss is: tensor(0.7664, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9742445054945055\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.590954\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.549615\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.527307\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.553977\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.553382\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.514492\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.509453\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.482556\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.446625\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.440073\n",
            "Average loss is: tensor(0.5155, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9941191620879121\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640405\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.630675\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.585543\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.534973\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.489742\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.410567\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.410748\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.342573\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.244768\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.088603\n",
            "Average loss is: tensor(1.4398, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7119677197802198\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.084109\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.943221\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.884219\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.861494\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.786035\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.767664\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.656419\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.607077\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.508564\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.436377\n",
            "Average loss is: tensor(0.7564, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9600360576923077\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.439739\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.416688\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.395796\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.357334\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.309326\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.288183\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.302320\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.328782\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.277017\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.267565\n",
            "Average loss is: tensor(0.3336, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9921445741758241\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640019\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.600899\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.636133\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.531514\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.544826\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.469761\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.359082\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.290978\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.252016\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.166571\n",
            "Average loss is: tensor(1.4714, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.715573489010989\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.195776\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.119140\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.124267\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.063495\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.052139\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.058330\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.988833\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.971188\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.001004\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.963294\n",
            "Average loss is: tensor(1.0481, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9622682005494505\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.004847\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.873909\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.845742\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.820555\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.717438\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.622376\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.520468\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.450979\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.440944\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.387752\n",
            "Average loss is: tensor(0.6606, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9911143543956044\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641094\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.621249\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.566853\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.563028\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.459987\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.406091\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.290595\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.298148\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.367175\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.273036\n",
            "Average loss is: tensor(1.4429, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7039405906593407\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.252033\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.142372\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.145568\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.154301\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.949089\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.893374\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.884628\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.848674\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.849392\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.813733\n",
            "Average loss is: tensor(0.9874, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8711796016483516\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.780258\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.684117\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.679965\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.629688\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.661706\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.638306\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.626360\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.598124\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.561197\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.577284\n",
            "Average loss is: tensor(0.6407, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9843320741758241\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.639813\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.642873\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.620254\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.563540\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.502171\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.326664\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.222178\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.074544\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 0.982054\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.090096\n",
            "Average loss is: tensor(1.3676, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7384100274725275\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 0.949671\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 0.837080\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 0.783070\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 0.759193\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 0.702331\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 0.652471\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 0.911141\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 0.591655\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 0.592837\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 0.566332\n",
            "Average loss is: tensor(0.7180, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9717977335164835\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 0.569033\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 0.542806\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.532836\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.532566\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 0.520207\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 0.474252\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 0.441487\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 0.358347\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.317882\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 0.339943\n",
            "Average loss is: tensor(0.4758, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9941191620879121\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64), ('sheena_model', 'politifact', precision    0.632436\n",
            "recall       0.633402\n",
            "accuracy     0.631072\n",
            "f1           0.630514\n",
            "auc          0.632436\n",
            "dtype: float64, precision    0.012012\n",
            "recall       0.012595\n",
            "accuracy     0.011529\n",
            "f1           0.011450\n",
            "auc          0.012012\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641710\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.629207\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.605381\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.600696\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.552547\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.461404\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.452475\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.369167\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.415707\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.350960\n",
            "Average loss is: tensor(1.5088, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7029103708791209\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.234345\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.206634\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.192682\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.196957\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.322760\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.117931\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.147257\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.193891\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.222001\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.142161\n",
            "Average loss is: tensor(1.1846, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9033739697802198\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.054614\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.028885\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.000531\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 0.997464\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.056617\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.029248\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.041840\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.017039\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.992819\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.070464\n",
            "Average loss is: tensor(1.0252, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9728279532967034\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.649764\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.641535\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.626654\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.636086\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.577285\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.507947\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.474329\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.413416\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.414820\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.348149\n",
            "Average loss is: tensor(1.5369, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6689560439560439\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.313243\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.240441\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.247128\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.202756\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.208648\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.235656\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.193024\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.215536\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.142310\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.173184\n",
            "Average loss is: tensor(1.2112, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8918698489010989\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.095899\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.020352\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.045218\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.019944\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.021112\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.082214\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.090730\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.110861\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.089254\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.059983\n",
            "Average loss is: tensor(1.0574, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9590916895604396\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.645547\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.642197\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.640689\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.628490\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.532921\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.525650\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.555847\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.437323\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.396770\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.392339\n",
            "Average loss is: tensor(1.5499, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6466775412087912\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.348926\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.272844\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.282574\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.197565\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.241059\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.211120\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.198235\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.210864\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.186803\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.231617\n",
            "Average loss is: tensor(1.2347, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8819969093406593\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.072840\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.097228\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.132064\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.104446\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.074603\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.093853\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.069664\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.057366\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.109624\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.083076\n",
            "Average loss is: tensor(1.0785, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9501631181318682\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641920\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.644768\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.640983\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.655022\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.632880\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.592525\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.485296\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.466706\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.485926\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.410297\n",
            "Average loss is: tensor(1.5716, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.616929945054945\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.292227\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.236079\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.333032\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.273611\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.282607\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.267887\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.259078\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.233393\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.233237\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.223075\n",
            "Average loss is: tensor(1.2638, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8619505494505495\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.101178\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.063618\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.070453\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.036393\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.095870\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.082101\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.142315\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.038546\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.013084\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.047371\n",
            "Average loss is: tensor(1.0739, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9509787087912088\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.632787\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.641052\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.617950\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.655086\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.636081\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.645044\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.640129\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.641925\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.641723\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.643613\n",
            "Average loss is: tensor(1.6408, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5145947802197802\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.642167\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.641994\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.641565\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.642163\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.642098\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.641927\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.641842\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.641930\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.639786\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.639815\n",
            "Average loss is: tensor(1.6419, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5023179945054945\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.641558\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.641956\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.641714\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.641833\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.641855\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.640998\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.642304\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.642069\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.641948\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.641711\n",
            "Average loss is: tensor(1.6419, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.49828296703296704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.640791\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.638111\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.636649\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.613779\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.600282\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.551470\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.480122\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.403122\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.373569\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.337759\n",
            "Average loss is: tensor(1.5296, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6760817307692307\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.220479\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.201358\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.232560\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.233302\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.135679\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.157284\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.221421\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.175216\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.201431\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.138987\n",
            "Average loss is: tensor(1.1833, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9052197802197802\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.033334\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.037666\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 0.983153\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.011503\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.007091\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.026284\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.056366\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.034844\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.066701\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.073765\n",
            "Average loss is: tensor(1.0274, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9711538461538461\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.639882\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.647923\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.639691\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.640858\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.626726\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.619790\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.558047\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.476777\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.487695\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.429768\n",
            "Average loss is: tensor(1.5799, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6077438186813187\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.287333\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.285694\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.216263\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.260932\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.222004\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.267625\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.286636\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.199712\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.177246\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.239647\n",
            "Average loss is: tensor(1.2477, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8737122252747253\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.091376\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.099015\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.050742\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.056394\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.008933\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.082873\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.052250\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.109566\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 0.999364\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.052410\n",
            "Average loss is: tensor(1.0562, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9605940934065934\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.644090\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.636322\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.596769\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.564160\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.510034\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.444415\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.472959\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.364557\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.350970\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.343882\n",
            "Average loss is: tensor(1.4900, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7127403846153846\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.209233\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.258405\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.127780\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.178514\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.173372\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.166805\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.187427\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.109012\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.150286\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.159821\n",
            "Average loss is: tensor(1.1737, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9107142857142857\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.062289\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.040110\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.005686\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.007769\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.024770\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.083043\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.052485\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.042182\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.088464\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.083400\n",
            "Average loss is: tensor(1.0333, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.970209478021978\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641813\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.638992\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.640566\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.589866\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.561343\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.522304\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.479830\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.413232\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.408246\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.338885\n",
            "Average loss is: tensor(1.5278, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6697287087912088\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.233931\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.179521\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.183605\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.223734\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.164626\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.135041\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.280530\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.159714\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.250633\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.166449\n",
            "Average loss is: tensor(1.2037, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8954326923076923\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.058575\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.093095\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.067713\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.033070\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.059365\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.026687\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.033869\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.020655\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.052609\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.129036\n",
            "Average loss is: tensor(1.0499, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9627833104395604\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642750\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.639065\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.634596\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.611639\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.623565\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.549419\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.603830\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.476277\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.508738\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.446518\n",
            "Average loss is: tensor(1.5744, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6341002747252747\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.364742\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.337639\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.228735\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.237915\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.253802\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.203834\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.177043\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.194360\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.231809\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.194634\n",
            "Average loss is: tensor(1.2382, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8796789148351648\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.017359\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.018004\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.023926\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.024857\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.044802\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.030019\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.036574\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.041007\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.050208\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.055354\n",
            "Average loss is: tensor(1.0496, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9637706043956044\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64), ('sheena_model', 'politifact', precision    0.632436\n",
            "recall       0.633402\n",
            "accuracy     0.631072\n",
            "f1           0.630514\n",
            "auc          0.632436\n",
            "dtype: float64, precision    0.012012\n",
            "recall       0.012595\n",
            "accuracy     0.011529\n",
            "f1           0.011450\n",
            "auc          0.012012\n",
            "dtype: float64), ('broke_declare', 'politifact', precision    0.609768\n",
            "recall       0.586451\n",
            "accuracy     0.611636\n",
            "f1           0.593040\n",
            "auc          0.609768\n",
            "dtype: float64, precision    0.040286\n",
            "recall       0.115097\n",
            "accuracy     0.033908\n",
            "f1           0.088734\n",
            "auc          0.040286\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.643813\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.641765\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.642853\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.642203\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.636111\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.713071\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.671514\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.583403\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.582554\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.598674\n",
            "Average loss is: tensor(1.6510, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5471754807692307\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.581446\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.570998\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.514387\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.568493\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.548978\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.724499\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.516254\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.474536\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.414470\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.490105\n",
            "Average loss is: tensor(1.5308, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7299107142857143\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.548290\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.376025\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.405245\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.333262\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.361716\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.389010\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.330804\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.291283\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.429498\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.348775\n",
            "Average loss is: tensor(1.3665, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8597184065934066\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.663042\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.708559\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.620081\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.626839\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.637580\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.869525\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.569907\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.575044\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.581963\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.579835\n",
            "Average loss is: tensor(1.6551, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5963255494505495\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.564833\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.548466\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.511897\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.454511\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.500194\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.449759\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.429260\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 2.296564\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 2.406540\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.454820\n",
            "Average loss is: tensor(1.5706, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7362637362637363\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.422883\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.506877\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.320422\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.448734\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.459857\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.370383\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.398465\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.312961\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.317271\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.389077\n",
            "Average loss is: tensor(1.4236, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8248197115384616\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.642536\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.638046\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.641123\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.636108\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.634257\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.580259\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.622102\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.901861\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.542607\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.559663\n",
            "Average loss is: tensor(1.6225, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6012190934065934\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.491220\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.464588\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.432174\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.919392\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.489976\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.423688\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.352279\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.427416\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.433607\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.402452\n",
            "Average loss is: tensor(1.5755, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7635216346153846\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.360865\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.382688\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.332369\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.341132\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.342132\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.381074\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.339556\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.416543\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.385441\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.381005\n",
            "Average loss is: tensor(1.4030, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8404447115384616\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.643101\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.646222\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.642035\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.641619\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.640386\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.641345\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.642094\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.641092\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.641869\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.641821\n",
            "Average loss is: tensor(1.6493, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5029189560439561\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.641765\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.642025\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.641860\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.641881\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.641813\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.642900\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.640959\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.639493\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.642305\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.642204\n",
            "Average loss is: tensor(1.6418, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5019745879120879\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.641723\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.642759\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.642937\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.641887\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.641951\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.641575\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.642035\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.640734\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.641442\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.641026\n",
            "Average loss is: tensor(1.6419, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5025326236263736\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.653452\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.658745\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.647200\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.641527\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.639981\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.636565\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.622263\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.543344\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.532864\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.517011\n",
            "Average loss is: tensor(1.6408, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5778245192307693\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.467107\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.476487\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.427268\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.382849\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.451670\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.386466\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.385948\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.405696\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.369421\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.372289\n",
            "Average loss is: tensor(1.4004, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8269660027472527\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.285082\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.186034\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.199967\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.266397\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.218750\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.210223\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.227449\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.293695\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.255104\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.223006\n",
            "Average loss is: tensor(1.2241, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9077953296703297\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.639069\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.650191\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.633857\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.640178\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.613752\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.568954\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.542231\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.464849\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.482096\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.460139\n",
            "Average loss is: tensor(1.6217, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6196771978021978\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.434377\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.358062\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.308863\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.327554\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.451118\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.284960\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.298628\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.222875\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.251787\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.309226\n",
            "Average loss is: tensor(1.3078, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8485147664835165\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.263534\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.123416\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.097530\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.148017\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.081278\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.057267\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.096521\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.092245\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.202495\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.092413\n",
            "Average loss is: tensor(1.1177, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9356112637362637\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.637724\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.646681\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.642043\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.588217\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.547072\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.543503\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.525307\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.783130\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.450738\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.346508\n",
            "Average loss is: tensor(1.5562, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6868990384615384\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.382490\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.538376\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.273325\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.260429\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.290156\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.194523\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.245217\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.377472\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 2.005915\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.193465\n",
            "Average loss is: tensor(1.3954, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8615212912087912\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.143909\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.150601\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.122572\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.116704\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.434680\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.143762\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.121236\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.127219\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.093891\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.327525\n",
            "Average loss is: tensor(1.1734, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9259529532967034\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641594\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.644685\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.631182\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.667152\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.644358\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.647220\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.532276\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.475834\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.576021\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.380220\n",
            "Average loss is: tensor(1.5979, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6317393543956044\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.492698\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.385776\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.544419\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 2.378755\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.374777\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.337563\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.365049\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.242313\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.325690\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.397732\n",
            "Average loss is: tensor(1.4305, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8226734203296703\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.238173\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.200653\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.367112\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.192933\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.220230\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.153652\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.207479\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.192069\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.166506\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.082666\n",
            "Average loss is: tensor(1.2794, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9052627060439561\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.647101\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.644793\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.716148\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.583236\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.599481\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.632091\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.490749\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.443772\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.412011\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.442445\n",
            "Average loss is: tensor(1.5705, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6544900412087912\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.367833\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.283684\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.189421\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.290252\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.323379\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.202101\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.334486\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.423931\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 1.299453\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.276807\n",
            "Average loss is: tensor(1.2912, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8655133928571429\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.125211\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.049227\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.105842\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.074971\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.155233\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.134367\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.155495\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.136365\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.104221\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.110955\n",
            "Average loss is: tensor(1.1104, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9404618818681318\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/23454 (0%)]\tLoss: 1.641154\n",
            "Train Epoch: 0 [2560/23454 (11%)]\tLoss: 1.642393\n",
            "Train Epoch: 0 [5120/23454 (22%)]\tLoss: 1.641888\n",
            "Train Epoch: 0 [7680/23454 (33%)]\tLoss: 1.651490\n",
            "Train Epoch: 0 [10240/23454 (44%)]\tLoss: 1.642264\n",
            "Train Epoch: 0 [12800/23454 (55%)]\tLoss: 1.641756\n",
            "Train Epoch: 0 [15360/23454 (66%)]\tLoss: 1.641646\n",
            "Train Epoch: 0 [17920/23454 (77%)]\tLoss: 1.642099\n",
            "Train Epoch: 0 [20480/23454 (88%)]\tLoss: 1.641635\n",
            "Train Epoch: 0 [23040/23454 (99%)]\tLoss: 1.641521\n",
            "Average loss is: tensor(1.6498, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5020175137362637\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/23454 (0%)]\tLoss: 1.641689\n",
            "Train Epoch: 1 [2560/23454 (11%)]\tLoss: 1.641702\n",
            "Train Epoch: 1 [5120/23454 (22%)]\tLoss: 1.641978\n",
            "Train Epoch: 1 [7680/23454 (33%)]\tLoss: 1.642051\n",
            "Train Epoch: 1 [10240/23454 (44%)]\tLoss: 1.633044\n",
            "Train Epoch: 1 [12800/23454 (55%)]\tLoss: 1.589840\n",
            "Train Epoch: 1 [15360/23454 (66%)]\tLoss: 1.542451\n",
            "Train Epoch: 1 [17920/23454 (77%)]\tLoss: 1.531904\n",
            "Train Epoch: 1 [20480/23454 (88%)]\tLoss: 2.088126\n",
            "Train Epoch: 1 [23040/23454 (99%)]\tLoss: 1.467335\n",
            "Average loss is: tensor(1.6081, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6187328296703297\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/23454 (0%)]\tLoss: 1.453976\n",
            "Train Epoch: 2 [2560/23454 (11%)]\tLoss: 1.383475\n",
            "Train Epoch: 2 [5120/23454 (22%)]\tLoss: 1.381171\n",
            "Train Epoch: 2 [7680/23454 (33%)]\tLoss: 1.363176\n",
            "Train Epoch: 2 [10240/23454 (44%)]\tLoss: 1.318630\n",
            "Train Epoch: 2 [12800/23454 (55%)]\tLoss: 1.334485\n",
            "Train Epoch: 2 [15360/23454 (66%)]\tLoss: 1.399564\n",
            "Train Epoch: 2 [17920/23454 (77%)]\tLoss: 1.386267\n",
            "Train Epoch: 2 [20480/23454 (88%)]\tLoss: 1.333760\n",
            "Train Epoch: 2 [23040/23454 (99%)]\tLoss: 1.287822\n",
            "Average loss is: tensor(1.5968, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8229309752747253\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64), ('sheena_model', 'politifact', precision    0.632436\n",
            "recall       0.633402\n",
            "accuracy     0.631072\n",
            "f1           0.630514\n",
            "auc          0.632436\n",
            "dtype: float64, precision    0.012012\n",
            "recall       0.012595\n",
            "accuracy     0.011529\n",
            "f1           0.011450\n",
            "auc          0.012012\n",
            "dtype: float64), ('broke_declare', 'politifact', precision    0.609768\n",
            "recall       0.586451\n",
            "accuracy     0.611636\n",
            "f1           0.593040\n",
            "auc          0.609768\n",
            "dtype: float64, precision    0.040286\n",
            "recall       0.115097\n",
            "accuracy     0.033908\n",
            "f1           0.088734\n",
            "auc          0.040286\n",
            "dtype: float64), ('real_declare', 'politifact', precision    0.608128\n",
            "recall       0.591186\n",
            "accuracy     0.608555\n",
            "f1           0.585739\n",
            "auc          0.608128\n",
            "dtype: float64, precision    0.039355\n",
            "recall       0.116572\n",
            "accuracy     0.033756\n",
            "f1           0.087624\n",
            "auc          0.039355\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.636675\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.633869\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.633274\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 2.405400\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.215294\n",
            "Average loss is: tensor(1.5500, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6317708333333333\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.162897\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.022551\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.883769\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.865468\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.848141\n",
            "Average loss is: tensor(0.9352, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9683159722222222\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.819302\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.797101\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.793983\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.788222\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.748149\n",
            "Average loss is: tensor(0.7904, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9980034722222222\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.647287\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.644525\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.632896\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.473331\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.224998\n",
            "Average loss is: tensor(1.5131, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6491319444444444\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.094493\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 0.987380\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.029121\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.992574\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.978356\n",
            "Average loss is: tensor(1.0034, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9813368055555556\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.966202\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.943162\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.958002\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.951720\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.946020\n",
            "Average loss is: tensor(0.9493, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9982638888888888\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.643293\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640666\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.634289\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.383134\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.296023\n",
            "Average loss is: tensor(1.4870, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6703993055555556\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.018629\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 0.944352\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.984411\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.909982\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.895296\n",
            "Average loss is: tensor(0.9558, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9815104166666667\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.883915\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.855743\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.858656\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.848821\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.854205\n",
            "Average loss is: tensor(0.8572, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9981770833333333\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.639928\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.630645\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.598802\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.431053\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.133604\n",
            "Average loss is: tensor(1.4767, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6808159722222222\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.032791\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 0.983238\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.994212\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.871397\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.868341\n",
            "Average loss is: tensor(0.9360, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9845486111111111\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.874949\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.880220\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.852666\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.813392\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.799435\n",
            "Average loss is: tensor(0.8322, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9986111111111111\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.637032\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.634390\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.619496\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.597694\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.535762\n",
            "Average loss is: tensor(1.5955, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5865451388888889\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.402572\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.276287\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.108171\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.015773\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.006935\n",
            "Average loss is: tensor(1.1350, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9043402777777778\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.942555\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.922387\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.912952\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.917681\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.883935\n",
            "Average loss is: tensor(0.9117, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9922743055555555\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.650178\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.643139\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.641094\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.637651\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.637253\n",
            "Average loss is: tensor(1.6484, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.49322916666666666\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.632106\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.529968\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.278725\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.225330\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.113068\n",
            "Average loss is: tensor(1.3462, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8040798611111111\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.034333\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.988159\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.004296\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.007884\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.992216\n",
            "Average loss is: tensor(1.0163, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9730034722222223\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.637175\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.637411\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.618540\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.577911\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.453785\n",
            "Average loss is: tensor(1.5780, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6030381944444444\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.272070\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.084502\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.024064\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.021538\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.942281\n",
            "Average loss is: tensor(1.0702, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9360243055555556\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.913903\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.908738\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.883804\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.909522\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.883242\n",
            "Average loss is: tensor(0.8952, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9956597222222222\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.643569\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.636944\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.612178\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.418042\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.141792\n",
            "Average loss is: tensor(1.4747, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6847222222222222\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.107957\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 0.985385\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.038273\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.956698\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.927319\n",
            "Average loss is: tensor(0.9805, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9837673611111111\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.913788\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.898343\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.896545\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.894714\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.885530\n",
            "Average loss is: tensor(0.8982, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.99765625\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.643115\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640662\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.640016\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.639485\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.640991\n",
            "Average loss is: tensor(1.6410, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5091145833333334\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.632376\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.525392\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.422813\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.125628\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.075071\n",
            "Average loss is: tensor(1.3273, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7943576388888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.981166\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.959689\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.925309\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.939804\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.902196\n",
            "Average loss is: tensor(0.9360, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9916666666666667\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.637075\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640517\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.637950\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.490753\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.142571\n",
            "Average loss is: tensor(1.4780, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6677951388888889\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.027763\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 0.983163\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.937580\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.990791\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.884327\n",
            "Average loss is: tensor(0.9505, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9796006944444444\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.860357\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.823175\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.805788\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.800114\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.795912\n",
            "Average loss is: tensor(0.8160, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9980902777777778\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64), ('sheena_model', 'politifact', precision    0.632436\n",
            "recall       0.633402\n",
            "accuracy     0.631072\n",
            "f1           0.630514\n",
            "auc          0.632436\n",
            "dtype: float64, precision    0.012012\n",
            "recall       0.012595\n",
            "accuracy     0.011529\n",
            "f1           0.011450\n",
            "auc          0.012012\n",
            "dtype: float64), ('broke_declare', 'politifact', precision    0.609768\n",
            "recall       0.586451\n",
            "accuracy     0.611636\n",
            "f1           0.593040\n",
            "auc          0.609768\n",
            "dtype: float64, precision    0.040286\n",
            "recall       0.115097\n",
            "accuracy     0.033908\n",
            "f1           0.088734\n",
            "auc          0.040286\n",
            "dtype: float64), ('real_declare', 'politifact', precision    0.608128\n",
            "recall       0.591186\n",
            "accuracy     0.608555\n",
            "f1           0.585739\n",
            "auc          0.608128\n",
            "dtype: float64, precision    0.039355\n",
            "recall       0.116572\n",
            "accuracy     0.033756\n",
            "f1           0.087624\n",
            "auc          0.039355\n",
            "dtype: float64), ('my_model', 'snopes', precision    0.665821\n",
            "recall       0.667794\n",
            "accuracy     0.668721\n",
            "f1           0.663418\n",
            "auc          0.665821\n",
            "dtype: float64, precision    0.025402\n",
            "recall       0.024066\n",
            "accuracy     0.024999\n",
            "f1           0.025761\n",
            "auc          0.025402\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.639190\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.624207\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.492238\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.337692\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.204917\n",
            "Average loss is: tensor(1.4630, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6962673611111111\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.072954\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.164286\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.934192\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.862914\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.854989\n",
            "Average loss is: tensor(0.9681, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9604166666666667\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.802257\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.795508\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.762255\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.750520\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.749390\n",
            "Average loss is: tensor(0.7773, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9962673611111111\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.641146\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.639577\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.638913\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.626928\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.522811\n",
            "Average loss is: tensor(1.6108, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5730034722222223\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.407885\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.104889\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.978303\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.954800\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.920241\n",
            "Average loss is: tensor(1.0358, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9415798611111111\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.884794\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.844816\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.790731\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.761494\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.697537\n",
            "Average loss is: tensor(0.7918, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9954861111111111\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.637188\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.634852\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.581966\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.453513\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.289015\n",
            "Average loss is: tensor(1.5088, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6677951388888889\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.196818\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.208219\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.060780\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.049142\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.040386\n",
            "Average loss is: tensor(1.1197, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9074652777777777\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.959117\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.952244\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.918974\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.880735\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.899802\n",
            "Average loss is: tensor(0.9168, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9767361111111111\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.640117\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.635914\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.629808\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.407605\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.378562\n",
            "Average loss is: tensor(1.5087, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.65234375\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.150577\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.002790\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.970145\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.880893\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.869319\n",
            "Average loss is: tensor(0.9549, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9690972222222223\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.850333\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.849599\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.829746\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.820522\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.804242\n",
            "Average loss is: tensor(0.8321, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9970486111111111\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.639504\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.631697\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.604204\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.522536\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.360026\n",
            "Average loss is: tensor(1.5463, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.63984375\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.269627\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.053259\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.990469\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.944652\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.925571\n",
            "Average loss is: tensor(1.0204, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9644965277777777\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.919739\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.874575\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.866216\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.803766\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.750579\n",
            "Average loss is: tensor(0.8253, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9963541666666667\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.637599\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640020\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.624159\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.524479\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.366380\n",
            "Average loss is: tensor(1.5647, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6252604166666667\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.362874\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.145160\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.042918\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.078288\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.948118\n",
            "Average loss is: tensor(1.0592, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9529513888888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.951012\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.885499\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.834073\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.852607\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.759855\n",
            "Average loss is: tensor(0.8483, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9899305555555555\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.638972\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.617826\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.534988\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.427214\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.352143\n",
            "Average loss is: tensor(1.5081, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6832465277777777\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.321903\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.222241\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.068471\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.034448\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.052734\n",
            "Average loss is: tensor(1.0991, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9280381944444445\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.972671\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.924181\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.888205\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.888744\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.879024\n",
            "Average loss is: tensor(0.9109, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9859375\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.641324\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.635215\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.598709\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.620708\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.453897\n",
            "Average loss is: tensor(1.5702, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.62421875\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.299497\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.131178\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.101151\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.038472\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.079686\n",
            "Average loss is: tensor(1.1298, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9365451388888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.976528\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.958323\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.938597\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.904493\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.943047\n",
            "Average loss is: tensor(0.9422, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9871527777777778\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.640493\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.632083\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.519998\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.496435\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.289477\n",
            "Average loss is: tensor(1.5033, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6793402777777777\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.127681\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.044847\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.007662\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.968103\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.977190\n",
            "Average loss is: tensor(1.0263, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9647569444444445\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.917502\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.922534\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.889221\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.904745\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.845763\n",
            "Average loss is: tensor(0.8961, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9917534722222222\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.636746\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.632480\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.541165\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.347471\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.126705\n",
            "Average loss is: tensor(1.4508, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6989583333333333\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.068056\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 0.927408\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 0.945360\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 0.906809\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 0.877330\n",
            "Average loss is: tensor(0.9274, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9762152777777777\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.844978\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.808288\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.773518\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.729125\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.681374\n",
            "Average loss is: tensor(0.7633, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9982638888888888\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64), ('sheena_model', 'politifact', precision    0.632436\n",
            "recall       0.633402\n",
            "accuracy     0.631072\n",
            "f1           0.630514\n",
            "auc          0.632436\n",
            "dtype: float64, precision    0.012012\n",
            "recall       0.012595\n",
            "accuracy     0.011529\n",
            "f1           0.011450\n",
            "auc          0.012012\n",
            "dtype: float64), ('broke_declare', 'politifact', precision    0.609768\n",
            "recall       0.586451\n",
            "accuracy     0.611636\n",
            "f1           0.593040\n",
            "auc          0.609768\n",
            "dtype: float64, precision    0.040286\n",
            "recall       0.115097\n",
            "accuracy     0.033908\n",
            "f1           0.088734\n",
            "auc          0.040286\n",
            "dtype: float64), ('real_declare', 'politifact', precision    0.608128\n",
            "recall       0.591186\n",
            "accuracy     0.608555\n",
            "f1           0.585739\n",
            "auc          0.608128\n",
            "dtype: float64, precision    0.039355\n",
            "recall       0.116572\n",
            "accuracy     0.033756\n",
            "f1           0.087624\n",
            "auc          0.039355\n",
            "dtype: float64), ('my_model', 'snopes', precision    0.665821\n",
            "recall       0.667794\n",
            "accuracy     0.668721\n",
            "f1           0.663418\n",
            "auc          0.665821\n",
            "dtype: float64, precision    0.025402\n",
            "recall       0.024066\n",
            "accuracy     0.024999\n",
            "f1           0.025761\n",
            "auc          0.025402\n",
            "dtype: float64), ('sheena_model', 'snopes', precision    0.664485\n",
            "recall       0.668358\n",
            "accuracy     0.664623\n",
            "f1           0.659211\n",
            "auc          0.664485\n",
            "dtype: float64, precision    0.014321\n",
            "recall       0.011208\n",
            "accuracy     0.013563\n",
            "f1           0.016334\n",
            "auc          0.014321\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.646261\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640248\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.635933\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.631612\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.578871\n",
            "Average loss is: tensor(1.6294, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5549479166666667\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.517204\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.392424\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.320156\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.286164\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.241429\n",
            "Average loss is: tensor(1.3211, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8326388888888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.102693\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.998649\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.015724\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.006582\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.031325\n",
            "Average loss is: tensor(1.0445, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9662326388888889\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.641591\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640364\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.642937\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.644703\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.610442\n",
            "Average loss is: tensor(1.6358, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5375\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.592708\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.394181\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.356912\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.355065\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.214321\n",
            "Average loss is: tensor(1.3632, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8040798611111111\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.125399\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.034968\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.033760\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.054497\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.019388\n",
            "Average loss is: tensor(1.0396, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9677083333333333\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.650154\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.644631\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.638211\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.637665\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.548369\n",
            "Average loss is: tensor(1.6204, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5705729166666667\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.388022\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.384083\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.266215\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.207217\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.235627\n",
            "Average loss is: tensor(1.2701, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8638020833333333\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.051804\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.064590\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.018757\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.043112\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.997015\n",
            "Average loss is: tensor(1.0283, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9728298611111111\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.647341\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.636547\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.644146\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.576765\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.410616\n",
            "Average loss is: tensor(1.5831, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6146701388888889\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.283903\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.232789\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.113362\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.112617\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.072290\n",
            "Average loss is: tensor(1.1610, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9223958333333333\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.003221\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.998193\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.987964\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.976789\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.956536\n",
            "Average loss is: tensor(0.9833, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9885416666666667\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.641706\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.641489\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.637791\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.606189\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.506094\n",
            "Average loss is: tensor(1.6105, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5688368055555556\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.427337\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.337361\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.178502\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.200770\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.113837\n",
            "Average loss is: tensor(1.2308, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8828993055555555\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.004242\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.018205\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.987507\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.012501\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.983809\n",
            "Average loss is: tensor(0.9998, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9830729166666666\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.666346\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.638413\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.636046\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.639221\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.630586\n",
            "Average loss is: tensor(1.6384, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5466145833333333\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.616407\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.481279\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.283789\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.144242\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.158783\n",
            "Average loss is: tensor(1.3293, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8402777777777778\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.050059\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.014656\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.045642\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.015292\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.030381\n",
            "Average loss is: tensor(1.0147, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.98046875\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.639698\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.645823\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.636394\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.546541\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.502716\n",
            "Average loss is: tensor(1.5839, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6175347222222223\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.329665\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.314390\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.233084\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.195001\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.118579\n",
            "Average loss is: tensor(1.1996, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8983506944444445\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.006221\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.998519\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.968825\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.965146\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.957856\n",
            "Average loss is: tensor(0.9889, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9872395833333333\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.647267\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.638841\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.614416\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.560321\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.406508\n",
            "Average loss is: tensor(1.5657, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6339409722222222\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.240566\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.179863\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.114087\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.062956\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.068669\n",
            "Average loss is: tensor(1.1169, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9365451388888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 0.984225\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.976101\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 0.954919\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.961727\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.974581\n",
            "Average loss is: tensor(0.9731, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9936631944444444\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.643004\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640797\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.615150\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.559606\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.461101\n",
            "Average loss is: tensor(1.5850, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6397569444444444\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 2.558320\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.266632\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.186174\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.130924\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.138581\n",
            "Average loss is: tensor(1.2544, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8848090277777778\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.059160\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.018430\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.042991\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.988788\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.977830\n",
            "Average loss is: tensor(0.9993, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9828125\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.652401\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640672\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.645455\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.627524\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.609451\n",
            "Average loss is: tensor(1.6208, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5776041666666667\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.487477\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.387507\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.266439\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.202722\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.133311\n",
            "Average loss is: tensor(1.2833, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8683159722222222\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.012274\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.975340\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.019155\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.009367\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.035975\n",
            "Average loss is: tensor(1.0129, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9785590277777778\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64), ('sheena_model', 'politifact', precision    0.632436\n",
            "recall       0.633402\n",
            "accuracy     0.631072\n",
            "f1           0.630514\n",
            "auc          0.632436\n",
            "dtype: float64, precision    0.012012\n",
            "recall       0.012595\n",
            "accuracy     0.011529\n",
            "f1           0.011450\n",
            "auc          0.012012\n",
            "dtype: float64), ('broke_declare', 'politifact', precision    0.609768\n",
            "recall       0.586451\n",
            "accuracy     0.611636\n",
            "f1           0.593040\n",
            "auc          0.609768\n",
            "dtype: float64, precision    0.040286\n",
            "recall       0.115097\n",
            "accuracy     0.033908\n",
            "f1           0.088734\n",
            "auc          0.040286\n",
            "dtype: float64), ('real_declare', 'politifact', precision    0.608128\n",
            "recall       0.591186\n",
            "accuracy     0.608555\n",
            "f1           0.585739\n",
            "auc          0.608128\n",
            "dtype: float64, precision    0.039355\n",
            "recall       0.116572\n",
            "accuracy     0.033756\n",
            "f1           0.087624\n",
            "auc          0.039355\n",
            "dtype: float64), ('my_model', 'snopes', precision    0.665821\n",
            "recall       0.667794\n",
            "accuracy     0.668721\n",
            "f1           0.663418\n",
            "auc          0.665821\n",
            "dtype: float64, precision    0.025402\n",
            "recall       0.024066\n",
            "accuracy     0.024999\n",
            "f1           0.025761\n",
            "auc          0.025402\n",
            "dtype: float64), ('sheena_model', 'snopes', precision    0.664485\n",
            "recall       0.668358\n",
            "accuracy     0.664623\n",
            "f1           0.659211\n",
            "auc          0.664485\n",
            "dtype: float64, precision    0.014321\n",
            "recall       0.011208\n",
            "accuracy     0.013563\n",
            "f1           0.016334\n",
            "auc          0.014321\n",
            "dtype: float64), ('broke_declare', 'snopes', precision    0.655288\n",
            "recall       0.657719\n",
            "accuracy     0.649574\n",
            "f1           0.646893\n",
            "auc          0.655288\n",
            "dtype: float64, precision    0.028610\n",
            "recall       0.028034\n",
            "accuracy     0.034236\n",
            "f1           0.033503\n",
            "auc          0.028610\n",
            "dtype: float64)]\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.655838\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.646275\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.642861\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.605739\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.545354\n",
            "Average loss is: tensor(1.6257, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5847222222222223\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.595628\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.564728\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.426004\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.294894\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.430853\n",
            "Average loss is: tensor(1.4550, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7740451388888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.214383\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.286735\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.264625\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.365914\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.312872\n",
            "Average loss is: tensor(1.3458, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8440104166666667\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.649959\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.640609\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.649414\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.578494\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.476549\n",
            "Average loss is: tensor(1.6080, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6243923611111111\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.421974\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.268353\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.390723\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.551773\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.099511\n",
            "Average loss is: tensor(1.2962, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8686631944444444\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.118123\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.003614\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.114620\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.017777\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.128428\n",
            "Average loss is: tensor(1.0678, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9638888888888889\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.644678\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.627054\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.635218\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.617805\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.535098\n",
            "Average loss is: tensor(1.6064, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6021701388888889\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.799929\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.281091\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.299783\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.156072\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.117671\n",
            "Average loss is: tensor(1.2796, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8728298611111112\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.079200\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.171199\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.035432\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.033942\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.169606\n",
            "Average loss is: tensor(1.0964, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9633680555555556\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.653384\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.651745\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.773007\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.656365\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.664081\n",
            "Average loss is: tensor(1.6600, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5090277777777777\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.623710\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.636876\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.629109\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.570481\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.557107\n",
            "Average loss is: tensor(1.6665, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5677951388888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.541892\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.511170\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.488066\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.466879\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.475192\n",
            "Average loss is: tensor(1.5009, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7571180555555556\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.650655\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.718703\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 2.163024\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.511367\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.566937\n",
            "Average loss is: tensor(1.6479, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.6071180555555555\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.408337\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.327442\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.302312\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.180715\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.140190\n",
            "Average loss is: tensor(1.2592, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8770833333333333\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.064579\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.047681\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.062755\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.963941\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 0.983186\n",
            "Average loss is: tensor(1.0468, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9691840277777778\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.654462\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.641926\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.754068\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.619850\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.607456\n",
            "Average loss is: tensor(1.6394, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5592013888888889\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.669548\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.591030\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.534352\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.378183\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.348400\n",
            "Average loss is: tensor(1.4512, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8105902777777778\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.294281\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.302351\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.318077\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.279474\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.225344\n",
            "Average loss is: tensor(1.2988, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9105902777777778\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.666242\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.644264\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.962482\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.593894\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.598401\n",
            "Average loss is: tensor(1.6503, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5596354166666667\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.577818\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.280313\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.235226\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.232685\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.197013\n",
            "Average loss is: tensor(1.2982, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8513888888888889\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.107226\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.071213\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.106755\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 0.997244\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.026344\n",
            "Average loss is: tensor(1.0504, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9674479166666666\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.645792\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.641777\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.641801\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.640999\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.638013\n",
            "Average loss is: tensor(1.6426, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5190972222222222\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.642690\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.632176\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.641741\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.640783\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.639434\n",
            "Average loss is: tensor(1.6406, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5255208333333333\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.638997\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.638566\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.637910\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.648255\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.643211\n",
            "Average loss is: tensor(1.6407, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.52578125\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.642285\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.637850\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.621503\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.586781\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.567463\n",
            "Average loss is: tensor(1.6312, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.5671875\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.528988\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.462423\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.500908\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.433039\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.319412\n",
            "Average loss is: tensor(1.4661, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.7850694444444445\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.392236\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 1.311257\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.369350\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.219929\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.224392\n",
            "Average loss is: tensor(1.2560, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9198784722222222\n",
            "Running EPOCH: 1\n",
            "Train Epoch: 0 [0/11721 (0%)]\tLoss: 1.651227\n",
            "Train Epoch: 0 [2560/11721 (22%)]\tLoss: 1.641584\n",
            "Train Epoch: 0 [5120/11721 (44%)]\tLoss: 1.631235\n",
            "Train Epoch: 0 [7680/11721 (67%)]\tLoss: 1.521641\n",
            "Train Epoch: 0 [10240/11721 (89%)]\tLoss: 1.540546\n",
            "Average loss is: tensor(1.5953, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.61484375\n",
            "Running EPOCH: 2\n",
            "Train Epoch: 1 [0/11721 (0%)]\tLoss: 1.495647\n",
            "Train Epoch: 1 [2560/11721 (22%)]\tLoss: 1.319828\n",
            "Train Epoch: 1 [5120/11721 (44%)]\tLoss: 1.334935\n",
            "Train Epoch: 1 [7680/11721 (67%)]\tLoss: 1.345401\n",
            "Train Epoch: 1 [10240/11721 (89%)]\tLoss: 1.166552\n",
            "Average loss is: tensor(1.2882, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.8571180555555555\n",
            "Running EPOCH: 3\n",
            "Train Epoch: 2 [0/11721 (0%)]\tLoss: 1.070743\n",
            "Train Epoch: 2 [2560/11721 (22%)]\tLoss: 0.993667\n",
            "Train Epoch: 2 [5120/11721 (44%)]\tLoss: 1.146979\n",
            "Train Epoch: 2 [7680/11721 (67%)]\tLoss: 1.039026\n",
            "Train Epoch: 2 [10240/11721 (89%)]\tLoss: 1.030846\n",
            "Average loss is: tensor(1.1104, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.9499131944444444\n",
            "[('my_model', 'politifact', precision    0.616111\n",
            "recall       0.616884\n",
            "accuracy     0.614749\n",
            "f1           0.614245\n",
            "auc          0.616111\n",
            "dtype: float64, precision    0.011587\n",
            "recall       0.012001\n",
            "accuracy     0.011775\n",
            "f1           0.011691\n",
            "auc          0.011587\n",
            "dtype: float64), ('sheena_model', 'politifact', precision    0.632436\n",
            "recall       0.633402\n",
            "accuracy     0.631072\n",
            "f1           0.630514\n",
            "auc          0.632436\n",
            "dtype: float64, precision    0.012012\n",
            "recall       0.012595\n",
            "accuracy     0.011529\n",
            "f1           0.011450\n",
            "auc          0.012012\n",
            "dtype: float64), ('broke_declare', 'politifact', precision    0.609768\n",
            "recall       0.586451\n",
            "accuracy     0.611636\n",
            "f1           0.593040\n",
            "auc          0.609768\n",
            "dtype: float64, precision    0.040286\n",
            "recall       0.115097\n",
            "accuracy     0.033908\n",
            "f1           0.088734\n",
            "auc          0.040286\n",
            "dtype: float64), ('real_declare', 'politifact', precision    0.608128\n",
            "recall       0.591186\n",
            "accuracy     0.608555\n",
            "f1           0.585739\n",
            "auc          0.608128\n",
            "dtype: float64, precision    0.039355\n",
            "recall       0.116572\n",
            "accuracy     0.033756\n",
            "f1           0.087624\n",
            "auc          0.039355\n",
            "dtype: float64), ('my_model', 'snopes', precision    0.665821\n",
            "recall       0.667794\n",
            "accuracy     0.668721\n",
            "f1           0.663418\n",
            "auc          0.665821\n",
            "dtype: float64, precision    0.025402\n",
            "recall       0.024066\n",
            "accuracy     0.024999\n",
            "f1           0.025761\n",
            "auc          0.025402\n",
            "dtype: float64), ('sheena_model', 'snopes', precision    0.664485\n",
            "recall       0.668358\n",
            "accuracy     0.664623\n",
            "f1           0.659211\n",
            "auc          0.664485\n",
            "dtype: float64, precision    0.014321\n",
            "recall       0.011208\n",
            "accuracy     0.013563\n",
            "f1           0.016334\n",
            "auc          0.014321\n",
            "dtype: float64), ('broke_declare', 'snopes', precision    0.655288\n",
            "recall       0.657719\n",
            "accuracy     0.649574\n",
            "f1           0.646893\n",
            "auc          0.655288\n",
            "dtype: float64, precision    0.028610\n",
            "recall       0.028034\n",
            "accuracy     0.034236\n",
            "f1           0.033503\n",
            "auc          0.028610\n",
            "dtype: float64), ('real_declare', 'snopes', precision    0.638269\n",
            "recall       0.627327\n",
            "accuracy     0.638426\n",
            "f1           0.609377\n",
            "auc          0.638269\n",
            "dtype: float64, precision    0.057057\n",
            "recall       0.144186\n",
            "accuracy     0.072239\n",
            "f1           0.112832\n",
            "auc          0.057057\n",
            "dtype: float64)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUR-r4M717ol",
        "colab_type": "code",
        "outputId": "a740dd5d-388b-4809-b9ca-3eae325cb558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for result in full_results:\n",
        "  print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6159650423533861, 'recall': 0.6166411105620413, 'accuracy': 0.6138970829236315, 'f1': 0.6137051929513584, 'auc': 0.6159650423533862}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6018810764573677, 'recall': 0.6028130181590461, 'accuracy': 0.5994755817764668, 'f1': 0.599095030895942, 'auc': 0.6018810764573677}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6126830275748218, 'recall': 0.6125150125043307, 'accuracy': 0.612094395280236, 'f1': 0.612035785541073, 'auc': 0.6126830275748218}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6004185885878162, 'recall': 0.6002776859191488, 'accuracy': 0.599803343166175, 'f1': 0.599753638082202, 'auc': 0.6004185885878162}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6235426645777775, 'recall': 0.623320277463816, 'accuracy': 0.6234021632251721, 'f1': 0.6231981091109211, 'auc': 0.6235426645777775}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6392856916080274, 'recall': 0.6391408109939862, 'accuracy': 0.6396263520157326, 'f1': 0.6391784066651932, 'auc': 0.6392856916080274}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6233115443555491, 'recall': 0.6243466713665812, 'accuracy': 0.6209439528023599, 'f1': 0.6206247467447248, 'auc': 0.6233115443555491}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6079872242983282, 'recall': 0.6078237080734235, 'accuracy': 0.6081612586037365, 'f1': 0.607808517613075, 'auc': 0.6079872242983282}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.614115811518257, 'recall': 0.6140821824660765, 'accuracy': 0.6130776794493609, 'f1': 0.6130764220704881, 'auc': 0.614115811518257}\n",
            "{'model_name': 'my_model', 'dataset_name': 'politifact', 'precision': 0.6219176122864909, 'recall': 0.6278816846459164, 'accuracy': 0.6170108161258604, 'f1': 0.6139764785741044, 'auc': 0.621917612286491}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6193021600673547, 'recall': 0.6210740389614362, 'accuracy': 0.6163552933464438, 'f1': 0.6156521221129377, 'auc': 0.6193021600673547}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6570567111447085, 'recall': 0.6600551546717394, 'accuracy': 0.6537200917731891, 'f1': 0.6528413519006585, 'auc': 0.6570567111447085}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6294233962113073, 'recall': 0.6297685816908088, 'accuracy': 0.6306129137987545, 'f1': 0.629502887278476, 'auc': 0.6294233962113073}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6278819669294137, 'recall': 0.6289554599173116, 'accuracy': 0.6297935103244838, 'f1': 0.6278849950065375, 'auc': 0.6278819669294137}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6403531495471066, 'recall': 0.6403855426044536, 'accuracy': 0.63913470993117, 'f1': 0.6391340896602438, 'auc': 0.6403531495471066}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6302473034808158, 'recall': 0.6302294334990213, 'accuracy': 0.6291379875450672, 'f1': 0.629137738539966, 'auc': 0.6302473034808158}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6431561885170434, 'recall': 0.6443291882704177, 'accuracy': 0.6407735168797116, 'f1': 0.6404811853136299, 'auc': 0.6431561885170434}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6353636971314509, 'recall': 0.6373274452462934, 'accuracy': 0.6324156014421501, 'f1': 0.6317623957261492, 'auc': 0.635363697131451}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6245294591377832, 'recall': 0.6247183950633918, 'accuracy': 0.6230744018354638, 'f1': 0.6230529802519791, 'auc': 0.6245294591377832}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'politifact', 'precision': 0.6170504733202242, 'recall': 0.6171724036288606, 'accuracy': 0.6156997705670272, 'f1': 0.615688530553887, 'auc': 0.6170504733202241}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.6235089786035948, 'recall': 0.623488724312604, 'accuracy': 0.6224188790560472, 'f1': 0.6224185139922342, 'auc': 0.6235089786035948}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.6352835718669811, 'recall': 0.6368596590611535, 'accuracy': 0.6374959029826286, 'f1': 0.6352198579106905, 'auc': 0.6352835718669811}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.6333844039966418, 'recall': 0.6334691679769109, 'accuracy': 0.6320878400524418, 'f1': 0.6320834824915091, 'auc': 0.6333844039966416}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.5927667065481337, 'recall': 0.5927936707079695, 'accuracy': 0.5935758767617175, 'f1': 0.5927785915230988, 'auc': 0.5927667065481337}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.5, 'recall': 0.2606522451655195, 'accuracy': 0.521304490331039, 'f1': 0.3426693956695034, 'auc': 0.5}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.618657714018133, 'recall': 0.6189794374966546, 'accuracy': 0.6170108161258604, 'f1': 0.6169529492224282, 'auc': 0.6186577140181329}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.6220416477026112, 'recall': 0.6219563857380093, 'accuracy': 0.621107833497214, 'f1': 0.6210998554532168, 'auc': 0.6220416477026112}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.6196864815172163, 'recall': 0.6215479703587121, 'accuracy': 0.616683054736152, 'f1': 0.6159366695743826, 'auc': 0.6196864815172163}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.6246470909901212, 'recall': 0.6244569438067067, 'accuracy': 0.6240576860045887, 'f1': 0.6239962480176382, 'auc': 0.6246470909901212}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'politifact', 'precision': 0.6277025057091269, 'recall': 0.6303053304592163, 'accuracy': 0.6306129137987545, 'f1': 0.6272452027486365, 'auc': 0.6277025057091269}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6261008075916347, 'recall': 0.6420982482863671, 'accuracy': 0.6330711242215668, 'f1': 0.6192890231121283, 'auc': 0.6261008075916348}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6202899770451072, 'recall': 0.6369826422647598, 'accuracy': 0.6125860373647984, 'f1': 0.6031278475496217, 'auc': 0.6202899770451072}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6123045715741391, 'recall': 0.6132397681380233, 'accuracy': 0.6099639462471321, 'f1': 0.6096391977642491, 'auc': 0.6123045715741391}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.5, 'recall': 0.2606522451655195, 'accuracy': 0.521304490331039, 'f1': 0.3426693956695034, 'auc': 0.5}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6207076616003895, 'recall': 0.6207650567415558, 'accuracy': 0.6194690265486725, 'f1': 0.6194664102425548, 'auc': 0.6207076616003895}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6277677790105385, 'recall': 0.628048206992896, 'accuracy': 0.6261881350376926, 'f1': 0.6261482843270567, 'auc': 0.6277677790105386}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.5990769612582239, 'recall': 0.6339044896138925, 'accuracy': 0.5880039331366765, 'f1': 0.5640623976967678, 'auc': 0.5990769612582239}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6365484640541059, 'recall': 0.6377409728118539, 'accuracy': 0.6384791871517536, 'f1': 0.6365723301683845, 'auc': 0.6365484640541059}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6255169532467737, 'recall': 0.6253359440823976, 'accuracy': 0.6248770894788593, 'f1': 0.6248262963833144, 'auc': 0.6255169532467737}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'politifact', 'precision': 0.6129669906511197, 'recall': 0.6130966933141688, 'accuracy': 0.6116027531956736, 'f1': 0.6115892339688644, 'auc': 0.6129669906511197}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.684506067702789, 'recall': 0.6831393951519136, 'accuracy': 0.6868852459016394, 'f1': 0.6835954893116596, 'auc': 0.684506067702789}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6537151373216947, 'recall': 0.667105136475803, 'accuracy': 0.6698360655737705, 'f1': 0.6541778404262709, 'auc': 0.6537151373216947}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6592505854800936, 'recall': 0.6630161634413174, 'accuracy': 0.6685245901639344, 'f1': 0.6602137902148668, 'auc': 0.6592505854800936}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6404087715563125, 'recall': 0.6436162617182759, 'accuracy': 0.6498360655737705, 'f1': 0.6411695137513131, 'auc': 0.6404087715563126}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6609271875665319, 'recall': 0.6638610703408474, 'accuracy': 0.6478688524590164, 'f1': 0.6475619728008264, 'auc': 0.6609271875665319}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6724771130508835, 'recall': 0.6707877410188055, 'accuracy': 0.6659016393442623, 'f1': 0.6656773439724517, 'auc': 0.6724771130508835}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.7084043006174154, 'recall': 0.7073033607905884, 'accuracy': 0.7111475409836066, 'f1': 0.7077469755855257, 'auc': 0.7084043006174154}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6569885032999787, 'recall': 0.6550182249450625, 'accuracy': 0.6521311475409836, 'f1': 0.6516184902459529, 'auc': 0.6569885032999787}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6969076005961252, 'recall': 0.6977870622591965, 'accuracy': 0.7022950819672131, 'f1': 0.6972948252652031, 'auc': 0.6969076005961251}\n",
            "{'model_name': 'my_model', 'dataset_name': 'snopes', 'precision': 0.6246274217585693, 'recall': 0.6263092408260289, 'accuracy': 0.6327868852459017, 'f1': 0.6251263662146674, 'auc': 0.6246274217585693}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6639078134979775, 'recall': 0.6667354557655029, 'accuracy': 0.6721311475409836, 'f1': 0.66476870799442, 'auc': 0.6639078134979775}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6797423887587821, 'recall': 0.6824997873757921, 'accuracy': 0.6668852459016393, 'f1': 0.6666672403644587, 'auc': 0.6797423887587822}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6656908665105387, 'recall': 0.6666481641479107, 'accuracy': 0.6718032786885246, 'f1': 0.6660887935525241, 'auc': 0.6656908665105385}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6799020651479668, 'recall': 0.6799598498734398, 'accuracy': 0.6845901639344262, 'f1': 0.6799306561874255, 'auc': 0.6799020651479667}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6747924206940601, 'recall': 0.6722879297651729, 'accuracy': 0.6727868852459017, 'f1': 0.6714300086225471, 'auc': 0.6747924206940601}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6584522035341708, 'recall': 0.6781822267160993, 'accuracy': 0.6350819672131147, 'f1': 0.6301274443115527, 'auc': 0.6584522035341707}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6573078560783479, 'recall': 0.6557256899816188, 'accuracy': 0.659016393442623, 'f1': 0.6560340860244437, 'auc': 0.6573078560783479}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6771343410687674, 'recall': 0.6753993526031737, 'accuracy': 0.6704918032786885, 'f1': 0.6702705894919667, 'auc': 0.6771343410687672}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6345539706195444, 'recall': 0.6521157242705211, 'accuracy': 0.6540983606557377, 'f1': 0.6331400803483365, 'auc': 0.6345539706195443}\n",
            "{'model_name': 'sheena_model', 'dataset_name': 'snopes', 'precision': 0.6533691718117947, 'recall': 0.6540260283101578, 'accuracy': 0.6593442622950819, 'f1': 0.6536522449732607, 'auc': 0.6533691718117947}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.6141952309985097, 'recall': 0.6135633863496246, 'accuracy': 0.6068852459016394, 'f1': 0.6068209593850282, 'auc': 0.6141952309985097}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.656961890568448, 'recall': 0.6547048989153654, 'accuracy': 0.6544262295081967, 'f1': 0.653271673567366, 'auc': 0.6569618905684479}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.6001170960187353, 'recall': 0.6096774131879392, 'accuracy': 0.579672131147541, 'f1': 0.5755658073270014, 'auc': 0.6001170960187354}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.671865020225676, 'recall': 0.6834367772256734, 'accuracy': 0.6527868852459017, 'f1': 0.6506416881341479, 'auc': 0.671865020225676}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.6371886310410901, 'recall': 0.6374606574575974, 'accuracy': 0.6275409836065574, 'f1': 0.6275352179598372, 'auc': 0.6371886310410901}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.6825367255695125, 'recall': 0.6824787689108469, 'accuracy': 0.6727868852459017, 'f1': 0.67278674454684, 'auc': 0.6825367255695125}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.6756972535661061, 'recall': 0.6756692799697797, 'accuracy': 0.680327868852459, 'f1': 0.6756831919644684, 'auc': 0.6756972535661061}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.664519906323185, 'recall': 0.6670824876239543, 'accuracy': 0.6724590163934426, 'f1': 0.6653349039895216, 'auc': 0.664519906323185}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.672077922077922, 'recall': 0.6755607425053323, 'accuracy': 0.680655737704918, 'f1': 0.6730897644644052, 'auc': 0.6720779220779222}\n",
            "{'model_name': 'broke_declare', 'dataset_name': 'snopes', 'precision': 0.6777198211624441, 'recall': 0.6775525041232693, 'accuracy': 0.6681967213114755, 'f1': 0.6681954372518937, 'auc': 0.6777198211624442}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6620183095592932, 'recall': 0.6598064600249327, 'accuracy': 0.6580327868852459, 'f1': 0.657300380420655, 'auc': 0.6620183095592931}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6717319565680222, 'recall': 0.6748628223763002, 'accuracy': 0.68, 'f1': 0.6726835942877526, 'auc': 0.6717319565680222}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6691239088780072, 'recall': 0.7065592987784571, 'accuracy': 0.6403278688524591, 'f1': 0.6309180547296067, 'auc': 0.6691239088780072}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6338354268682138, 'recall': 0.6507387337453024, 'accuracy': 0.6531147540983606, 'f1': 0.6325224205850306, 'auc': 0.6338354268682138}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6686714924419843, 'recall': 0.6672269474587532, 'accuracy': 0.6616393442622951, 'f1': 0.6614808288621901, 'auc': 0.6686714924419843}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6371354055780285, 'recall': 0.6743489363779165, 'accuracy': 0.6636065573770492, 'f1': 0.6310511710909557, 'auc': 0.6371354055780286}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6612997658079626, 'recall': 0.676137384120142, 'accuracy': 0.6777049180327869, 'f1': 0.6619219710293578, 'auc': 0.6612997658079626}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.5, 'recall': 0.22, 'accuracy': 0.44, 'f1': 0.3055555555555556, 'auc': 0.5}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.6945124547583563, 'recall': 0.6931366484380956, 'accuracy': 0.6865573770491803, 'f1': 0.6864662407648512, 'auc': 0.6945124547583564}\n",
            "{'model_name': 'real_declare', 'dataset_name': 'snopes', 'precision': 0.5843623589525229, 'recall': 0.6504561458110447, 'accuracy': 0.6232786885245901, 'f1': 0.55387394305316, 'auc': 0.584362358952523}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWgiTDHE2tP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}