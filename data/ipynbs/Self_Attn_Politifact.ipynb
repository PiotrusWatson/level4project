{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_Attn_Politifact.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/data/ipynbs/Self_Attn_Politifact.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv50EWWHrPzg",
        "colab_type": "text"
      },
      "source": [
        "#Hybrid Model on PolitiFact Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOgQQ0mWrWfJ",
        "colab_type": "text"
      },
      "source": [
        "The Hybrid model is created as a combination of the DeClarE model (Popat et al.,2018) and Self-Attentive Sentence Embedding model (Lin et al.,2017). This model uses the overall structure of DeClarE but replaces its attention mechanism with the Self-attention created in the second paper. The code for the Self-Attention model can be found at: https://github.com/kaushalshetty/Structured-Self-Attention/blob/master . This code have been used as the base for the Hybrid model. Before running the code in Google Colab, please upload the config.json and model_params.json file which is provided as a part of the project code zip file. Both of these file contain the hyper-parameters of the model and were originally created by the developer of this code base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTf9j0cnrGsF",
        "colab_type": "text"
      },
      "source": [
        "## Import Statements and Data load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogyFod2-5Gkb",
        "colab_type": "text"
      },
      "source": [
        "ALRIGHT NERDS ITS TIME FOR MORE ANNOTATIONS BAYBEEEEE\n",
        "\n",
        "LOOK AT ALL THESE IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ZE1b3gn8dD",
        "colab_type": "code",
        "outputId": "f4a6ff61-d05f-47d9-cd91-321abb7d8779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Most of the code taken from https://github.com/kaushalshetty/Structured-Self-Attention/blob/master\n",
        "# All imports\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "import gzip\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.optimizers import SGD, Adam\n",
        "from nltk import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch,keras\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "from keras.datasets import imdb\n",
        "\n",
        "import random, os, numpy, scipy\n",
        "from codecs import open\n",
        "import os,sys"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKq8B8IX532j",
        "colab_type": "text"
      },
      "source": [
        "AS WITH LAST TIME, THE SAME SEED - I'M THINKING TO DIRECTLY COMPARE THE TWO MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijDljoHJoMZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random seed set to ensure that code provides the same output whenever randomisation is done.\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXKQZ0k65_WD",
        "colab_type": "text"
      },
      "source": [
        "LETS GET THAT POLITIFACT DATASET UPPPPP MY DUDES. THIS IS USED AS THE TRAINING/TESTING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQox57_oPpU",
        "colab_type": "code",
        "outputId": "daec0bcd-f0c9-4359-d20c-6b1903caa2bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Get the PolitiFact Dataset from the location provided in the DeClarE paper.\n",
        "!wget http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
        "!unzip PolitiFact.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-22 15:49:15--  http://resources.mpi-inf.mpg.de/impact/dl_cred_analysis/PolitiFact.zip\n",
            "Resolving resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)... 139.19.206.46\n",
            "Connecting to resources.mpi-inf.mpg.de (resources.mpi-inf.mpg.de)|139.19.206.46|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4976217 (4.7M) [application/zip]\n",
            "Saving to: ‘PolitiFact.zip’\n",
            "\n",
            "PolitiFact.zip      100%[===================>]   4.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-10-22 15:49:20 (33.1 MB/s) - ‘PolitiFact.zip’ saved [4976217/4976217]\n",
            "\n",
            "Archive:  PolitiFact.zip\n",
            "   creating: PolitiFact/\n",
            "  inflating: PolitiFact/README       \n",
            "  inflating: PolitiFact/politifact.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkZnAQf46TgM",
        "colab_type": "text"
      },
      "source": [
        "WE TAKE THE GLOVE DATASET SO WE CAN USE PRETRAINED WORD EMBEDDINGS\n",
        "\n",
        "CAUSE WE'RE LAZY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nuFU97Xoe_0",
        "colab_type": "code",
        "outputId": "0dd040b7-7e45-4232-a290-c4fca841ed5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-22 15:49:23--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-10-22 15:49:24--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-10-22 15:49:24--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.01MB/s    in 6m 30s  \n",
            "\n",
            "2019-10-22 15:55:55 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSeDOppErPpt",
        "colab_type": "text"
      },
      "source": [
        "## Data Load functions and other misc functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixiqyMMm6qOz",
        "colab_type": "text"
      },
      "source": [
        "THIS IS QUITE NICE! A HELPER FUNCTION CREATED TO LOAD POLITIFACT INTO THE DATAFRAME (IS THIS BECAUSE IT WILL BE DONE MULTIPLE TIMES?)\n",
        "\n",
        "WE LOAD THE DATA, GIVING THEM LABELS, AND THEN CONVERT THE LABEL RANGE INTO BINARY VALUES: 1 FOR ANYTHING WITH \"TRUE\" IN IT (EVEN HALF-TRUE 🤔). THIS IS THEN STORED IN A LIST SOMEWHERE\n",
        "\n",
        "THEN WE USE THE KERAS FUNCTION train_test_split() TO SPLIT THE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc5TtB7moikm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to load the PolitiFact into the Datafarme, process it and divide it into traning and test sets.\n",
        "def loadPolitifact():\n",
        "  facts=list()\n",
        "  fact=pd.read_csv('./PolitiFact/politifact.tsv', delimiter = '\\t', names = ['cred_label','claim_id','claim_text','claim_source','article','article_source'])\n",
        "  \n",
        "  # Convert label to binary\n",
        "  claim_label = fact['cred_label']\n",
        "  # The original credibility labels are -{'False', 'Half-True', 'Mostly False', 'Mostly True', 'Pants on Fire!', 'True'}\n",
        "  # Convert the labels to binary values of 0 for false and 1 for true.\n",
        "  claim_label = [1 if tuple == 'True'  or tuple == 'Half-True' or tuple == 'Mostly True' else 0 for tuple in claim_label]\n",
        "  \n",
        "  # Get Train and test sets\n",
        "  input_train,input_test,label_train,label_test = train_test_split(fact, claim_label, test_size=0.2, random_state=8)\n",
        "  # Convert the traning and test data as lists and return.\n",
        "  train_list= list()\n",
        "  train_list.append(input_train)\n",
        "  train_list.append(label_train)\n",
        "  \n",
        "  test_list = list()\n",
        "  test_list.append(input_test)\n",
        "  test_list.append(label_test)\n",
        "  return train_list, test_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McZ9Q2gPpPFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Politifact dataset. This method calls the previous method\n",
        "def load_data_set(type,max_len,vocab_size,batch_size):\n",
        "    \"\"\"\n",
        "        Loads the dataset.\n",
        " \n",
        "        Args:\n",
        "            type   : {bool} 0 for binary classification returns imdb dataset. 1 for multiclass classfication return reuters set\n",
        "            max_len: {int} timesteps used for padding\n",
        "\t\t\tvocab_size: {int} size of the vocabulary\n",
        "\t\t\tbatch_size: batch_size\n",
        "      \n",
        "        \"\"\"\n",
        "    NUM_WORDS=vocab_size\n",
        "    # Get the PolitiFact dataframe \n",
        "    train_set,test_set = loadPolitifact()\n",
        "    x_train,y_train = train_set[0],train_set[1]\n",
        "    x_test,y_test = test_set[0],test_set[1]\n",
        "\n",
        "    # Code to check how many unique claims are present in the test set.\n",
        "    # Code found here: https://stackoverflow.com/questions/23460345/selecting-unique-rows-between-two-dataframes-in-pandas\n",
        "    unique_test = x_test[~x_test['claim_text'].isin(x_train['claim_text'])]\n",
        "    uniq_set = set(unique_test['claim_text'].tolist())\n",
        "    print(\"There are %d total unique rows in the test set and %d unique claims\" % (len(unique_test),len(uniq_set)))\n",
        "    # Get the required columns as lists for both traning and test data\n",
        "    art_list = x_train['article'].tolist()\n",
        "    clm_list = x_train['claim_text'].tolist()\n",
        "    clm_src_list = x_train['claim_source'].tolist()\n",
        "    art_src_list = x_train['article_source'].tolist()\n",
        "    \n",
        "    test_art_list = x_test['article'].tolist()\n",
        "    test_clm_list = x_test['claim_text'].tolist()\n",
        "    test_clm_src_list = x_test['claim_source'].tolist()\n",
        "    test_art_src_list = x_test['article_source'].tolist()\n",
        "\n",
        "    # Text to Sequence conversion\n",
        "    t = Tokenizer()\n",
        "    t.fit_on_texts(clm_list+art_list+clm_src_list+art_src_list)\n",
        "    t.num_words=NUM_WORDS\n",
        "    encoded_claim = t.texts_to_sequences(clm_list) # claim\n",
        "    encoded_art = t.texts_to_sequences(art_list) # article\n",
        "    encoded_claim_src = t.texts_to_sequences(clm_src_list) # claim source\n",
        "    encoded_art_src = t.texts_to_sequences(art_src_list) # article source\n",
        "    \n",
        "    encoded_claim_test = t.texts_to_sequences(test_clm_list)\n",
        "    encoded_art_test = t.texts_to_sequences(test_art_list)\n",
        "    encoded_claim_src_test = t.texts_to_sequences(test_clm_src_list)\n",
        "    encoded_art_src_test = t.texts_to_sequences(test_art_src_list)\n",
        "    \n",
        "    word_to_id = t.word_index\n",
        "\n",
        "    # Add a padding of 150 to article and 50 to claim. Padding of 8 given to article source.\n",
        "    # This is done for the training data.\n",
        "    x_train_pad_cl = pad_sequences(encoded_claim, maxlen=50, padding='post') # claim padding\n",
        "    x_train_pad = pad_sequences(encoded_art, maxlen=max_len, padding='post') # article padding\n",
        "    x_train_pad_cls = pad_sequences(encoded_claim_src, maxlen=4, padding='post') # claim source padding\n",
        "    x_train_pad_s = pad_sequences(encoded_art_src, maxlen=4, padding='post') # article source padding\n",
        "    \n",
        "    # Test padding\n",
        "    x_test_pad_cl = pad_sequences(encoded_claim_test, maxlen=50, padding='post')\n",
        "    x_test_pad = pad_sequences(encoded_art_test, maxlen=max_len, padding='post')\n",
        "    x_test_pad_cls= pad_sequences(encoded_claim_src_test, maxlen=4, padding='post')\n",
        "    x_test_pad_s = pad_sequences(encoded_art_src_test, maxlen=4, padding='post')\n",
        "    # Covert training data to a Numpy array\n",
        "    x_train_pad_np_cl = np.array(x_train_pad_cl, dtype=np.float32) # claim numpy\n",
        "    x_train_pad_np = np.array(x_train_pad, dtype=np.float32) # article numpy\n",
        "    x_train_pad_np_cls = np.array(x_train_pad_cls, dtype=np.float32) # claim source numpy\n",
        "    x_train_pad_np_s = np.array(x_train_pad_s, dtype=np.float32) # article source numpy\n",
        "    \n",
        "    # Test data numpy\n",
        "    x_test_pad_np_cl = np.array(x_test_pad_cl, dtype=np.float32)\n",
        "    x_test_pad_np = np.array(x_test_pad, dtype=np.float32)\n",
        "    x_test_pad_np_cls = np.array(x_test_pad_cls, dtype=np.float32)\n",
        "    x_test_pad_np_s = np.array(x_test_pad_s, dtype=np.float32)\n",
        "    \n",
        "    y_train_np = np.array(y_train, dtype=np.float32)\n",
        "    y_test_np = np.array(y_test, dtype=np.float32)\n",
        "    # Convert the training data to a Tensor Variable to be used by the Hybrid model.\n",
        "    train_data = data_utils.TensorDataset(torch.from_numpy(x_train_pad_np_cl).type(torch.LongTensor),\n",
        "                                          torch.from_numpy(x_train_pad_np).type(torch.LongTensor),\n",
        "                                          torch.from_numpy(x_train_pad_np_cls).type(torch.LongTensor),\n",
        "                                          torch.from_numpy(x_train_pad_np_s).type(torch.LongTensor),\n",
        "                                          torch.from_numpy(y_train_np).type(torch.DoubleTensor))\n",
        "    train_loader = data_utils.DataLoader(train_data,batch_size=batch_size,drop_last=True)\n",
        "    return train_loader, x_test_pad_np_cl, x_test_pad_np,x_test_pad_np_cls, x_test_pad_np_s, y_test_np, word_to_id, x_test, x_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ8fHDT2piV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxpfhq33qJOR",
        "colab_type": "code",
        "outputId": "4e9fb9af-a4a2-4a94-cd24-30e03fd5360e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Method to read and store the contents of the uploaded json files.\n",
        "def json_to_dict(json_set):\n",
        "    for k,v in json_set.items():\n",
        "        if v == 'False':\n",
        "            json_set[k] = False\n",
        "        elif v == 'True':\n",
        "            json_set[k] = True\n",
        "        else:\n",
        "            json_set[k] = v\n",
        "    return json_set\n",
        "\n",
        "  \n",
        "\"\"\"\n",
        "with open('config.json', 'r') as f:\n",
        "    params_set = json.load(f)\n",
        "\n",
        "with open('model_params.json', 'r') as f:\n",
        "    model_params = json.load(f)\n",
        "\"\"\"\n",
        "\n",
        "params_set = {\n",
        "\t\n",
        "\t\"epochs\":2,\n",
        "\t\"use_regularization\":True,\n",
        "\t\"C\":0.03,\n",
        "\t\"clip\":True,\n",
        "\t\"use_embeddings\":True,\n",
        "\t\"attention_hops\":10\n",
        "\n",
        "}\n",
        "model_params = {\n",
        "\n",
        "\t\"batch_size\":512,\n",
        "\t\"vocab_size\":20000,\n",
        "\t\"timesteps\":100,\n",
        "\t\"lstm_hidden_dimension\":50,\n",
        "\t\"d_a\":100\n",
        "\n",
        "}\n",
        "print(\"Using settings:\",params_set)\n",
        "print(\"Using model settings\",model_params)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using settings: {'epochs': 2, 'use_regularization': True, 'C': 0.03, 'clip': True, 'use_embeddings': True, 'attention_hops': 10}\n",
            "Using model settings {'batch_size': 512, 'vocab_size': 20000, 'timesteps': 100, 'lstm_hidden_dimension': 50, 'd_a': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyvbeDI8qtmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creates and HTML file which show the attention weights visulaization on the article. This code taken from: https://github.com/kaushalshetty/Structured-Self-Attention/blob/master\n",
        "\n",
        "def createHTML(texts, weights, fileName):\n",
        "    \"\"\"\n",
        "    Creates a html file with text heat.\n",
        "\tweights: attention weights for visualizing\n",
        "\ttexts: text on which attention weights are to be visualized\n",
        "    \"\"\"\n",
        "    fileName = \"visualization/\"+fileName\n",
        "    fOut = open(fileName, \"w\", encoding=\"utf-8\")\n",
        "    part1 = \"\"\"\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\">\n",
        "    <style>\n",
        "    body {\n",
        "    font-family: Sans-Serif;\n",
        "    }\n",
        "    </style>\n",
        "    </head>\n",
        "    <body>\n",
        "    <h3>\n",
        "    Heatmaps\n",
        "    </h3>\n",
        "    </body>\n",
        "    <script>\n",
        "    \"\"\"\n",
        "    part2 = \"\"\"\n",
        "    var color = \"255,0,0\";\n",
        "    var ngram_length = 3;\n",
        "    var half_ngram = 1;\n",
        "    for (var k=0; k < any_text.length; k++) {\n",
        "    var tokens = any_text[k].split(\" \");\n",
        "    var intensity = new Array(tokens.length);\n",
        "    var max_intensity = Number.MIN_SAFE_INTEGER;\n",
        "    var min_intensity = Number.MAX_SAFE_INTEGER;\n",
        "    for (var i = 0; i < intensity.length; i++) {\n",
        "    intensity[i] = 0.0;\n",
        "    for (var j = -half_ngram; j < ngram_length-half_ngram; j++) {\n",
        "    if (i+j < intensity.length && i+j > -1) {\n",
        "    intensity[i] += trigram_weights[k][i + j];\n",
        "    }\n",
        "    }\n",
        "    if (i == 0 || i == intensity.length-1) {\n",
        "    intensity[i] /= 2.0;\n",
        "    } else {\n",
        "    intensity[i] /= 3.0;\n",
        "    }\n",
        "    if (intensity[i] > max_intensity) {\n",
        "    max_intensity = intensity[i];\n",
        "    }\n",
        "    if (intensity[i] < min_intensity) {\n",
        "    min_intensity = intensity[i];\n",
        "    }\n",
        "    }\n",
        "    var denominator = max_intensity - min_intensity;\n",
        "    for (var i = 0; i < intensity.length; i++) {\n",
        "    intensity[i] = (intensity[i] - min_intensity) / denominator;\n",
        "    }\n",
        "    if (k%2 == 0) {\n",
        "    var heat_text = \"<p><br><b>Example:</b><br>\";\n",
        "    } else {\n",
        "    var heat_text = \"<b>Example:</b><br>\";\n",
        "    }\n",
        "    var space = \"\";\n",
        "    for (var i = 0; i < tokens.length; i++) {\n",
        "    heat_text += \"<span style='background-color:rgba(\" + color + \",\" + intensity[i] + \")'>\" + space + tokens[i] + \"</span>\";\n",
        "    if (space == \"\") {\n",
        "    space = \" \";\n",
        "    }\n",
        "    }\n",
        "    //heat_text += \"<p>\";\n",
        "    document.body.innerHTML += heat_text;\n",
        "    }\n",
        "    </script>\n",
        "    </html>\"\"\"\n",
        "    putQuote = lambda x: \"\\\"%s\\\"\"%x\n",
        "    textsString = \"var any_text = [%s];\\n\"%(\",\".join(map(putQuote, texts)))\n",
        "    weightsString = \"var trigram_weights = [%s];\\n\"%(\",\".join(map(str,weights)))\n",
        "    fOut.write(part1)\n",
        "    fOut.write(textsString)\n",
        "    fOut.write(weightsString)\n",
        "    fOut.write(part2)\n",
        "    fOut.close()\n",
        "  \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rudpdy2dq2g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new directory called visualization.\n",
        "!mkdir visualization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGra6Syipp3w",
        "colab_type": "code",
        "outputId": "4114c146-f5a2-41d1-8f6d-0403d5593da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Call the method to load the Snopes dataset and embedding matrix\n",
        "MAXLENGTH = 150\n",
        "train_loader,x_test_pad_cl,x_test_pad,x_test_pad_cls,x_test_pad_s,y_test,word_to_id,x_test_or,x_train_or = load_data_set(0,MAXLENGTH,\n",
        "                                                                                                                         model_params[\"vocab_size\"],\n",
        "                                                                                                                         model_params['batch_size']) #loading snopes dataset  \n",
        "\n",
        "embeddings = load_glove_embeddings(\"glove.6B.50d.txt\",word_to_id,50)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 159 total unique rows in the test set and 138 unique claims\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77rDYtM2qbmW",
        "colab_type": "text"
      },
      "source": [
        "## Structured Self Attention Model Full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25yAkbH-qTrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Structured Self Attention class. This class reprsents the Hybrid model. Most of this clas is taken from: https://github.com/kaushalshetty/Structured-Self-Attention/blob/master\n",
        " \n",
        "class StructuredSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
        "    and without pruning. Slight modifications have been done for speedup\n",
        "    \"\"\"\n",
        "    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type= 0,n_classes = 1):\n",
        "        \"\"\"\n",
        "        Initializes parameters suggested in paper\n",
        " \n",
        "        Args:\n",
        "            batch_size  : {int} batch_size used for training\n",
        "            lstm_hid_dim: {int} hidden dimension for lstm\n",
        "            d_a         : {int} hidden dimension for the dense layer\n",
        "            r           : {int} attention-hops or attention heads\n",
        "            max_len     : {int} number of lstm timesteps\n",
        "            emb_dim     : {int} embeddings dimension\n",
        "            vocab_size  : {int} size of the vocabulary\n",
        "            use_pretrained_embeddings: {bool} use or train your own embeddings\n",
        "            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n",
        "            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n",
        "            n_classes   : {int} number of classes\n",
        " \n",
        "        Returns:\n",
        "            self\n",
        " \n",
        "        Raises:\n",
        "            Exception\n",
        "        \"\"\"\n",
        "        super(StructuredSelfAttention,self).__init__()\n",
        "         # Model layers initialization\n",
        "        self.embeddings,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first.bias.data.fill_(0)\n",
        "        self.linear_second = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second.bias.data.fill_(0)\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        self.embeddings2,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm2 = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first2 = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first2.bias.data.fill_(0)\n",
        "        self.linear_second2 = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second2.bias.data.fill_(0)\n",
        "        \n",
        "        self.embeddings3,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.embeddings4,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        \n",
        "        self.linear_penul = torch.nn.Linear(3*lstm_hid_dim,8)\n",
        "        self.linear_final = torch.nn.Linear(8,self.n_classes)\n",
        "        self.batch_size = batch_size       \n",
        "        self.max_len = max_len\n",
        "        self.lstm_hid_dim = lstm_hid_dim\n",
        "        self.hidden_state = self.init_hidden()\n",
        "        self.r = r\n",
        "        self.type = type\n",
        "                 \n",
        "    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n",
        "        \"\"\"Load the embeddings based on flag\"\"\"\n",
        "        if use_pretrained_embeddings is True and embeddings is None:\n",
        "            raise Exception(\"Send a pretrained word embedding as an argument\")\n",
        "        \n",
        "        if not use_pretrained_embeddings and vocab_size is None:\n",
        "            raise Exception(\"Vocab size cannot be empty\")\n",
        "\n",
        "        if not use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
        "            \n",
        "        elif use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n",
        "            word_embeddings.weight = torch.nn.Parameter(embeddings)\n",
        "            emb_dim = embeddings.size(1)\n",
        "            \n",
        "        return word_embeddings,emb_dim\n",
        "       \n",
        "        \n",
        "    def softmax(self,input, axis=1):\n",
        "        \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        input_size = input.size()\n",
        "        trans_input = input.transpose(axis, len(input_size)-1)\n",
        "        trans_size = trans_input.size()\n",
        "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "        soft_max_2d = F.softmax(input_2d)\n",
        "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
        "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
        "       \n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
        "       \n",
        "        \n",
        "    def forward(self,x,y,xs,ys):\n",
        "        embeddings = self.embeddings(x) # claim embedding\n",
        "        embeddings_part_y = self.embeddings(y[:,:100]) # article embedding taking the first 100\n",
        "        main_embedding  = torch.cat((embeddings, embeddings_part_y), 1) # concatination of the two\n",
        "        # Bi-LSTM layer\n",
        "        outputs, self.hidden_state = self.lstm(main_embedding.view(self.batch_size,self.max_len,-1),self.hidden_state)\n",
        "        # Self-Attention mechanism \n",
        "        x = torch.tanh(self.linear_first(outputs))       \n",
        "        x = self.linear_second(x)       \n",
        "        x = self.softmax(x,1)       \n",
        "        attention = x.transpose(1,2)  \n",
        "        sentence_embeddings = attention@outputs\n",
        "        # Same processed followed for article only input\n",
        "        embeddings2 = self.embeddings2(y)    \n",
        "        outputs2, self.hidden_state2 = self.lstm2(embeddings2.view(self.batch_size,self.max_len,-1),self.hidden_state)       \n",
        "        x2 = torch.tanh(self.linear_first2(outputs2))       \n",
        "        x2 = self.linear_second2(x2)       \n",
        "        x2 = self.softmax(x2,1)       \n",
        "        attention2 = x2.transpose(1,2)       \n",
        "        sentence_embeddings2 = attention2@outputs2\n",
        "        # Element-wise multiplication of the two outputs\n",
        "        ele_mul = sentence_embeddings * sentence_embeddings2\n",
        "        # Average of the output\n",
        "        avg_sentence_embeddings = torch.sum(ele_mul,1)/self.r\n",
        "        # Claim and article source embedding. \n",
        "        embeddings3 = self.embeddings3(xs)\n",
        "        embeddings4 = self.embeddings4(ys) \n",
        "        # Average of these embeddings\n",
        "        avg_csource_embeddings = torch.sum(embeddings3,1)/self.r\n",
        "        avg_asource_embeddings = torch.sum(embeddings4,1)/self.r\n",
        "        # Combine with the average of the Sef-attention outputs.\n",
        "        comb_avg = torch.cat((avg_sentence_embeddings, avg_csource_embeddings, avg_asource_embeddings), 1)\n",
        "        \n",
        "        dense1 = self.linear_penul(comb_avg) \n",
        "        #output = torch.sigmoid(self.linear_final(avg_sentence_embeddings))\n",
        "        output = torch.sigmoid(self.linear_final(dense1))\n",
        "        return output,attention2\n",
        "        \n",
        "    #Regularization\n",
        "    def l2_matrix_norm(self,m):\n",
        "        \"\"\"\n",
        "        Frobenius norm calculation\n",
        " \n",
        "        Args:\n",
        "           m: {Variable} ||AAT - I||\n",
        " \n",
        "        Returns:\n",
        "            regularized value\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkoAg4Pnqib4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training function that applies the penalization. Most of this code is taken from: https://github.com/kaushalshetty/Structured-Self-Attention/blob/master\n",
        " \n",
        "def train(attention_model,train_loader,criterion,optimizer,epochs = 5,use_regularization = False,C=0,clip=False):\n",
        "    \"\"\"\n",
        "        Training code\n",
        " \n",
        "        Args:\n",
        "            attention_model : {object} model\n",
        "            train_loader    : {DataLoader} training data loaded into a dataloader\n",
        "            optimizer       :  optimizer\n",
        "            criterion       :  loss function. Must be BCELoss for binary_classification and NLLLoss for multiclass\n",
        "            epochs          : {int} number of epochs\n",
        "            use_regularizer : {bool} use penalization or not\n",
        "            C               : {int} penalization coeff\n",
        "            clip            : {bool} use gradient clipping or not\n",
        "       \n",
        "        Returns:\n",
        "            accuracy and losses of the model\n",
        "      \n",
        "        \"\"\"\n",
        "    losses = []\n",
        "    accuracy = []\n",
        "    for i in range(epochs):\n",
        "        print(\"Running EPOCH\",i+1)\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for batch_idx,train in enumerate(train_loader):\n",
        "            attention_model.hidden_state = attention_model.init_hidden()\n",
        "            x,x2,xs,x2s,y = Variable(train[0]),Variable(train[1]),Variable(train[2]),Variable(train[3]),Variable(train[4])\n",
        "            y_pred,att = attention_model(x,x2,xs,x2s)\n",
        "            \n",
        "            #penalization AAT - I\n",
        "            if use_regularization:\n",
        "                attT = att.transpose(1,2)\n",
        "                identity = torch.eye(att.size(1))\n",
        "                identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1)))\n",
        "                penal = attention_model.l2_matrix_norm(att@attT - identity)\n",
        "            \n",
        "            if not bool(attention_model.type) :\n",
        "                #binary classification\n",
        "                #Adding a very small value to prevent BCELoss from outputting NaN's\n",
        "                correct+=torch.eq(torch.round(y_pred.type(torch.DoubleTensor).squeeze(1)),y).data.sum()\n",
        "                if use_regularization:\n",
        "                    try:\n",
        "                        loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1)+1e-8,y) + C * penal/train_loader.batch_size\n",
        "                        \n",
        "                    except RuntimeError:\n",
        "                        raise Exception(\"BCELoss gets nan values on regularization. Either remove regularization or add very small values\")\n",
        "                else:\n",
        "                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y)\n",
        "\n",
        "            total_loss+=loss.data\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            #gradient clipping\n",
        "            if clip:\n",
        "                torch.nn.utils.clip_grad_norm_(attention_model.parameters(),0.5)\n",
        "            optimizer.step()\n",
        "            n_batches+=1\n",
        "        \n",
        "        print(\"avg_loss is\",total_loss/n_batches)\n",
        "        corr_nump = correct.data.cpu().numpy().astype(int)\n",
        "        acc = corr_nump/float(n_batches*train_loader.batch_size)\n",
        "        print(\"Accuracy of the model\",acc)\n",
        "        losses.append(total_loss/n_batches)\n",
        "        accuracy.append(acc)\n",
        "    return losses,accuracy\n",
        "\n",
        "# Evaluate the test result and return the accuracy and the predicted values\n",
        "def evaluate(attention_model,x_test_cl,x_test,x_test_cls,x_test_s,y_test):\n",
        "    \n",
        "    attention_model.batch_size = x_test.shape[0]\n",
        "    attention_model.hidden_state = attention_model.init_hidden()\n",
        "    \n",
        "    x_test_var = Variable(torch.from_numpy(x_test).type(torch.LongTensor)) # article var\n",
        "    x_test_var_cl = Variable(torch.from_numpy(x_test_cl).type(torch.LongTensor)) # claim var\n",
        "    x_test_var_s = Variable(torch.from_numpy(x_test_s).type(torch.LongTensor)) # article source var\n",
        "    x_test_var_cls = Variable(torch.from_numpy(x_test_cls).type(torch.LongTensor)) # claim source var\n",
        "    y_test_pred,_ = attention_model(x_test_var_cl,x_test_var,x_test_var_cls,x_test_var_s)\n",
        "\n",
        "    y_preds = torch.round(y_test_pred.type(torch.DoubleTensor).squeeze(1))\n",
        "    y_test_var = Variable(torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
        "    sum = torch.eq(y_preds,y_test_var).data.sum()\n",
        "    total = x_test_var.size(0)\n",
        "    acc = sum.data.cpu().numpy().astype(int)/float(total)\n",
        "    return acc, y_preds\n",
        "\n",
        "def get_activation_wts(attention_model,x,y,xs,ys):\n",
        "    \"\"\"\n",
        "        Get r attention heads\n",
        " \n",
        "        Args:\n",
        "            attention_model : {object} model\n",
        "            x               : {torch.Variable} input whose weights we want (claim)\n",
        "            y               : {torch.Variable} the second input (article)\n",
        "       \n",
        "        Returns:\n",
        "            r different attention weights\n",
        " \n",
        "      \n",
        "    \"\"\"\n",
        "    attention_model.batch_size = x.size(0)\n",
        "    attention_model.hidden_state = attention_model.init_hidden()\n",
        "    _,wts = attention_model(x,y,xs,ys)\n",
        "    return wts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alp7tm7wq9C-",
        "colab_type": "text"
      },
      "source": [
        "### Model Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrlmuBXVqnIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to process the article and the attention weights then create the HTML file.\n",
        "def visualize_attention(wts,x_test_pad,word_to_id,filename):\n",
        "    wts_add = torch.sum(wts,1)\n",
        "    wts_add_np = wts_add.data.numpy()\n",
        "    wts_add_list = wts_add_np.tolist()\n",
        "    id_to_word = {v:k for k,v in word_to_id.items()}\n",
        "    text= []\n",
        "    for test in x_test_pad:\n",
        "        line=list()\n",
        "        for i in test:\n",
        "          if i == 0.0:\n",
        "            word = '<UNK>'\n",
        "          else:\n",
        "            word = id_to_word.get(i)\n",
        "          line.append(word)\n",
        "        text.append(\" \".join(line))\n",
        "\n",
        "    createHTML(text, wts_add_list, filename)\n",
        "    print(\"Attention visualization created for {} samples\".format(len(x_test_pad)))\n",
        "    return\n",
        "\n",
        "# Method to perform the classification using Binary Cross Entropy Loss and RMSProp optimizer\n",
        "def binary_classfication(attention_model,train_loader,epochs=5,use_regularization=True,C=1.0,clip=True):\n",
        "    loss = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.RMSprop(attention_model.parameters())\n",
        "    loss,accuracy=train(attention_model,train_loader,loss,optimizer,epochs,use_regularization,C,clip)\n",
        "    return loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fxxDaonrBcW",
        "colab_type": "code",
        "outputId": "b5c74c03-7532-4b58-9092-b900d410a56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Model execution\n",
        "attention_model = StructuredSelfAttention(batch_size=train_loader.batch_size,lstm_hid_dim=model_params['lstm_hidden_dimension'],\n",
        "                                          d_a = model_params[\"d_a\"],r=params_set[\"attention_hops\"],vocab_size=len(word_to_id),\n",
        "                                          max_len=MAXLENGTH,type=0,n_classes=1,use_pretrained_embeddings=True,embeddings=embeddings)\n",
        "\n",
        "loss, acc = binary_classfication(attention_model,train_loader=train_loader,epochs=3,use_regularization=False,\n",
        "                                 C=params_set[\"C\"],clip=params_set[\"clip\"])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:95: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss is tensor(0.4913, dtype=torch.float64)\n",
            "Accuracy of the model 0.7500424592391305\n",
            "Running EPOCH 2\n",
            "avg_loss is tensor(0.0968, dtype=torch.float64)\n",
            "Accuracy of the model 0.9661175271739131\n",
            "Running EPOCH 3\n",
            "avg_loss is tensor(0.0247, dtype=torch.float64)\n",
            "Accuracy of the model 0.9933763586956522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXfZWZbArhhD",
        "colab_type": "code",
        "outputId": "06b25724-98a6-4b80-d1d8-bcad5b5788cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Accuracy of the test data\n",
        "acc, pred = evaluate(attention_model, x_test_pad_cl, x_test_pad, x_test_pad_cls, x_test_pad_s, y_test)\n",
        "print(acc)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:95: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9742895805142084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6voYfvYrqNT",
        "colab_type": "text"
      },
      "source": [
        "### Metric evaluation methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-GVkUJBrlqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to plot the confusion matrix. Code taken from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Greens):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=['False','True'], yticklabels=['False','True'],\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lpAO6GUru_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code taken from Lab5 of TaD by Jeff Dalton. This method returns the accuarcy, precision, recall and F1 measure values for an input.\n",
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YV73roer62a",
        "colab_type": "code",
        "outputId": "85e62edb-1120-47f2-f153-974bd08d9c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "plot_confusion_matrix(y_test,pred.detach().numpy(),[0,1])\n",
        "plot_confusion_matrix(y_test,pred.detach().numpy(),[0,1], normalize=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2874   44]\n",
            " [ 108 2886]]\n",
            "Normalized confusion matrix\n",
            "[[0.98492118 0.01507882]\n",
            " [0.03607214 0.96392786]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8981f2b0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEYCAYAAADFzZobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nz8xyx4nPzHLHic/Mcuf/A0YEsqoaOSeeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyMVdH2Jr72e",
        "colab_type": "code",
        "outputId": "9a0d1881-e928-4797-f577-52028dda9e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "full_p,full_r,full_a,full_f1 = evaluation_summary(\"Self Attention-Full\", pred.detach().numpy(), y_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: Self Attention-Full\n",
            "Classifier 'Self Attention-Full' has Acc=0.974 P=0.974 R=0.974 F1=0.974\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.985     0.964     0.974      2982\n",
            "         1.0      0.964     0.985     0.974      2930\n",
            "\n",
            "    accuracy                          0.974      5912\n",
            "   macro avg      0.974     0.974     0.974      5912\n",
            "weighted avg      0.975     0.974     0.974      5912\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[2874   44]\n",
            " [ 108 2886]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlAxxkt1sFAd",
        "colab_type": "code",
        "outputId": "b41e40ba-230c-42c4-b545-55ef450da74a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calculate the AUC value of the result.\n",
        "fpr, tpr, thresholds = roc_curve(y_test, pred.detach().numpy())\n",
        "auc_full = auc(fpr, tpr)\n",
        "print(\"Self Attention-Full AUC-\",auc_full)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Self Attention-Full AUC- 0.9744245173005367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swn0rIovsKhJ",
        "colab_type": "code",
        "outputId": "a541bb70-2f06-4096-ba23-ecca364d2e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Visualize the attention weights for this model.\n",
        "test_last_idx = 15\n",
        "wts = get_activation_wts(attention_model,Variable(torch.from_numpy(x_test_pad_cl[:test_last_idx]).type(torch.LongTensor)),\n",
        "                            Variable(torch.from_numpy(x_test_pad[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_cls[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_s[:test_last_idx]).type(torch.LongTensor)))\n",
        "\n",
        "visualize_attention(wts,x_test_pad[:test_last_idx],word_to_id,filename='attention_pol_full.html')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention visualization created for 15 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:95: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pL6jocvsYIO",
        "colab_type": "text"
      },
      "source": [
        "##Sub-Model- Self Attention model using only Article and Claim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWVKMKn2saV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Same model but it uses only article and claim as input.\n",
        "class StructuredSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
        "    and without pruning. Slight modifications have been done for speedup\n",
        "    \"\"\"\n",
        "    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type= 0,n_classes = 1):\n",
        "        \"\"\"\n",
        "        Initializes parameters suggested in paper\n",
        " \n",
        "        Args:\n",
        "            batch_size  : {int} batch_size used for training\n",
        "            lstm_hid_dim: {int} hidden dimension for lstm\n",
        "            d_a         : {int} hidden dimension for the dense layer\n",
        "            r           : {int} attention-hops or attention heads\n",
        "            max_len     : {int} number of lstm timesteps\n",
        "            emb_dim     : {int} embeddings dimension\n",
        "            vocab_size  : {int} size of the vocabulary\n",
        "            use_pretrained_embeddings: {bool} use or train your own embeddings\n",
        "            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n",
        "            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n",
        "            n_classes   : {int} number of classes\n",
        " \n",
        "        Returns:\n",
        "            self\n",
        " \n",
        "        Raises:\n",
        "            Exception\n",
        "        \"\"\"\n",
        "        super(StructuredSelfAttention,self).__init__()\n",
        "        # Initialization of the required layers\n",
        "        self.embeddings,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first.bias.data.fill_(0)\n",
        "        self.linear_second = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second.bias.data.fill_(0)\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        self.embeddings2,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm2 = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first2 = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first2.bias.data.fill_(0)\n",
        "        self.linear_second2 = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second2.bias.data.fill_(0)\n",
        "\n",
        "        self.linear_final = torch.nn.Linear(lstm_hid_dim,self.n_classes)\n",
        "        self.batch_size = batch_size       \n",
        "        self.max_len = max_len\n",
        "        self.lstm_hid_dim = lstm_hid_dim\n",
        "        self.hidden_state = self.init_hidden()\n",
        "        self.r = r\n",
        "        self.type = type\n",
        "                 \n",
        "    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n",
        "        \"\"\"Load the embeddings based on flag\"\"\"\n",
        "        if use_pretrained_embeddings is True and embeddings is None:\n",
        "            raise Exception(\"Send a pretrained word embedding as an argument\")\n",
        "          \n",
        "        if not use_pretrained_embeddings and vocab_size is None:\n",
        "            raise Exception(\"Vocab size cannot be empty\")\n",
        "        \n",
        "        if not use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
        "            \n",
        "        elif use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n",
        "            word_embeddings.weight = torch.nn.Parameter(embeddings)\n",
        "            emb_dim = embeddings.size(1)\n",
        "            \n",
        "        return word_embeddings,emb_dim\n",
        "       \n",
        "        \n",
        "    def softmax(self,input, axis=1):\n",
        "        \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        input_size = input.size()\n",
        "        trans_input = input.transpose(axis, len(input_size)-1)\n",
        "        trans_size = trans_input.size()\n",
        "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "        soft_max_2d = F.softmax(input_2d)\n",
        "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
        "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
        "       \n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
        "       \n",
        "        \n",
        "    def forward(self,x,y,xs,ys):\n",
        "        embeddings = self.embeddings(x) # claim embedding\n",
        "        embeddings_part_y = self.embeddings(y[:,:100]) # Article embedding taking the first 100 values\n",
        "        main_embedding  = torch.cat((embeddings, embeddings_part_y), 1) #Combination of the two\n",
        "        # Bi-LSTM layer\n",
        "        outputs, self.hidden_state = self.lstm(main_embedding.view(self.batch_size,self.max_len,-1),self.hidden_state)  \n",
        "        # Self-Attention mechanism    \n",
        "        x = torch.tanh(self.linear_first(outputs))       \n",
        "        x = self.linear_second(x)       \n",
        "        x = self.softmax(x,1)       \n",
        "        attention = x.transpose(1,2)  \n",
        "        sentence_embeddings = attention@outputs\n",
        "\n",
        "        # Same processing followed for article only.\n",
        "        embeddings2 = self.embeddings2(y)    \n",
        "        outputs2, self.hidden_state2 = self.lstm2(embeddings2.view(self.batch_size,self.max_len,-1),self.hidden_state)       \n",
        "        x2 = torch.tanh(self.linear_first2(outputs2))       \n",
        "        x2 = self.linear_second2(x2)       \n",
        "        x2 = self.softmax(x2,1)       \n",
        "        attention2 = x2.transpose(1,2)       \n",
        "        sentence_embeddings2 = attention2@outputs2\n",
        "\n",
        "        # Element-wise multiplication of the two outputs.\n",
        "        ele_mul = sentence_embeddings * sentence_embeddings2\n",
        "        # Average of the output.\n",
        "        avg_sentence_embeddings = torch.sum(ele_mul,1)/self.r\n",
        "        \n",
        "        output = torch.sigmoid(self.linear_final(avg_sentence_embeddings))\n",
        "        return output,attention2\n",
        "        \n",
        "    #Regularization\n",
        "    def l2_matrix_norm(self,m):\n",
        "        \"\"\"\n",
        "        Frobenius norm calculation\n",
        " \n",
        "        Args:\n",
        "           m: {Variable} ||AAT - I||\n",
        " \n",
        "        Returns:\n",
        "            regularized value\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np8FA1ZEt39P",
        "colab_type": "code",
        "outputId": "a4e667f5-e9dc-4bb2-c1e6-67c1b399bb3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Initialize and Train the model.\n",
        "attention_model2 = StructuredSelfAttention(batch_size=train_loader.batch_size,lstm_hid_dim=model_params['lstm_hidden_dimension'],d_a = model_params[\"d_a\"],r=params_set[\"attention_hops\"],vocab_size=len(word_to_id),max_len=MAXLENGTH,type=0,n_classes=1,use_pretrained_embeddings=True,embeddings=embeddings)\n",
        "\n",
        "loss2, acc2 = binary_classfication(attention_model2,train_loader=train_loader,epochs=3,use_regularization=True,C=params_set[\"C\"],clip=params_set[\"clip\"])\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss is tensor(0.6043, dtype=torch.float64)\n",
            "Accuracy of the model 0.7349269701086957\n",
            "Running EPOCH 2\n",
            "avg_loss is tensor(0.2234, dtype=torch.float64)\n",
            "Accuracy of the model 0.9545261548913043\n",
            "Running EPOCH 3\n",
            "avg_loss is tensor(0.1433, dtype=torch.float64)\n",
            "Accuracy of the model 0.9833559782608695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq8B_gWGuArH",
        "colab_type": "code",
        "outputId": "347685ce-a327-4ca2-c001-7203c5648c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Accuracy of the test data\n",
        "acc2, pred2 = evaluate(attention_model2, x_test_pad_cl, x_test_pad, x_test_pad_cls, x_test_pad_s, y_test)\n",
        "print(acc2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9575439783491204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4-XA2w3uD8t",
        "colab_type": "code",
        "outputId": "2e850575-fbdf-474c-d6c1-357bb4b0c9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "plot_confusion_matrix(y_test,pred2.detach().numpy(),[0,1])\n",
        "plot_confusion_matrix(y_test,pred2.detach().numpy(),[0,1], normalize=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2794  124]\n",
            " [ 127 2867]]\n",
            "Normalized confusion matrix\n",
            "[[0.95750514 0.04249486]\n",
            " [0.04241817 0.95758183]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8982efe978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFeEAaWZWhAOkmVkRDpBmZkU4QJqZFfH/pLOQWnPS8oAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEYCAYAAADFzZobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npawAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuZZx5oEuHuj",
        "colab_type": "code",
        "outputId": "6d3f2012-5af9-4f52-b149-87b742fcdb7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "art_cl_p,art_cl_r,art_cl_a,art_cl_f1 = evaluation_summary(\"Self Attention-Article and Claim\", pred2.detach().numpy(), y_test)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: Self Attention-Article and Claim\n",
            "Classifier 'Self Attention-Article and Claim' has Acc=0.958 P=0.958 R=0.958 F1=0.958\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.958     0.957     0.957      2921\n",
            "         1.0      0.958     0.959     0.958      2991\n",
            "\n",
            "    accuracy                          0.958      5912\n",
            "   macro avg      0.958     0.958     0.958      5912\n",
            "weighted avg      0.958     0.958     0.958      5912\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[2794  124]\n",
            " [ 127 2867]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAju_9NLuLYt",
        "colab_type": "code",
        "outputId": "c4f04e81-fa4f-4b61-b3ab-0ff9ab8839f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, pred2.detach().numpy())\n",
        "auc_art_cl = auc(fpr, tpr)\n",
        "print(\"Self Attention-Article_claim AUC-\",auc_art_cl)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Self Attention-Article_claim AUC- 0.957543485417259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojxxOaxEuOde",
        "colab_type": "code",
        "outputId": "95d4a1c9-e9c3-4eff-d987-edc49ff40e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Visulaisation\n",
        "test_last_idx = 15\n",
        "wts2 = get_activation_wts(attention_model2,Variable(torch.from_numpy(x_test_pad_cl[:test_last_idx]).type(torch.LongTensor)),\n",
        "                             Variable(torch.from_numpy(x_test_pad[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_cls[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_s[:test_last_idx]).type(torch.LongTensor)))\n",
        "print(wts2.size())\n",
        "visualize_attention(wts2,x_test_pad[:test_last_idx],word_to_id,filename='attention_art_cl.html')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([15, 10, 150])\n",
            "Attention visualization created for 15 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjZHBmdOvA8D",
        "colab_type": "text"
      },
      "source": [
        "## Sub Model- Using only Article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNhLeb5uvCDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Self attention class this time only using the Article.\n",
        "class StructuredSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
        "    and without pruning. Slight modifications have been done for speedup\n",
        "    \"\"\"\n",
        "    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type= 0,n_classes = 1):\n",
        "        \"\"\"\n",
        "        Initializes parameters suggested in paper\n",
        " \n",
        "        Args:\n",
        "            batch_size  : {int} batch_size used for training\n",
        "            lstm_hid_dim: {int} hidden dimension for lstm\n",
        "            d_a         : {int} hidden dimension for the dense layer\n",
        "            r           : {int} attention-hops or attention heads\n",
        "            max_len     : {int} number of lstm timesteps\n",
        "            emb_dim     : {int} embeddings dimension\n",
        "            vocab_size  : {int} size of the vocabulary\n",
        "            use_pretrained_embeddings: {bool} use or train your own embeddings\n",
        "            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n",
        "            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n",
        "            n_classes   : {int} number of classes\n",
        " \n",
        "        Returns:\n",
        "            self\n",
        " \n",
        "        Raises:\n",
        "            Exception\n",
        "        \"\"\"\n",
        "        super(StructuredSelfAttention,self).__init__()        \n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # Self Attention layers for Article\n",
        "        self.embeddings2,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm2 = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first2 = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first2.bias.data.fill_(0)\n",
        "        self.linear_second2 = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second2.bias.data.fill_(0)\n",
        "\n",
        "        self.linear_final = torch.nn.Linear(lstm_hid_dim,self.n_classes)\n",
        "        self.batch_size = batch_size       \n",
        "        self.max_len = max_len\n",
        "        self.lstm_hid_dim = lstm_hid_dim\n",
        "        self.hidden_state = self.init_hidden()\n",
        "        self.r = r\n",
        "        self.type = type\n",
        "                 \n",
        "    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n",
        "        \"\"\"Load the embeddings based on flag\"\"\"\n",
        "        if use_pretrained_embeddings is True and embeddings is None:\n",
        "            raise Exception(\"Send a pretrained word embedding as an argument\")\n",
        "          \n",
        "        if not use_pretrained_embeddings and vocab_size is None:\n",
        "            raise Exception(\"Vocab size cannot be empty\")\n",
        "        \n",
        "        if not use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
        "            \n",
        "        elif use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n",
        "            word_embeddings.weight = torch.nn.Parameter(embeddings)\n",
        "            emb_dim = embeddings.size(1)\n",
        "            \n",
        "        return word_embeddings,emb_dim\n",
        "       \n",
        "        \n",
        "    def softmax(self,input, axis=1):\n",
        "        \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        input_size = input.size()\n",
        "        trans_input = input.transpose(axis, len(input_size)-1)\n",
        "        trans_size = trans_input.size()\n",
        "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "        soft_max_2d = F.softmax(input_2d)\n",
        "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
        "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
        "       \n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
        "       \n",
        "        \n",
        "    def forward(self,x,y,xs,ys):\n",
        "        # Article embedding\n",
        "        embeddings2 = self.embeddings2(y)    \n",
        "        # Bi-LSTM layer\n",
        "        outputs2, self.hidden_state2 = self.lstm2(embeddings2.view(self.batch_size,self.max_len,-1),self.hidden_state)  \n",
        "         # Self-Attention mechanism     \n",
        "        x2 = torch.tanh(self.linear_first2(outputs2))       \n",
        "        x2 = self.linear_second2(x2)       \n",
        "        x2 = self.softmax(x2,1)       \n",
        "        attention2 = x2.transpose(1,2)       \n",
        "        sentence_embeddings2 = attention2@outputs2\n",
        "        # Average of the output\n",
        "        avg_sentence_embeddings = torch.sum(sentence_embeddings2,1)/self.r\n",
        "        \n",
        "        output = torch.sigmoid(self.linear_final(avg_sentence_embeddings))\n",
        "\n",
        "        return output,attention2\n",
        "        \n",
        "    #Regularization\n",
        "    def l2_matrix_norm(self,m):\n",
        "        \"\"\"\n",
        "        Frobenius norm calculation\n",
        " \n",
        "        Args:\n",
        "           m: {Variable} ||AAT - I||\n",
        " \n",
        "        Returns:\n",
        "            regularized value\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA0S_CNpv-HI",
        "colab_type": "code",
        "outputId": "57ceff34-b06b-4409-c187-d7e02b650291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Initialize and train the model\n",
        "attention_model3 = StructuredSelfAttention(batch_size=train_loader.batch_size,lstm_hid_dim=model_params['lstm_hidden_dimension'],d_a = model_params[\"d_a\"],r=params_set[\"attention_hops\"],vocab_size=len(word_to_id),max_len=MAXLENGTH,type=0,n_classes=1,use_pretrained_embeddings=True,embeddings=embeddings)\n",
        "\n",
        "loss, acc = binary_classfication(attention_model3,train_loader=train_loader,epochs=3,use_regularization=True,C=params_set[\"C\"],clip=params_set[\"clip\"])\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss is tensor(0.5402, dtype=torch.float64)\n",
            "Accuracy of the model 0.7817595108695652\n",
            "Running EPOCH 2\n",
            "avg_loss is tensor(0.2789, dtype=torch.float64)\n",
            "Accuracy of the model 0.9287109375\n",
            "Running EPOCH 3\n",
            "avg_loss is tensor(0.2106, dtype=torch.float64)\n",
            "Accuracy of the model 0.957625679347826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLhlDjvNwEgy",
        "colab_type": "code",
        "outputId": "ac9d1780-65ad-47c5-bde1-c534c21a7be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Accuracy of the test data\n",
        "acc3, pred3 = evaluate(attention_model3, x_test_pad_cl, x_test_pad, x_test_pad_cls, x_test_pad_s, y_test)\n",
        "print(acc3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.841339648173207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLZqN5WNwG_0",
        "colab_type": "code",
        "outputId": "e314f2f1-03d1-420c-f8fc-7c8490085793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "plot_confusion_matrix(y_test,pred3.detach().numpy(),[0,1])\n",
        "plot_confusion_matrix(y_test,pred3.detach().numpy(),[0,1], normalize=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2353  565]\n",
            " [ 373 2621]]\n",
            "Normalized confusion matrix\n",
            "[[0.80637423 0.19362577]\n",
            " [0.1245825  0.8754175 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f89826552b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnMvCA6RzzmXhAdI557LwAOmcc1n8P4UOVeD0IptUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEYCAYAAADFzZobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ2ZV5/8DkVSS65XLfBoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzr-unOBwKcY",
        "colab_type": "code",
        "outputId": "1719724e-7774-40cd-8e28-35ba51c13fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "art_p,art_r,art_a,art_f1 = evaluation_summary(\"Self Attention-Article only\", pred3.detach().numpy(), y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: Self Attention-Article only\n",
            "Classifier 'Self Attention-Article only' has Acc=0.841 P=0.841 R=0.843 F1=0.841\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.806     0.863     0.834      2726\n",
            "         1.0      0.875     0.823     0.848      3186\n",
            "\n",
            "    accuracy                          0.841      5912\n",
            "   macro avg      0.841     0.843     0.841      5912\n",
            "weighted avg      0.844     0.841     0.842      5912\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[2353  565]\n",
            " [ 373 2621]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjQNgYzbwTzO",
        "colab_type": "code",
        "outputId": "e3b5c8dc-9f70-446e-9979-9a7399c3f47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, pred3.detach().numpy())\n",
        "auc_art = auc(fpr, tpr)\n",
        "print(\"Self Attention-Article only AUC-\",auc_art)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Self Attention-Article only AUC- 0.8408958652969636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtDuSrAmwXKg",
        "colab_type": "code",
        "outputId": "3bcfcdc2-3d5c-4919-b79f-ee51fef073af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Visulaisation\n",
        "test_last_idx = 15\n",
        "wts3 = get_activation_wts(attention_model3,Variable(torch.from_numpy(x_test_pad_cl[:test_last_idx]).type(torch.LongTensor)),\n",
        "                             Variable(torch.from_numpy(x_test_pad[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_cls[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_s[:test_last_idx]).type(torch.LongTensor)))\n",
        "print(wts3.size())\n",
        "visualize_attention(wts3,x_test_pad[:test_last_idx],word_to_id,filename='attention_art.html')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([15, 10, 150])\n",
            "Attention visualization created for 15 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1IT7vJowfs5",
        "colab_type": "text"
      },
      "source": [
        "## Sub Model- Article and sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g13BZORwhu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Self attention class uing only the article and article source\n",
        "class StructuredSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
        "    and without pruning. Slight modifications have been done for speedup\n",
        "    \"\"\"\n",
        "    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type= 0,n_classes = 1):\n",
        "        \"\"\"\n",
        "        Initializes parameters suggested in paper\n",
        " \n",
        "        Args:\n",
        "            batch_size  : {int} batch_size used for training\n",
        "            lstm_hid_dim: {int} hidden dimension for lstm\n",
        "            d_a         : {int} hidden dimension for the dense layer\n",
        "            r           : {int} attention-hops or attention heads\n",
        "            max_len     : {int} number of lstm timesteps\n",
        "            emb_dim     : {int} embeddings dimension\n",
        "            vocab_size  : {int} size of the vocabulary\n",
        "            use_pretrained_embeddings: {bool} use or train your own embeddings\n",
        "            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n",
        "            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n",
        "            n_classes   : {int} number of classes\n",
        " \n",
        "        Returns:\n",
        "            self\n",
        " \n",
        "        Raises:\n",
        "            Exception\n",
        "        \"\"\"\n",
        "        super(StructuredSelfAttention,self).__init__()       \n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # Self Attention layers for Article\n",
        "        self.embeddings2,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm2 = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first2 = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first2.bias.data.fill_(0)\n",
        "        self.linear_second2 = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second2.bias.data.fill_(0)\n",
        "        \n",
        "         # Embedding layers for sources\n",
        "        self.embeddings3,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.embeddings4,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        \n",
        "        self.linear_final = torch.nn.Linear(3*lstm_hid_dim,self.n_classes)\n",
        "        self.batch_size = batch_size       \n",
        "        self.max_len = max_len\n",
        "        self.lstm_hid_dim = lstm_hid_dim\n",
        "        self.hidden_state = self.init_hidden()\n",
        "        self.r = r\n",
        "        self.type = type\n",
        "                 \n",
        "    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n",
        "        \"\"\"Load the embeddings based on flag\"\"\"\n",
        "        if use_pretrained_embeddings is True and embeddings is None:\n",
        "            raise Exception(\"Send a pretrained word embedding as an argument\")\n",
        "          \n",
        "        if not use_pretrained_embeddings and vocab_size is None:\n",
        "            raise Exception(\"Vocab size cannot be empty\")\n",
        "        \n",
        "        if not use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
        "            \n",
        "        elif use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n",
        "            word_embeddings.weight = torch.nn.Parameter(embeddings)\n",
        "            emb_dim = embeddings.size(1)\n",
        "            \n",
        "        return word_embeddings,emb_dim\n",
        "       \n",
        "        \n",
        "    def softmax(self,input, axis=1):\n",
        "        \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        input_size = input.size()\n",
        "        trans_input = input.transpose(axis, len(input_size)-1)\n",
        "        trans_size = trans_input.size()\n",
        "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "        soft_max_2d = F.softmax(input_2d)\n",
        "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
        "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
        "       \n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
        "       \n",
        "        \n",
        "    def forward(self,x,y,xs,ys):\n",
        "        # Article embedding\n",
        "        embeddings2 = self.embeddings2(y)    \n",
        "        # Bi-LSTM layer\n",
        "        outputs2, self.hidden_state2 = self.lstm2(embeddings2.view(self.batch_size,self.max_len,-1),self.hidden_state)  \n",
        "        # Self-Attention mechanism     \n",
        "        x2 = torch.tanh(self.linear_first2(outputs2))       \n",
        "        x2 = self.linear_second2(x2)       \n",
        "        x2 = self.softmax(x2,1)       \n",
        "        attention2 = x2.transpose(1,2)       \n",
        "        sentence_embeddings2 = attention2@outputs2\n",
        "        # Average of the output.\n",
        "        avg_sentence_embeddings = torch.sum(sentence_embeddings2,1)/self.r\n",
        "        \n",
        "        # Claim and article source embedding\n",
        "        embeddings3 = self.embeddings3(xs)\n",
        "        embeddings4 = self.embeddings4(ys) \n",
        "        # Average of the embeddings\n",
        "        avg_csource_embeddings = torch.sum(embeddings3,1)/self.r\n",
        "        avg_asource_embeddings = torch.sum(embeddings4,1)/self.r\n",
        "        # Combined average of the sources and self-attention outputs.\n",
        "        comb_avg = torch.cat((avg_sentence_embeddings, avg_csource_embeddings, avg_asource_embeddings), 1)\n",
        " \n",
        "        output = torch.sigmoid(self.linear_final(comb_avg))\n",
        "        \n",
        "        return output,attention2\n",
        "        \n",
        "    #Regularization\n",
        "    def l2_matrix_norm(self,m):\n",
        "        \"\"\"\n",
        "        Frobenius norm calculation\n",
        " \n",
        "        Args:\n",
        "           m: {Variable} ||AAT - I||\n",
        " \n",
        "        Returns:\n",
        "            regularized value\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN4SZAXQwkgZ",
        "colab_type": "code",
        "outputId": "2f27746c-f960-4ed2-ce27-74b9d9da439d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Initialize and Train the model\n",
        "attention_model4 = StructuredSelfAttention(batch_size=train_loader.batch_size,lstm_hid_dim=model_params['lstm_hidden_dimension'],d_a = model_params[\"d_a\"],r=params_set[\"attention_hops\"],vocab_size=len(word_to_id),max_len=MAXLENGTH,type=0,n_classes=1,use_pretrained_embeddings=True,embeddings=embeddings)\n",
        "\n",
        "loss, acc = binary_classfication(attention_model4,train_loader=train_loader,epochs=2,use_regularization=True,C=params_set[\"C\"],clip=params_set[\"clip\"])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss is tensor(0.4629, dtype=torch.float64)\n",
            "Accuracy of the model 0.8291440217391305\n",
            "Running EPOCH 2\n",
            "avg_loss is tensor(0.2099, dtype=torch.float64)\n",
            "Accuracy of the model 0.9576681385869565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Y46_JBwohh",
        "colab_type": "code",
        "outputId": "245acb53-ac7f-4c7b-d4fd-814db8fedad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Accuracy of the test data\n",
        "acc4, pred4 = evaluate(attention_model4, x_test_pad_cl, x_test_pad, x_test_pad_cls, x_test_pad_s, y_test)\n",
        "print(acc4)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8643437077131259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBOq-IxzwrxF",
        "colab_type": "code",
        "outputId": "8388a9b6-d6ae-470d-c67e-f17d6068a76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "plot_confusion_matrix(y_test,pred4.detach().numpy(),[0,1])\n",
        "plot_confusion_matrix(y_test,pred4.detach().numpy(),[0,1], normalize=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2538  380]\n",
            " [ 422 2572]]\n",
            "Normalized confusion matrix\n",
            "[[0.86977382 0.13022618]\n",
            " [0.14094856 0.85905144]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8982f6c438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ADpnHNZeIB0zrksPEA651wW/w8DbngzfE052AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEYCAYAAADFzZobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLZDZ9trwzV1",
        "colab_type": "code",
        "outputId": "dd3721ea-8557-4252-a206-395398a83fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "arts_p,arts_r,arts_a,arts_f1 = evaluation_summary(\"Self Attention-Article and sources\", pred4.detach().numpy(), y_test)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: Self Attention-Article and sources\n",
            "Classifier 'Self Attention-Article and sources' has Acc=0.864 P=0.864 R=0.864 F1=0.864\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.870     0.857     0.864      2960\n",
            "         1.0      0.859     0.871     0.865      2952\n",
            "\n",
            "    accuracy                          0.864      5912\n",
            "   macro avg      0.864     0.864     0.864      5912\n",
            "weighted avg      0.864     0.864     0.864      5912\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[2538  380]\n",
            " [ 422 2572]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxHEvviXw29j",
        "colab_type": "code",
        "outputId": "69aa6e28-d602-4fed-f207-b91e26ed7d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, pred4.detach().numpy())\n",
        "auc_arts = auc(fpr, tpr)\n",
        "print(\"Self Attention-Article_souces AUC-\",auc_arts)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Self Attention-Article_souces AUC- 0.8644126269445448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDKyl96ow6ba",
        "colab_type": "code",
        "outputId": "f23d7f16-ac70-46e2-96b3-8e55fcb483c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Visulaisation\n",
        "test_last_idx = 15\n",
        "wts4 = get_activation_wts(attention_model4,Variable(torch.from_numpy(x_test_pad_cl[:test_last_idx]).type(torch.LongTensor)),\n",
        "                             Variable(torch.from_numpy(x_test_pad[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_cls[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_s[:test_last_idx]).type(torch.LongTensor)))\n",
        "print(wts4.size())\n",
        "visualize_attention(wts4,x_test_pad[:test_last_idx],word_to_id,filename='attention_art_sources.html')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([15, 10, 150])\n",
            "Attention visualization created for 15 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ADn0xMAUeo",
        "colab_type": "text"
      },
      "source": [
        "## Full Model using Batchwise dot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA-faCuQAZIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The full model buing rather than just using element-wise multiplication to combine the two self-attentions outputs, we use batch-wise dot product.\n",
        "# The batch-wise dot product is a technique mentioned by lin et al.(2017) in their paper.\n",
        "class StructuredSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
        "    and without pruning. Slight modifications have been done for speedup\n",
        "    \"\"\"\n",
        "    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type= 0,n_classes = 1):\n",
        "        \"\"\"\n",
        "        Initializes parameters suggested in paper\n",
        " \n",
        "        Args:\n",
        "            batch_size  : {int} batch_size used for training\n",
        "            lstm_hid_dim: {int} hidden dimension for lstm\n",
        "            d_a         : {int} hidden dimension for the dense layer\n",
        "            r           : {int} attention-hops or attention heads\n",
        "            max_len     : {int} number of lstm timesteps\n",
        "            emb_dim     : {int} embeddings dimension\n",
        "            vocab_size  : {int} size of the vocabulary\n",
        "            use_pretrained_embeddings: {bool} use or train your own embeddings\n",
        "            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n",
        "            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n",
        "            n_classes   : {int} number of classes\n",
        " \n",
        "        Returns:\n",
        "            self\n",
        " \n",
        "        Raises:\n",
        "            Exception\n",
        "        \"\"\"\n",
        "        super(StructuredSelfAttention,self).__init__()\n",
        "        # Model layers initialization.\n",
        "        # Claim related layers\n",
        "        self.embeddings,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first.bias.data.fill_(0)\n",
        "        self.linear_second = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second.bias.data.fill_(0)\n",
        "        self.n_classes = n_classes\n",
        "        # Article related layers\n",
        "        self.embeddings2,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.lstm2 = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
        "        self.linear_first2 = torch.nn.Linear(lstm_hid_dim,d_a)\n",
        "        self.linear_first2.bias.data.fill_(0)\n",
        "        self.linear_second2 = torch.nn.Linear(d_a,r)\n",
        "        self.linear_second2.bias.data.fill_(0)\n",
        "        # ources related layers\n",
        "        self.embeddings3,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        self.embeddings4,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
        "        \n",
        "        self.linear_penul = torch.nn.Linear(3*lstm_hid_dim,8)\n",
        "        self.linear_final = torch.nn.Linear(8,self.n_classes)\n",
        "        self.batch_size = batch_size       \n",
        "        self.max_len = max_len\n",
        "        self.lstm_hid_dim = lstm_hid_dim\n",
        "        self.hidden_state = self.init_hidden()\n",
        "        self.r = r\n",
        "        self.type = type\n",
        "                 \n",
        "    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n",
        "        \"\"\"Load the embeddings based on flag\"\"\"\n",
        "        if use_pretrained_embeddings is True and embeddings is None:\n",
        "            raise Exception(\"Send a pretrained word embedding as an argument\")\n",
        "        \n",
        "        if not use_pretrained_embeddings and vocab_size is None:\n",
        "            raise Exception(\"Vocab size cannot be empty\")\n",
        "\n",
        "        if not use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
        "            \n",
        "        elif use_pretrained_embeddings:\n",
        "            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n",
        "            word_embeddings.weight = torch.nn.Parameter(embeddings)\n",
        "            emb_dim = embeddings.size(1)\n",
        "            \n",
        "        return word_embeddings,emb_dim\n",
        "       \n",
        "        \n",
        "    def softmax(self,input, axis=1):\n",
        "        \"\"\"\n",
        "        Softmax applied to axis=n\n",
        " \n",
        "        Args:\n",
        "           input: {Tensor,Variable} input on which softmax is to be applied\n",
        "           axis : {int} axis on which softmax is to be applied\n",
        " \n",
        "        Returns:\n",
        "            softmaxed tensors\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        input_size = input.size()\n",
        "        trans_input = input.transpose(axis, len(input_size)-1)\n",
        "        trans_size = trans_input.size()\n",
        "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "        soft_max_2d = F.softmax(input_2d)\n",
        "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
        "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
        "       \n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
        "       \n",
        "        \n",
        "    def forward(self,x,y,xs,ys):\n",
        "        embeddings = self.embeddings(x) # claim embedding\n",
        "        embeddings_part_y = self.embeddings(y[:,:100]) # article embedding uing first 100 values\n",
        "        main_embedding  = torch.cat((embeddings, embeddings_part_y), 1) # combination of the two\n",
        "        # Bi-LSTM layer\n",
        "        outputs, self.hidden_state = self.lstm(main_embedding.view(self.batch_size,self.max_len,-1),self.hidden_state)  \n",
        "        # Self-attention mechanism     \n",
        "        x = torch.tanh(self.linear_first(outputs))       \n",
        "        x = self.linear_second(x)       \n",
        "        x = self.softmax(x,1)       \n",
        "        attention = x.transpose(1,2)  \n",
        "        sentence_embeddings = attention@outputs\n",
        "        # Creation of a random weight matrix\n",
        "        rand_wt = torch.from_numpy(np.random.rand(sentence_embeddings.shape[0], sentence_embeddings.shape[1],sentence_embeddings.shape[1])) \n",
        "        print(rand_wt.shape)\n",
        "        # Multiply the output of the the weight matrix and self-attention output using batch-wise multiplication \n",
        "        factor_rel1 = torch.bmm(sentence_embeddings.transpose(2,1), rand_wt.float()).transpose(2,1)\n",
        "\n",
        "        # Same process is followed for article only input.\n",
        "        embeddings2 = self.embeddings2(y)    \n",
        "        outputs2, self.hidden_state2 = self.lstm2(embeddings2.view(self.batch_size,self.max_len,-1),self.hidden_state)       \n",
        "        x2 = torch.tanh(self.linear_first2(outputs2))       \n",
        "        x2 = self.linear_second2(x2)       \n",
        "        x2 = self.softmax(x2,1)       \n",
        "        attention2 = x2.transpose(1,2)       \n",
        "        sentence_embeddings2 = attention2@outputs2\n",
        "        \n",
        "        rand_wt2 = torch.from_numpy(np.random.rand(sentence_embeddings2.shape[0], sentence_embeddings2.shape[1],sentence_embeddings2.shape[1]))\n",
        "        factor_rel2 = torch.bmm(sentence_embeddings2.transpose(2,1), rand_wt2.float()).transpose(2,1)\n",
        "        # Take element-wise multiplication of the two outputs of BMM.\n",
        "        ele_mul = factor_rel1 * factor_rel2\n",
        "        # Average of the output\n",
        "        avg_sentence_embeddings = torch.sum(ele_mul,1)/self.r\n",
        "\n",
        "        # Claim and article embeddings\n",
        "        embeddings3 = self.embeddings3(xs)\n",
        "        embeddings4 = self.embeddings4(ys) \n",
        "        # Average of the embeddings\n",
        "        avg_csource_embeddings = torch.sum(embeddings3,1)/self.r\n",
        "        avg_asource_embeddings = torch.sum(embeddings4,1)/self.r\n",
        "        # Combined average of all three averages\n",
        "        comb_avg = torch.cat((avg_sentence_embeddings, avg_csource_embeddings, avg_asource_embeddings), 1)\n",
        "        \n",
        "        dense1 = self.linear_penul(comb_avg) \n",
        "        output = torch.sigmoid(self.linear_final(dense1))\n",
        "        return output,attention2\n",
        "        \n",
        "    #Regularization\n",
        "    def l2_matrix_norm(self,m):\n",
        "        \"\"\"\n",
        "        Frobenius norm calculation\n",
        " \n",
        "        Args:\n",
        "           m: {Variable} ||AAT - I||\n",
        " \n",
        "        Returns:\n",
        "            regularized value\n",
        " \n",
        "       \n",
        "        \"\"\"\n",
        "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipz-9qWVBEAz",
        "colab_type": "code",
        "outputId": "d78232be-6680-4a18-fcc7-cad8164e6c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Initialize and train the output\n",
        "attention_model5 = StructuredSelfAttention(batch_size=train_loader.batch_size,lstm_hid_dim=model_params['lstm_hidden_dimension'],d_a = model_params[\"d_a\"],r=params_set[\"attention_hops\"],vocab_size=len(word_to_id),max_len=MAXLENGTH,type=0,n_classes=1,use_pretrained_embeddings=True,embeddings=embeddings)\n",
        "\n",
        "loss, acc = binary_classfication(attention_model5,train_loader=train_loader,epochs=2,use_regularization=False,C=params_set[\"C\"],clip=params_set[\"clip\"])\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:95: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 10, 10])\n",
            "torch.Size([512, 10, 10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-5682a0a59db2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattention_model5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructuredSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_hid_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm_hidden_dimension'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d_a\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_hops\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAXLENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_pretrained_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_classfication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clip\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-87b0dd2f1e24>\u001b[0m in \u001b[0;36mbinary_classfication\u001b[0;34m(attention_model, train_loader, epochs, use_regularization, C, clip)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_regularization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-43d2b81b275e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(attention_model, train_loader, criterion, optimizer, epochs, use_regularization, C, clip)\u001b[0m\n\u001b[1;32m     29\u001b[0m            \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m            \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m            \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m            \u001b[0;31m#penalization AAT - I\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-e9a4403200a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, xs, ys)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mmain_embedding\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_part_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# combination of the two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Bi-LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Self-attention mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjkNKmssBPDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accuracy of the test data\n",
        "acc5, pred5 = evaluate(attention_model5, x_test_pad_cl, x_test_pad, x_test_pad_cls, x_test_pad_s, y_test)\n",
        "print(acc5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zjrROjYCTWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(y_test,pred5.detach().numpy(),[0,1])\n",
        "plot_confusion_matrix(y_test,pred5.detach().numpy(),[0,1], normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc0goin7CTSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_p,batch_r,batch_a,batch_f1 = evaluation_summary(\"Self Attention-Article and sources\", pred5.detach().numpy(), y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah0xu_kLC1gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, pred5.detach().numpy())\n",
        "auc_batch = auc(fpr, tpr)\n",
        "print(\"Self Attention-Article_souces AUC-\",auc_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgrULbGVC-g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visulaisation\n",
        "test_last_idx = 15\n",
        "wts5 = get_activation_wts(attention_model5,Variable(torch.from_numpy(x_test_pad_cl[:test_last_idx]).type(torch.LongTensor)),\n",
        "                             Variable(torch.from_numpy(x_test_pad[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_cls[:test_last_idx]).type(torch.LongTensor)),\n",
        "                        Variable(torch.from_numpy(x_test_pad_s[:test_last_idx]).type(torch.LongTensor)))\n",
        "print(wts5.size())\n",
        "visualize_attention(wts5,x_test_pad[:test_last_idx],word_to_id,filename='attention_batch_dot.html')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaczWn08sSTj",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation on Unique Claims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej6cJ2SgzgC8",
        "colab_type": "text"
      },
      "source": [
        "Due to the problem of experimental bias introduced into the model, we test the performance of the Hybrid model on unique claims. This bias is introduced because a claim can be associated with many articles and hence can be repeated in traning and test data making it seen data for the model when testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib8RGSXBsWrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For full model\n",
        "x_test_or['bin_cred_label'] = y_test\n",
        "x_test_or['pred_label_full'] = pred.detach().numpy()\n",
        "unique_test_or2 = x_test_or[~x_test_or['claim_text'].isin(x_train_or['claim_text'])]\n",
        "unique_test_or2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laV8y5eBsjES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bin_pred2=unique_test_or2['bin_cred_label'].tolist()\n",
        "pred_val2=unique_test_or2['pred_label_full'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4zk7CE3spIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(bin_pred2,pred_val2,[0,1])\n",
        "plot_confusion_matrix(bin_pred2,pred_val2,[0,1], normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h77GFZ4spwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ac_p,ac_r,ac_a,ac_f1 = evaluation_summary(\"Self Attention-article and claim\", pred_val2, bin_pred2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--d7_OuFstNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(bin_pred2, pred_val2)\n",
        "auc_arts = auc(fpr, tpr)\n",
        "print(\"Self Attention-Article_souces AUC-\",auc_arts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynUS8fN4swEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_counts = unique_test_or2['bin_cred_label'].value_counts()\n",
        "print(label_counts.describe())\n",
        "top_labels = label_counts.nlargest(5)\n",
        "top_labels_list = top_labels.index.tolist()\n",
        "print(top_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8NcHl0AszAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incorrect2 = unique_test_or2[unique_test_or2['bin_cred_label']!=unique_test_or2['pred_label_full'].astype(int)]\n",
        "incorrect2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfwu1VYfs1XO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write to csv\n",
        "unique_test_or2.to_csv('op_attn_pol_full.csv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qsC9KM-s6QH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For Art_claim model\n",
        "x_test_or['pred_label2'] = pred2.detach().numpy()\n",
        "unique_test_or3 = x_test_or[~x_test_or['claim_text'].isin(x_train_or['claim_text'])]\n",
        "unique_test_or3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GF60ET-tAvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bin_pred3=unique_test_or3['bin_cred_label'].tolist()\n",
        "pred_val3=unique_test_or3['pred_label2'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOTrL89jtDP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(bin_pred3,pred_val3,[0,1])\n",
        "plot_confusion_matrix(bin_pred3,pred_val3,[0,1], normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BysqYlVVtHfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ac_p,ac_r,ac_a,ac_f1 = evaluation_summary(\"Self Attention-article and claim\", pred_val3, bin_pred3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHix3vnTtHcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(bin_pred3, pred_val3)\n",
        "auc_arts = auc(fpr, tpr)\n",
        "print(\"Self Attention-Article_souces AUC-\",auc_arts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr9FjUVKtMUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_counts = unique_test_or3['bin_cred_label'].value_counts()\n",
        "print(label_counts.describe())\n",
        "top_labels = label_counts.nlargest(5)\n",
        "top_labels_list = top_labels.index.tolist()\n",
        "print(top_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgkCJihTtNNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incorrect3 = unique_test_or2[unique_test_or3['bin_cred_label']!=unique_test_or3['pred_label2'].astype(int)]\n",
        "incorrect3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUBuVjxUtPW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write to csv\n",
        "unique_test_or3.to_csv('op_attn_pol_artcl.csv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgQKsXrf0Hdq",
        "colab_type": "text"
      },
      "source": [
        "## Bibliography"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuGHVyUJ0LE6",
        "colab_type": "text"
      },
      "source": [
        "Kashyap Popat, Subhabrata Mukherjee, Andrew Yates and Gerhard Weikum, <i>DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning</i>, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Pages 22-32, 2018.\n",
        "\n",
        "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou and Yoshua Bengio,<i> A Structured Self-attentive Sentence Embedding</i>, ICLR, 2017."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wD06Kq4KG_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}