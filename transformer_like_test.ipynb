{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/level4project/blob/master/transformer_like_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMOOZyQ1xwfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "bef3d438-354f-4f7a-bd57-ea6543054116"
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-17 00:07:23--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip.2’\n",
            "\n",
            "snli_1.0.zip.2        8%[>                   ]   7.91M  1.23MB/s    eta 79s    N\n",
            "snli_1.0.zip.2       10%[=>                  ]   9.29M   812KB/s    eta 87s    "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-375658c45ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip snli_1.0.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK7YRq7oPCWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhlX3fSGx2aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch,keras\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "from torch import nn\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INou3VKpN7_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 150\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 128\n",
        "SAMPLE_SAMPLE_SIZE = 4\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "\n",
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTxMpyqkR7KV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#alright lets tensordataset this boy\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.LongTensor))\n",
        "\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True)\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.LongTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JwnWeWhORhm",
        "colab_type": "text"
      },
      "source": [
        "STUFF I STILL DON'T KNOW A THING ABOUT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izag5KDB6c91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features, epsilon=1e-8):\n",
        "        '''Applies layer normalization. USE LATER\n",
        "        Args:\n",
        "          epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
        "        '''\n",
        "        super(layer_normalization, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n",
        "\n",
        "#todo: rewrite, understand, use\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_units, zeros_pad=True, scale=True):\n",
        "        '''Sinusoidal Positional_Encoding.\n",
        "        Args:\n",
        "          num_units: Output dimensionality\n",
        "          zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n",
        "          scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)\n",
        "        '''\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.num_units = num_units\n",
        "        self.zeros_pad = zeros_pad\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: A 2d Tensor with shape of (N, T).\n",
        "        N, T = inputs.size()[0: 2]\n",
        "\n",
        "        # First part of the PE function: sin and cos argument\n",
        "        position_ind = Variable(torch.unsqueeze(torch.arange(0, T), 0).repeat(N, 1).long())\n",
        "        position_enc = torch.Tensor([\n",
        "            [pos / np.power(10000, 2. * i / self.num_units) for i in range(self.num_units)]\n",
        "            for pos in range(T)])\n",
        "\n",
        "        # Second part, apply the cosine to even columns and sin to odds.\n",
        "        position_enc[:, 0::2] = torch.sin(position_enc[:, 0::2])  # dim 2i\n",
        "        position_enc[:, 1::2] = torch.cos(position_enc[:, 1::2])  # dim 2i+1\n",
        "\n",
        "        # Convert to a Variable\n",
        "        lookup_table = Variable(position_enc)\n",
        "\n",
        "        if self.zeros_pad:\n",
        "            lookup_table = torch.cat((Variable(torch.zeros(1, self.num_units)),\n",
        "                                     lookup_table[1:, :]), 0)\n",
        "            padding_idx = 0\n",
        "        else:\n",
        "            padding_idx = -1\n",
        "\n",
        "        outputs = F.embedding(\n",
        "            position_ind, lookup_table, padding_idx, None, 2, False, False)   # copied from torch.nn.modules.sparse.py\n",
        "\n",
        "        if self.scale:\n",
        "            outputs = outputs * self.num_units ** 0.5\n",
        "\n",
        "        return outputs\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9uBrXogONRG",
        "colab_type": "text"
      },
      "source": [
        "real shit: multi head attention :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw2nohf4OK0-",
        "colab_type": "text"
      },
      "source": [
        "basic feedforward network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwSVpWMlJo5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Forwarder(nn.Module):\n",
        "  #incredibly basic feedforward network\n",
        "  #two linear changes\n",
        "  def __init__(self, in_channels, num_units=[2048, 512], dropout=0.1):\n",
        "\n",
        "    super(Forwarder, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.num_units = num_units\n",
        "\n",
        "    self.conv1 = nn.Sequential(nn.Linear(self.in_channels, self.num_units[0]), nn.ReLU())\n",
        "    self.conv2 = nn.Linear(self.num_units[0], self.num_units[1])\n",
        "    self.norm1 = nn.LayerNorm(self.in_channels)\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    conv1_outputs = self.conv1(inputs)\n",
        "    conv2_outputs = self.conv2(conv1_outputs)\n",
        "\n",
        "    conv2_outputs += inputs\n",
        "    \n",
        "    outputs = self.norm1(conv2_outputs)\n",
        "\n",
        "    #normalise here\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwBQL9x_OHpG",
        "colab_type": "text"
      },
      "source": [
        "hyperparams: set them HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPSVL_bwIyjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2f8af4f-f35f-45f7-cc87-c55943ed75d0"
      },
      "source": [
        "class Hyperparameters:\n",
        "  attention_size = 300 #hidden/num_units\n",
        "  num_heads = 5\n",
        "  dropout = 0.1\n",
        "  num_classes = 3\n",
        "  num_blocks = 6\n",
        "  epochs = 12\n",
        "  sinusoid = True\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(\"glove.6B.{}d.txt\".format(Hyperparameters.attention_size),x_tokeniser.word_to_id,Hyperparameters.attention_size)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fha2mYJWOEA2",
        "colab_type": "text"
      },
      "source": [
        "putting everything together, here's an attempt at a basic entailment model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iaiiUOKA2q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicEntailmentModel(nn.Module):\n",
        "  def __init__(self,\n",
        "               max_length,\n",
        "               batch_size, \n",
        "               word_embeddings,\n",
        "               vocab_size,\n",
        "               hp):\n",
        "    super(BasicEntailmentModel, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hp.attention_size)\n",
        "    self.premise_embeddings.weight = nn.Parameter(word_embeddings)\n",
        "    self.premise_dropout = nn.Dropout(hp.dropout)\n",
        "    self.premise_norm = nn.LayerNorm(hp.attention_size)\n",
        "\n",
        "\n",
        "    self.premise_positional_encoding = PositionalEncoding(num_units=hp.attention_size,\n",
        "                                                          zeros_pad=False,\n",
        "                                                          scale=False)\n",
        "\n",
        "    self.hypothesis_embeddings = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hp.attention_size)\n",
        "    self.hypothesis_embeddings.weight = nn.Parameter(word_embeddings)\n",
        "    self.hypothesis_dropout = nn.Dropout(hp.dropout)\n",
        "\n",
        "    self.hypothesis_positional_encoding = PositionalEncoding(num_units=hp.attention_size,\n",
        "                                                          zeros_pad=False,\n",
        "                                                          scale=False)\n",
        "    self.hypothesis_norm1 = nn.LayerNorm(hp.attention_size)\n",
        "    self.word_embeddings_size = word_embeddings.size(1)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    \n",
        "    #create a buncha attributes with THIS HACKY CODE -> attention into feedforward\n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.__setattr__('premise_self_attention%d' % i, nn.MultiheadAttention(hp.attention_size,\n",
        "                                                                           num_heads=hp.num_heads,\n",
        "                                                                           dropout=hp.dropout))\n",
        "      self.__setattr__('premise_feed_forward%d' % i, Forwarder(in_channels=hp.attention_size,\n",
        "                                                               num_units = [4*hp.attention_size,\n",
        "                                                                            hp.attention_size]))\n",
        "    \n",
        "    #same for hypothesis\n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.__setattr__('hypothesis_self_attention%d' % i, nn.MultiheadAttention(hp.attention_size,\n",
        "                                                                           num_heads=hp.num_heads,\n",
        "                                                                           dropout=hp.dropout))\n",
        "      self.__setattr__('hypothesis_feed_forward%d' % i, Forwarder(in_channels=hp.attention_size,\n",
        "                                                               num_units = [4*hp.attention_size,\n",
        "                                                                           hp.attention_size]))\n",
        "      \n",
        "    self.mlp1 = nn.Linear(hp.attention_size * max_length, 20)\n",
        "    self.mlp2 = nn.Linear(20, hp.num_classes)\n",
        "    \n",
        "  def forward(self, premise_words, hypothesis_words):\n",
        "    self.premise = self.premise_embeddings(premise_words)\n",
        "    self.premise += self.premise_positional_encoding(premise_words)\n",
        "    self.premise = self.premise_dropout(self.premise.float())\n",
        "\n",
        "    \n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.premise, _ = self.__getattr__('premise_self_attention%d' % i)(self.premise, \n",
        "                                                                      self.premise, \n",
        "                                                                      self.premise)\n",
        "      self.premise = self.__getattr__('premise_feed_forward%d' % i)(self.premise)\n",
        "\n",
        "    self.premise = self.premise_norm(premise)\n",
        "    self.hypothesis = self.hypothesis_embeddings(hypothesis_words)\n",
        "    self.hypothesis += self.hypothesis_positional_encoding(hypothesis_words)\n",
        "\n",
        "    self.hypothesis = self.hypothesis_dropout(self.hypothesis.float())\n",
        "\n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.hypothesis, _ = self.__getattr__('hypothesis_self_attention%d' % i)(self.hypothesis, \n",
        "                                                                      self.hypothesis, \n",
        "                                                                      self.hypothesis)\n",
        "      self.hypothesis = self.__getattr__('hypothesis_feed_forward%d' % i)(self.hypothesis)\n",
        "\n",
        "    self.hypothesis = self.hypothesis_norm(self.hypothesis)\n",
        "    \n",
        "    self.mush = self.premise * self.hypothesis\n",
        "    self.mush = self.mush.reshape(self.batch_size, -1)\n",
        "    self.mush = self.mlp1(self.mush)\n",
        "    self.output = torch.softmax(self.mlp2(self.mush),-1)\n",
        "    return self.output\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6NQnWUvOCVM",
        "colab_type": "text"
      },
      "source": [
        "training function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6SVXr1EF6IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          train_loader=None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          epochs=5, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_index, train_data in enumerate(train_loader):\n",
        "      premise = Variable(train_data[0]).cuda()\n",
        "      hypothesis = Variable(train_data[1]).cuda()\n",
        "      actual_y = Variable(train_data[2]).cuda()\n",
        "      \n",
        "      torch.cuda.synchronize()\n",
        "      optimiser.zero_grad()\n",
        "      predicted_y = model(premise, hypothesis)\n",
        "      squeezed_y = predicted_y.double().squeeze(1)\n",
        "      loss = loss_function(squeezed_y,actual_y.long())\n",
        "\n",
        "      correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "      \n",
        "      if (batch_index == 1):\n",
        "        print(total_loss, correct.data.cpu().numpy().astype(int)/train_loader.batch_size)\n",
        "      total_loss += loss.data\n",
        "\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "      optimiser.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.cuda.synchronize()\n",
        "\n",
        "      if using_gradient_clipping:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      batch_count += 1\n",
        "      optimiser.step()\n",
        "\n",
        "    print(\"Average loss is:\",total_loss/batch_count)\n",
        "    correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "    accuracy = correct_but_numpy / float(batch_count * train_loader.batch_size)\n",
        "    print(\"Accuracy of the model\", accuracy)\n",
        "    losses.append(total_loss/batch_count)\n",
        "    accuracies.append(accuracy)\n",
        "  return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC95R5iyOc4u",
        "colab_type": "text"
      },
      "source": [
        "test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25tdNU7LObr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, test_premise, test_hypothesis, test_y):\n",
        "\n",
        "  model.batch_size = test_hypothesis.shape[0] #why\n",
        "  model = model.cpu()\n",
        "  premise_variable = Variable(torch.from_numpy(test_premise).type(torch.LongTensor))\n",
        "  hypothesis_variable = Variable(torch.from_numpy(test_hypothesis).type(torch.LongTensor))\n",
        "  y_actual_variable = Variable(torch.from_numpy(test_y).type(torch.DoubleTensor))\n",
        "  y_predicted = model(premise_variable, hypothesis_variable)\n",
        "  y_predicted_rounded = torch.round(y_predicted.type(torch.DoubleTensor).squeeze(1))\n",
        "\n",
        "  test_data_count = premise_variable.size(0)\n",
        "\n",
        "\n",
        "\n",
        "  total_accuracy = torch.eq(torch.argmax(y_predicted,1), y_actual_variable).data.sum()\n",
        "  average_accuracy = total_accuracy.data.cpu().numpy().astype(int)/float(test_data_count)\n",
        "  return average_accuracy, torch.argmax(y_predicted,1)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tueScWvqOpCF",
        "colab_type": "text"
      },
      "source": [
        "ok lets run\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d88Jpa8Op7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "440d8bde-4322-4f9a-b890-99119ba11d95"
      },
      "source": [
        "model = BasicEntailmentModel(max_length = MAX_LENGTH,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             vocab_size=VOCAB_SIZE,\n",
        "                             word_embeddings=glove_embeddings,\n",
        "                             hp=Hyperparameters\n",
        "                             )\n",
        "model = model.cuda()\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adagrad(model.parameters(), \n",
        "                                    lr=0.01)\n",
        "loss, accuracy = train(model=model,\n",
        "                       train_loader=train_loader,\n",
        "                       loss_function=loss,\n",
        "                       optimiser = optimiser,\n",
        "                       epochs = 5,\n",
        "                       using_gradient_clipping=True\n",
        "                      )"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-8a98d41751bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                        \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                        \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                        \u001b[0musing_gradient_clipping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                       )\n",
            "\u001b[0;32m<ipython-input-122-7de1d79a96e2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss_function, optimiser, epochs, using_gradient_clipping)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mpredicted_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0msqueezed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueezed_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactual_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-121-b1ed484b7e44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, premise_words, hypothesis_words)\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpremise_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_positional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected device cuda:0 but got device cpu"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSrYQ3HOO16c",
        "colab_type": "text"
      },
      "source": [
        "testan\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7pXaM-aO3PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy, predicted_ys = evaluate(model, \n",
        "         test_hypothesis=x_test[\"hypothesis\"],\n",
        "         test_premise=x_test[\"premise\"],\n",
        "         test_y=y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmx1wl0rTzCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}